{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary as d\n",
    "from gensim.corpora import MmCorpus\n",
    "from gensim.models.ldamodel import LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "from gensim.models.word2vec import Text8Corpus\n",
    "from gensim.models.phrases import Phrases, Phraser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Trigrams from \"Amazon\" mentions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mayb', 14),\n",
       " ('drone', 14),\n",
       " ('content', 14),\n",
       " ('shit', 14),\n",
       " ('increas', 14),\n",
       " ('tri', 13),\n",
       " ('exampl', 13),\n",
       " ('bezo', 13),\n",
       " ('uber', 13),\n",
       " ('googl_amazon', 13),\n",
       " ('stop', 13),\n",
       " ('gamestop', 13),\n",
       " ('shop', 13),\n",
       " ('weed', 13),\n",
       " ('advertis', 12),\n",
       " ('countri', 12),\n",
       " ('monopoli', 12),\n",
       " ('valuat', 12),\n",
       " ('aprn', 12),\n",
       " ('great', 12),\n",
       " ('real', 12),\n",
       " ('month', 12),\n",
       " ('lose', 12),\n",
       " ('talk', 12),\n",
       " ('general', 12),\n",
       " ('today', 12),\n",
       " ('hard', 12),\n",
       " ('wouldn', 12),\n",
       " ('feel', 12),\n",
       " ('internet', 11),\n",
       " ('oper', 11),\n",
       " ('drive', 11),\n",
       " ('past', 11),\n",
       " ('number', 11),\n",
       " ('consum', 11),\n",
       " ('fact', 11),\n",
       " ('think_amazon', 11),\n",
       " ('provid', 11),\n",
       " ('happen', 11),\n",
       " ('compani_like', 11),\n",
       " ('benefit', 11),\n",
       " ('miss', 11),\n",
       " ('control', 11),\n",
       " ('give', 11),\n",
       " ('agre', 11),\n",
       " ('fuck', 11),\n",
       " ('appl_amazon', 11),\n",
       " ('understand', 11),\n",
       " ('spend', 11),\n",
       " ('larg', 11),\n",
       " ('amazon_prime', 11),\n",
       " ('success', 11),\n",
       " ('focus', 11),\n",
       " ('case', 11),\n",
       " ('cours', 11),\n",
       " ('higher', 11),\n",
       " ('debt', 10),\n",
       " ('million', 10),\n",
       " ('warehous', 10),\n",
       " ('manag', 10)]"
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"training-data-2019-04-11.csv\")\n",
    "sell_data = data[data.comments.str.contains(\"Amazon\")]\n",
    "data = sell_data\n",
    "data_text = data[['comments']]\n",
    "data_text= data_text.drop_duplicates(subset='comments', keep=\"last\").reset_index()\n",
    "data_text['index'] = data_text.index\n",
    "documents = data_text\n",
    "processed_documents = documents['comments'].map(tokenization)\n",
    "sentences = processed_documents\n",
    "bigrams = Phrases(sentences, min_count=2, threshold=100)\n",
    "trigrams = Phrases(bigrams[sentences], threshold=100)\n",
    "sorted(trigrams.vocab.items(), key=lambda x:x[1], reverse=True)[110:170]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Trigrams from \"Uber\" Mentions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('travel', 12),\n",
       " ('player', 12),\n",
       " ('data', 12),\n",
       " ('busi_model', 12),\n",
       " ('like_lyft', 12),\n",
       " ('month', 12),\n",
       " ('soon', 12),\n",
       " ('manufactur', 12),\n",
       " ('stop', 12),\n",
       " ('lose_money', 12),\n",
       " ('pinterest', 12),\n",
       " ('uber_go', 12),\n",
       " ('app', 12),\n",
       " ('instead', 12),\n",
       " ('hold', 11),\n",
       " ('offer', 11),\n",
       " ('uber_market', 11),\n",
       " ('number', 11),\n",
       " ('sell', 11),\n",
       " ('person', 11),\n",
       " ('us', 11),\n",
       " ('tri', 11),\n",
       " ('expand', 11),\n",
       " ('away', 11),\n",
       " ('product', 11),\n",
       " ('drop', 11),\n",
       " ('space', 11),\n",
       " ('gonna', 11),\n",
       " ('self_drive_car', 11),\n",
       " ('interest', 11),\n",
       " ('nofollow', 11),\n",
       " ('scale', 11),\n",
       " ('beat', 11),\n",
       " ('hail', 11),\n",
       " ('brand', 11),\n",
       " ('risk', 11),\n",
       " ('insur', 11),\n",
       " ('capit', 11),\n",
       " ('rent', 11),\n",
       " ('rideshar', 11),\n",
       " ('option', 11),\n",
       " ('overvalu', 10),\n",
       " ('think_uber', 10),\n",
       " ('regul', 10),\n",
       " ('take', 10),\n",
       " ('industri', 10),\n",
       " ('plan', 10),\n",
       " ('worth', 10),\n",
       " ('rais', 10),\n",
       " ('global', 10),\n",
       " ('yeah', 10),\n",
       " ('area', 10),\n",
       " ('put', 10),\n",
       " ('save', 10),\n",
       " ('ford', 10),\n",
       " ('margin', 10),\n",
       " ('file', 10),\n",
       " ('wouldn', 10),\n",
       " ('bought', 10),\n",
       " ('second', 9)]"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"training-data-2019-04-11.csv\")\n",
    "sell_data = data[data.comments.str.contains(\"Uber\")]\n",
    "data = sell_data\n",
    "data_text = data[['comments']]\n",
    "data_text= data_text.drop_duplicates(subset='comments', keep=\"last\").reset_index()\n",
    "data_text['index'] = data_text.index\n",
    "documents = data_text\n",
    "processed_documents = documents['comments'].map(tokenization)\n",
    "sentences = processed_documents\n",
    "bigrams = Phrases(sentences, min_count=2, threshold=100)\n",
    "trigrams = Phrases(bigrams[sentences], threshold=100)\n",
    "sorted(trigrams.vocab.items(), key=lambda x:x[1], reverse=True)[110:170]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracted Trigrams from \"Gold\" mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pretti', 7),\n",
       " ('perman_portfolio', 7),\n",
       " ('invest_gold', 7),\n",
       " ('talk', 7),\n",
       " ('share', 7),\n",
       " ('hard', 7),\n",
       " ('idea', 7),\n",
       " ('investor', 7),\n",
       " ('buy', 7),\n",
       " ('instead', 7),\n",
       " ('small', 7),\n",
       " ('bitcoin', 7),\n",
       " ('protect', 7),\n",
       " ('place', 7),\n",
       " ('outperform', 7),\n",
       " ('liter', 7),\n",
       " ('kind', 7),\n",
       " ('strategi', 7),\n",
       " ('bond_gold', 7),\n",
       " ('silver_gold', 7),\n",
       " ('crash', 7),\n",
       " ('futur', 7),\n",
       " ('long_term', 6),\n",
       " ('histor', 6),\n",
       " ('data', 6),\n",
       " ('million', 6),\n",
       " ('paper', 6),\n",
       " ('expens', 6),\n",
       " ('alloc', 6),\n",
       " ('standard', 6),\n",
       " ('buffett', 6),\n",
       " ('scenario', 6),\n",
       " ('like_gold', 6),\n",
       " ('true', 6),\n",
       " ('tesla', 6),\n",
       " ('drop', 6),\n",
       " ('poor', 6),\n",
       " ('correl', 6),\n",
       " ('depend', 6),\n",
       " ('robinhood', 6),\n",
       " ('http', 6),\n",
       " ('call', 6),\n",
       " ('recommend', 6),\n",
       " ('answer', 6),\n",
       " ('explain', 6),\n",
       " ('said', 6),\n",
       " ('hous', 6),\n",
       " ('hold_gold', 6),\n",
       " ('have', 6),\n",
       " ('bear', 6),\n",
       " ('autist', 6),\n",
       " ('nofollow_http', 6),\n",
       " ('metal', 6),\n",
       " ('absolut', 6),\n",
       " ('golden_cross', 6),\n",
       " ('huge', 6),\n",
       " ('lmao', 6),\n",
       " ('potenti', 6),\n",
       " ('caus', 6),\n",
       " ('wouldn', 6)]"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"training-data-2019-04-11.csv\")\n",
    "sell_data = data[data.comments.str.contains(\"gold\")]\n",
    "data = sell_data\n",
    "data_text = data[['comments']]\n",
    "data_text= data_text.drop_duplicates(subset='comments', keep=\"last\").reset_index()\n",
    "data_text['index'] = data_text.index\n",
    "documents = data_text\n",
    "processed_documents = documents['comments'].map(tokenization)\n",
    "sentences = processed_documents\n",
    "bigrams = Phrases(sentences, min_count=2, threshold=100)\n",
    "trigrams = Phrases(bigrams[sentences], threshold=100)\n",
    "sorted(trigrams.vocab.items(), key=lambda x:x[1], reverse=True)[110:170]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [appl, issu, card, payment, processor, fact, g...\n",
       "1      [wait, reason, volum, option, liter, appl, wen...\n",
       "2                                            [appl, put]\n",
       "3                            [appl, need, day, handcuff]\n",
       "4      [watch, govern, announc, appl, buyout, tomorro...\n",
       "5      [yeah, pretti, impress, succeed, appl, follow,...\n",
       "6      [samsung, appl, lot, busi, think, mcdonald, ho...\n",
       "7      [cours, argument, math, point, lower, rate, te...\n",
       "8      [wish, appl, acquir, interest, instead, let, o...\n",
       "9      [href, http, businessinsid, aapl, stock, price...\n",
       "10     [dont, thing, like, free, dividend, cash, give...\n",
       "11                                              [snappl]\n",
       "12                                 [appl, product, wear]\n",
       "13     [good, point, mayb, appl, gift, card, thousand...\n",
       "14     [disney, medium, entertain, compani, appl, tec...\n",
       "15                  [keep, shut, look, like, appl, meme]\n",
       "16     [pictur, appl, logo, look, way, attempt, steal...\n",
       "17     [noob, invest, work, technolog, lucki, bought,...\n",
       "18     [click, read, analysi, chart, download, report...\n",
       "19     [click, chart, download, copi, report, href, h...\n",
       "20                                   [action, appl, min]\n",
       "21                                          [appl, dick]\n",
       "22               [appl, need, bring, musk, shake, thing]\n",
       "23                          [imagin, appl, tesla, dildo]\n",
       "24                                  [invest, appl, meme]\n",
       "25                  [probabl, best, appl, invent, stoke]\n",
       "26     [disney, produc, decent, product, think, zero,...\n",
       "27     [nope, overvalu, stock, current, stock, like, ...\n",
       "28     [valu, compani, weed, tech, amazon, msft, appl...\n",
       "29     [amazon, msft, appl, googl, basic, market, big...\n",
       "                             ...                        \n",
       "271    [food, week, laugh, said, bought, worth, appl,...\n",
       "272               [appl, bought, today, prime, discount]\n",
       "273    [appl, appl, comparison, sizeabl, risk, invest...\n",
       "274    [know, soni, conglomer, electron, divis, blown...\n",
       "275    [risk, manag, varieti, way, absolut, posit, lo...\n",
       "276    [mayb, kinda, compar, appl, orang, give, insig...\n",
       "277                              [subscrib, appl, music]\n",
       "278    [that, think, look, past, coupl, week, graph, ...\n",
       "279    [true, appl, announc, abund, servic, includ, l...\n",
       "280    [appl, know, custom, custom, love, see, everyd...\n",
       "281              [appl, good, confid, hold, appl, stock]\n",
       "282    [dont, appl, product, like, appl, brand, high,...\n",
       "283    [question, better, cash, like, deploy, money, ...\n",
       "284                                        [pacif, appl]\n",
       "285    [yeah, brought, appl, purpos, stupid, downgrad...\n",
       "286    [match, put, went, appl, call, decreas, share,...\n",
       "287                                  [bore, watch, appl]\n",
       "288    [mix, appl, orang, describ, subsidi, sign, poo...\n",
       "289     [honest, ball, appl, safest, frick, heck, money]\n",
       "290                  [appl, earn, want, think, buy, put]\n",
       "291    [appl, orang, comparison, bond, fund, roll, co...\n",
       "292    [doubl, invest, appl, continu, growth, compani...\n",
       "293    [economi, dumb, appl, pie, dollar, minimum, wa...\n",
       "294                                 [green, appl, great]\n",
       "295                                   [appl, pump, dump]\n",
       "296                      [applesauc, bare, reach, mouth]\n",
       "297    [learn, nice, learn, bang, fee, grant, appl, c...\n",
       "298                                         [know, appl]\n",
       "299                        [wonder, knew, qualiti, appl]\n",
       "300    [stock, like, appl, cocnsysnt, beat, stock, ti...\n",
       "Name: comments, Length: 301, dtype: object"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF IDF Vectorizer from Trigrams model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to be completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"training-data-2019-04-11.csv\")\n",
    "data_text = data[['comments']]\n",
    "data_text= data_text.drop_duplicates(subset='comments', keep=\"last\").reset_index()\n",
    "data_text['index'] = data_text.index\n",
    "documents = data_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"training-data-2019-04-11.csv\")\n",
    "sell_data = data[data.comments.str.contains(\"apple\")]\n",
    "data = sell_data\n",
    "data_text = data[['comments']]\n",
    "data_text= data_text.drop_duplicates(subset='comments', keep=\"last\").reset_index()\n",
    "data_text['index'] = data_text.index\n",
    "documents = data_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301\n",
      "   index                                           comments\n",
      "0      0  apple is just issuing the card they are not th...\n",
      "1      1  wait why would you do that LOL there s a reaso...\n",
      "2      2                    I have 5000 in apple puts lol  \n",
      "3      3  All apple needs to do is go up 5  per day for ...\n",
      "4      4  Watch the US government announce an apple buyo...\n"
     ]
    }
   ],
   "source": [
    "print len(documents)\n",
    "print documents[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-Processing:  Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/selenacordona/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2019)\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I thought this would make a difference, but it just adds random words to the topics...\n",
    "#data_text.title = \\\n",
    "#data_text.title.apply(lambda x: re.sub(r\"(\\d+)\", \\\n",
    "#                                          lambda x: num2words.num2words(int(x.group(0))), x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function to run lemmatization and stemming on a given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization + lemmatization step\n",
    "def lemmatization(token):\n",
    "    w = WordNetLemmatizer()\n",
    "    sanitized_token = w.lemmatize(token)\n",
    "    #SnowballStemmer()\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    return stemmer.stem(sanitized_token)\n",
    "\n",
    "def tokenization(text):\n",
    "    result=[]\n",
    "    tokens = gensim.utils.simple_preprocess(text, deacc=True)\n",
    "    for token in tokens:\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and \\\n",
    "        len(token) > 3:\n",
    "            result.append(lemmatization(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_text = data_text.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_text = data_text[['comments']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>apple is just issuing the card they are not th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wait why would you do that LOL there s a reaso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I have 5000 in apple puts lol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>All apple needs to do is go up 5  per day for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Watch the US government announce an apple buyo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            comments\n",
       "0  apple is just issuing the card they are not th...\n",
       "1  wait why would you do that LOL there s a reaso...\n",
       "2                    I have 5000 in apple puts lol  \n",
       "3  All apple needs to do is go up 5  per day for ...\n",
       "4  Watch the US government announce an apple buyo..."
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "['Amazon', 'randomly', 'screens', 'for', 'drugs', '', 'What', 'the', 'fuck', '', '', '']\n",
      " \n",
      "tokenized and lemmatized document: \n",
      "[u'amazon', u'random', u'screen', u'drug', u'fuck']\n"
     ]
    }
   ],
   "source": [
    "sample = data_text.comments[2]\n",
    "\n",
    "print \"original document: \"\n",
    "words = []\n",
    "for word in sample.split(\" \"):\n",
    "    words.append(word)\n",
    "print words\n",
    "print \" \"\n",
    "print \"tokenized and lemmatized document: \"\n",
    "print tokenization(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [appl, issu, card, payment, processor, fact, g...\n",
       "1    [wait, reason, volum, option, liter, appl, wen...\n",
       "2                                          [appl, put]\n",
       "3                          [appl, need, day, handcuff]\n",
       "4    [watch, govern, announc, appl, buyout, tomorro...\n",
       "5    [yeah, pretti, impress, succeed, appl, follow,...\n",
       "6    [samsung, appl, lot, busi, think, mcdonald, ho...\n",
       "7    [cours, argument, math, point, lower, rate, te...\n",
       "8    [wish, appl, acquir, interest, instead, let, o...\n",
       "9    [href, http, businessinsid, aapl, stock, price...\n",
       "Name: comments, dtype: object"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_documents = documents['comments'].map(tokenization)\n",
    "processed_documents[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = pd.concat([processed_documents, data.title], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[appl, issu, card, payment, processor, fact, g...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[wait, reason, volum, option, liter, appl, wen...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[appl, put]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[appl, need, day, handcuff]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[watch, govern, announc, appl, buyout, tomorro...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[yeah, pretti, impress, succeed, appl, follow,...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[samsung, appl, lot, busi, think, mcdonald, ho...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[cours, argument, math, point, lower, rate, te...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[wish, appl, acquir, interest, instead, let, o...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[href, http, businessinsid, aapl, stock, price...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[dont, thing, like, free, dividend, cash, give...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[snappl]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[appl, product, wear]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[good, point, mayb, appl, gift, card, thousand...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[disney, medium, entertain, compani, appl, tec...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[keep, shut, look, like, appl, meme]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[pictur, appl, logo, look, way, attempt, steal...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[noob, invest, work, technolog, lucki, bought,...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[click, read, analysi, chart, download, report...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[click, chart, download, copi, report, href, h...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[action, appl, min]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[appl, dick]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[appl, need, bring, musk, shake, thing]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[imagin, appl, tesla, dildo]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[invest, appl, meme]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[probabl, best, appl, invent, stoke]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[disney, produc, decent, product, think, zero,...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[nope, overvalu, stock, current, stock, like, ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[valu, compani, weed, tech, amazon, msft, appl...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[amazon, msft, appl, googl, basic, market, big...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62760</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Whole Foods shoppers blast Amazon s Prime memb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62761</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Whole Foods shoppers blast Amazon s Prime memb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63096</th>\n",
       "      <td>NaN</td>\n",
       "      <td>What dollar amount did your company contribute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64456</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Exclusive Loeb s Third Point building stake to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64659</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Ladybaybee recommended stock picks 2019 04 08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66003</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Fun Fact Lyft s market cap is 4x larger than J...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66231</th>\n",
       "      <td>NaN</td>\n",
       "      <td>What Are Your Moves Tomorrow April 09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66339</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Argument for DIS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66340</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Argument for DIS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66344</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Argument for DIS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66788</th>\n",
       "      <td>NaN</td>\n",
       "      <td>AAPL this is no longer the Steve Jobs company ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66794</th>\n",
       "      <td>NaN</td>\n",
       "      <td>AAPL this is no longer the Steve Jobs company ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67491</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Your Apple gains are cute but</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67502</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Your Apple gains are cute but</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67514</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Your Apple gains are cute but</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68289</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Daily Discussion Thread April 09 2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68290</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Daily Discussion Thread April 09 2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68597</th>\n",
       "      <td>NaN</td>\n",
       "      <td>The US Treasury is going to add an additional ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69296</th>\n",
       "      <td>NaN</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70777</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Please kill me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71559</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Learning to Invest bonds vs bond ETFs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72485</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Getting tired of the ROKU turbulence any sugge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72657</th>\n",
       "      <td>NaN</td>\n",
       "      <td>What do you see happening in the economy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73315</th>\n",
       "      <td>NaN</td>\n",
       "      <td>What Are Your Moves Tomorrow April 10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73317</th>\n",
       "      <td>NaN</td>\n",
       "      <td>What Are Your Moves Tomorrow April 10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74574</th>\n",
       "      <td>NaN</td>\n",
       "      <td>They live among us undetected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75291</th>\n",
       "      <td>NaN</td>\n",
       "      <td>50k in dis calls May the force be with me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77134</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Let s talk about Apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77136</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Let s talk about Apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80244</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NVDA Price target 45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>602 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                comments  \\\n",
       "0      [appl, issu, card, payment, processor, fact, g...   \n",
       "1      [wait, reason, volum, option, liter, appl, wen...   \n",
       "2                                            [appl, put]   \n",
       "3                            [appl, need, day, handcuff]   \n",
       "4      [watch, govern, announc, appl, buyout, tomorro...   \n",
       "5      [yeah, pretti, impress, succeed, appl, follow,...   \n",
       "6      [samsung, appl, lot, busi, think, mcdonald, ho...   \n",
       "7      [cours, argument, math, point, lower, rate, te...   \n",
       "8      [wish, appl, acquir, interest, instead, let, o...   \n",
       "9      [href, http, businessinsid, aapl, stock, price...   \n",
       "10     [dont, thing, like, free, dividend, cash, give...   \n",
       "11                                              [snappl]   \n",
       "12                                 [appl, product, wear]   \n",
       "13     [good, point, mayb, appl, gift, card, thousand...   \n",
       "14     [disney, medium, entertain, compani, appl, tec...   \n",
       "15                  [keep, shut, look, like, appl, meme]   \n",
       "16     [pictur, appl, logo, look, way, attempt, steal...   \n",
       "17     [noob, invest, work, technolog, lucki, bought,...   \n",
       "18     [click, read, analysi, chart, download, report...   \n",
       "19     [click, chart, download, copi, report, href, h...   \n",
       "20                                   [action, appl, min]   \n",
       "21                                          [appl, dick]   \n",
       "22               [appl, need, bring, musk, shake, thing]   \n",
       "23                          [imagin, appl, tesla, dildo]   \n",
       "24                                  [invest, appl, meme]   \n",
       "25                  [probabl, best, appl, invent, stoke]   \n",
       "26     [disney, produc, decent, product, think, zero,...   \n",
       "27     [nope, overvalu, stock, current, stock, like, ...   \n",
       "28     [valu, compani, weed, tech, amazon, msft, appl...   \n",
       "29     [amazon, msft, appl, googl, basic, market, big...   \n",
       "...                                                  ...   \n",
       "62760                                                NaN   \n",
       "62761                                                NaN   \n",
       "63096                                                NaN   \n",
       "64456                                                NaN   \n",
       "64659                                                NaN   \n",
       "66003                                                NaN   \n",
       "66231                                                NaN   \n",
       "66339                                                NaN   \n",
       "66340                                                NaN   \n",
       "66344                                                NaN   \n",
       "66788                                                NaN   \n",
       "66794                                                NaN   \n",
       "67491                                                NaN   \n",
       "67502                                                NaN   \n",
       "67514                                                NaN   \n",
       "68289                                                NaN   \n",
       "68290                                                NaN   \n",
       "68597                                                NaN   \n",
       "69296                                                NaN   \n",
       "70777                                                NaN   \n",
       "71559                                                NaN   \n",
       "72485                                                NaN   \n",
       "72657                                                NaN   \n",
       "73315                                                NaN   \n",
       "73317                                                NaN   \n",
       "74574                                                NaN   \n",
       "75291                                                NaN   \n",
       "77134                                                NaN   \n",
       "77136                                                NaN   \n",
       "80244                                                NaN   \n",
       "\n",
       "                                                   title  \n",
       "0                                                    NaN  \n",
       "1                                                    NaN  \n",
       "2                                                    NaN  \n",
       "3                                                    NaN  \n",
       "4                                                    NaN  \n",
       "5                                                    NaN  \n",
       "6                                                    NaN  \n",
       "7                                                    NaN  \n",
       "8                                                    NaN  \n",
       "9                                                    NaN  \n",
       "10                                                   NaN  \n",
       "11                                                   NaN  \n",
       "12                                                   NaN  \n",
       "13                                                   NaN  \n",
       "14                                                   NaN  \n",
       "15                                                   NaN  \n",
       "16                                                   NaN  \n",
       "17                                                   NaN  \n",
       "18                                                   NaN  \n",
       "19                                                   NaN  \n",
       "20                                                   NaN  \n",
       "21                                                   NaN  \n",
       "22                                                   NaN  \n",
       "23                                                   NaN  \n",
       "24                                                   NaN  \n",
       "25                                                   NaN  \n",
       "26                                                   NaN  \n",
       "27                                                   NaN  \n",
       "28                                                   NaN  \n",
       "29                                                   NaN  \n",
       "...                                                  ...  \n",
       "62760  Whole Foods shoppers blast Amazon s Prime memb...  \n",
       "62761  Whole Foods shoppers blast Amazon s Prime memb...  \n",
       "63096  What dollar amount did your company contribute...  \n",
       "64456  Exclusive Loeb s Third Point building stake to...  \n",
       "64659     Ladybaybee recommended stock picks 2019 04 08   \n",
       "66003  Fun Fact Lyft s market cap is 4x larger than J...  \n",
       "66231             What Are Your Moves Tomorrow April 09   \n",
       "66339                                  Argument for DIS   \n",
       "66340                                  Argument for DIS   \n",
       "66344                                  Argument for DIS   \n",
       "66788  AAPL this is no longer the Steve Jobs company ...  \n",
       "66794  AAPL this is no longer the Steve Jobs company ...  \n",
       "67491                     Your Apple gains are cute but   \n",
       "67502                     Your Apple gains are cute but   \n",
       "67514                     Your Apple gains are cute but   \n",
       "68289             Daily Discussion Thread April 09 2019   \n",
       "68290             Daily Discussion Thread April 09 2019   \n",
       "68597  The US Treasury is going to add an additional ...  \n",
       "69296                                              AAPL   \n",
       "70777                                    Please kill me   \n",
       "71559             Learning to Invest bonds vs bond ETFs   \n",
       "72485  Getting tired of the ROKU turbulence any sugge...  \n",
       "72657          What do you see happening in the economy   \n",
       "73315             What Are Your Moves Tomorrow April 10   \n",
       "73317             What Are Your Moves Tomorrow April 10   \n",
       "74574                     They live among us undetected   \n",
       "75291         50k in dis calls May the force be with me   \n",
       "77134                            Let s talk about Apple   \n",
       "77136                            Let s talk about Apple   \n",
       "80244                              NVDA Price target 45   \n",
       "\n",
       "[602 rows x 2 columns]"
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple_sample = processed[processed.title.str.contains(\"appl\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple_sample_raw = data[data.title.str.contains(\"appl\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(161, 2)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apple_sample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words dataset on documents "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the BoW dataset to return number of times a word appears in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 appar\n",
      "1 explod\n",
      "2 market\n",
      "3 peopl\n",
      "4 say\n",
      "5 curv\n",
      "6 look\n",
      "7 yield\n",
      "8 everybodi\n",
      "9 hate\n",
      "10 moon\n",
      "11 tesla\n",
      "12 upvot\n",
      "13 worri\n",
      "14 agre\n",
      "15 call\n",
      "16 certain\n",
      "17 dick\n",
      "18 morn\n",
      "19 move\n",
      "20 today\n",
      "21 faith\n",
      "22 musk\n",
      "23 papa\n",
      "24 coupl\n",
      "25 happen\n",
      "26 indic\n",
      "27 invert\n",
      "28 month\n",
      "29 recess\n",
      "30 remain\n",
      "31 serf\n",
      "32 time\n",
      "33 year\n",
      "34 appl\n",
      "35 compani\n",
      "36 depend\n",
      "37 expens\n",
      "38 great\n",
      "39 level\n",
      "40 nike\n",
      "41 portfolio\n",
      "42 risk\n",
      "43 toler\n",
      "44 nappl\n",
      "45 aapl\n",
      "46 insan\n",
      "47 difficult\n",
      "48 doubt\n",
      "49 high\n",
      "50 incred\n",
      "51 justifi\n",
      "52 differ\n",
      "53 edit\n",
      "54 sourc\n",
      "55 thought\n",
      "56 collaps\n",
      "57 consid\n",
      "58 hit\n",
      "59 stock\n",
      "60 area\n",
      "61 best\n",
      "62 fair\n",
      "63 growth\n",
      "64 larg\n",
      "65 left\n",
      "66 mindless\n",
      "67 occur\n",
      "68 plus\n",
      "69 primari\n",
      "70 proport\n",
      "71 purchas\n",
      "72 reason\n",
      "73 relat\n",
      "74 right\n",
      "75 servic\n",
      "76 shift\n",
      "77 transact\n",
      "78 fail\n",
      "79 quarter\n",
      "80 signal\n",
      "81 virtu\n",
      "82 work\n",
      "83 believ\n",
      "84 fall\n",
      "85 go\n",
      "86 sure\n",
      "87 truli\n",
      "88 butthurt\n",
      "89 basketbal\n",
      "90 gramma\n",
      "91 number\n",
      "92 play\n",
      "93 fine\n",
      "94 billion\n",
      "95 doubl\n",
      "96 easier\n",
      "97 increas\n",
      "98 price\n",
      "99 share\n",
      "100 think\n",
      "101 trillion\n",
      "102 base\n",
      "103 compar\n",
      "104 cook\n",
      "105 especi\n",
      "106 giant\n",
      "107 hardwar\n",
      "108 higher\n",
      "109 multipl\n",
      "110 push\n",
      "111 revenu\n",
      "112 simpli\n",
      "113 subscript\n",
      "114 tech\n",
      "115 valu\n",
      "116 worth\n",
      "117 answer\n",
      "118 like\n",
      "119 make\n",
      "120 sens\n",
      "121 ball\n",
      "122 crystal\n",
      "123 better\n",
      "124 contain\n",
      "125 power\n",
      "126 predict\n",
      "127 psychic\n",
      "128 understand\n",
      "129 want\n",
      "130 free\n",
      "131 money\n",
      "132 put\n",
      "133 break\n",
      "134 depth\n",
      "135 googl\n",
      "136 quick\n",
      "137 search\n",
      "138 varieti\n",
      "139 clear\n",
      "140 bought\n",
      "141 research\n",
      "142 generat\n",
      "143 introduc\n",
      "144 iphon\n",
      "145 ipod\n",
      "146 movi\n",
      "147 music\n",
      "148 sale\n",
      "149 show\n",
      "150 overvalu\n",
      "151 peer\n",
      "152 ridicul\n",
      "153 buy\n",
      "154 own\n",
      "155 shitload\n",
      "156 announc\n",
      "157 disappoint\n",
      "158 hold\n",
      "159 monday\n",
      "160 afford\n",
      "161 cover\n",
      "162 extra\n",
      "163 incom\n",
      "164 write\n",
      "165 disclaim\n",
      "166 sharehold\n",
      "167 butt\n",
      "168 competit\n",
      "169 data\n",
      "170 fuck\n",
      "171 lyft\n",
      "172 platform\n",
      "173 waymo\n",
      "174 comin\n",
      "175 european\n",
      "176 goddamn\n",
      "177 happn\n",
      "178 murican\n",
      "179 tendi\n",
      "180 cool\n",
      "181 seen\n",
      "182 tell\n",
      "183 ugliest\n",
      "184 silver\n",
      "185 black\n",
      "186 pretti\n",
      "187 love\n",
      "188 wallstreetbet\n",
      "189 liter\n",
      "190 tit\n",
      "191 baghold\n",
      "192 retard\n",
      "193 austria\n",
      "194 live\n",
      "195 author\n",
      "196 forc\n",
      "197 get\n",
      "198 govern\n",
      "199 jesus\n",
      "200 moat\n",
      "201 solut\n",
      "202 took\n",
      "203 word\n",
      "204 abl\n",
      "205 accept\n",
      "206 allow\n",
      "207 attent\n",
      "208 bet\n",
      "209 capit\n",
      "210 class\n",
      "211 compet\n",
      "212 condit\n",
      "213 couldn\n",
      "214 cyang\n",
      "215 degen\n",
      "216 essenti\n",
      "217 gang\n",
      "218 gold\n",
      "219 good\n",
      "220 hail\n",
      "221 href\n",
      "222 implement\n",
      "223 includ\n",
      "224 influx\n",
      "225 intent\n",
      "226 mobil\n",
      "227 mod\n",
      "228 opportun\n",
      "229 order\n",
      "230 particip\n",
      "231 pas\n",
      "232 peasant\n",
      "233 pinch\n",
      "234 post\n",
      "235 pyramid\n",
      "236 rank\n",
      "237 recent\n",
      "238 scheme\n",
      "239 shameless\n",
      "240 social\n",
      "241 sort\n",
      "242 street\n",
      "243 struck\n",
      "244 style\n",
      "245 subreddit\n",
      "246 titl\n",
      "247 trap\n",
      "248 wall\n",
      "249 whore\n",
      "250 achiev\n",
      "251 capitalist\n",
      "252 chanc\n",
      "253 chang\n",
      "254 control\n",
      "255 dream\n",
      "256 eat\n",
      "257 give\n",
      "258 grievous\n",
      "259 illus\n",
      "260 inequ\n",
      "261 lavish\n",
      "262 mean\n",
      "263 minut\n",
      "264 mistaken\n",
      "265 oppress\n",
      "266 outsid\n",
      "267 product\n",
      "268 realiz\n",
      "269 reddit\n",
      "270 rest\n",
      "271 start\n",
      "272 subsid\n",
      "273 week\n",
      "274 will\n",
      "275 advertis\n",
      "276 auto\n",
      "277 bare\n",
      "278 budg\n",
      "279 comment\n",
      "280 defi\n",
      "281 econom\n",
      "282 elast\n",
      "283 fact\n",
      "284 github\n",
      "285 given\n",
      "286 import\n",
      "287 law\n",
      "288 paid\n",
      "289 perma\n",
      "290 resid\n",
      "291 robot\n",
      "292 scenario\n",
      "293 smallmamm\n",
      "294 stem\n",
      "295 suppli\n",
      "296 technic\n",
      "297 tri\n",
      "298 truth\n",
      "299 unban\n",
      "300 vote\n",
      "301 winkerpack\n",
      "302 abil\n",
      "303 artifici\n",
      "304 autom\n",
      "305 away\n",
      "306 bamboozl\n",
      "307 began\n",
      "308 blame\n",
      "309 bubbl\n",
      "310 burden\n",
      "311 circumst\n",
      "312 effici\n",
      "313 filter\n",
      "314 futil\n",
      "315 grandkid\n",
      "316 ident\n",
      "317 identifi\n",
      "318 individu\n",
      "319 intellig\n",
      "320 kid\n",
      "321 leav\n",
      "322 lift\n",
      "323 loom\n",
      "324 machin\n",
      "325 manner\n",
      "326 meant\n",
      "327 need\n",
      "328 neglect\n",
      "329 notic\n",
      "330 person\n",
      "331 point\n",
      "332 posit\n",
      "333 preced\n",
      "334 problem\n",
      "335 qualiti\n",
      "336 respons\n",
      "337 result\n",
      "338 shitpost\n",
      "339 singl\n",
      "340 societi\n",
      "341 strip\n",
      "342 surviv\n",
      "343 talent\n",
      "344 wonder\n",
      "345 addit\n",
      "346 amazon\n",
      "347 andrew\n",
      "348 boy\n",
      "349 carl\n",
      "350 civil\n",
      "351 cuck\n",
      "352 gambl\n",
      "353 histori\n",
      "354 lord\n",
      "355 marx\n",
      "356 necessari\n",
      "357 promis\n",
      "358 said\n",
      "359 satiat\n",
      "360 savior\n",
      "361 success\n",
      "362 yang\n",
      "363 actual\n",
      "364 caus\n",
      "365 idea\n",
      "366 inflat\n",
      "367 repercuss\n",
      "368 ad\n",
      "369 advanc\n",
      "370 busi\n",
      "371 circul\n",
      "372 citizen\n",
      "373 collar\n",
      "374 come\n",
      "375 communiti\n",
      "376 concern\n",
      "377 corpor\n",
      "378 cost\n",
      "379 deriv\n",
      "380 disenfranchis\n",
      "381 dividend\n",
      "382 effect\n",
      "383 enterpris\n",
      "384 eventu\n",
      "385 form\n",
      "386 freedom\n",
      "387 half\n",
      "388 hire\n",
      "389 huge\n",
      "390 human\n",
      "391 hurt\n",
      "392 incentiv\n",
      "393 inevit\n",
      "394 issu\n",
      "395 job\n",
      "396 labor\n",
      "397 lower\n",
      "398 maintain\n",
      "399 microeconom\n",
      "400 million\n",
      "401 minimum\n",
      "402 nation\n",
      "403 overal\n",
      "404 physic\n",
      "405 progress\n",
      "406 reduc\n",
      "407 replac\n",
      "408 resort\n",
      "409 retain\n",
      "410 save\n",
      "411 small\n",
      "412 spark\n",
      "413 stay\n",
      "414 suggest\n",
      "415 tackl\n",
      "416 take\n",
      "417 tax\n",
      "418 technolog\n",
      "419 threaten\n",
      "420 troubl\n",
      "421 type\n",
      "422 wage\n",
      "423 white\n",
      "424 accord\n",
      "425 aim\n",
      "426 american\n",
      "427 argument\n",
      "428 basic\n",
      "429 benefit\n",
      "430 cheaper\n",
      "431 close\n",
      "432 common\n",
      "433 consum\n",
      "434 demand\n",
      "435 direct\n",
      "436 fellow\n",
      "437 fire\n",
      "438 gift\n",
      "439 hinder\n",
      "440 jobless\n",
      "441 life\n",
      "442 local\n",
      "443 logic\n",
      "444 margin\n",
      "445 mind\n",
      "446 movement\n",
      "447 numer\n",
      "448 option\n",
      "449 perfect\n",
      "450 place\n",
      "451 process\n",
      "452 profit\n",
      "453 prove\n",
      "454 rais\n",
      "455 react\n",
      "456 solvent\n",
      "457 step\n",
      "458 suck\n",
      "459 suit\n",
      "460 thing\n",
      "461 unaffect\n",
      "462 worker\n",
      "463 awaken\n",
      "464 biggest\n",
      "465 diseas\n",
      "466 flair\n",
      "467 fourth\n",
      "468 gimm\n",
      "469 plagu\n",
      "470 revolut\n",
      "471 rude\n",
      "472 seiz\n",
      "473 shit\n",
      "474 symptom\n",
      "475 term\n",
      "476 wast\n",
      "477 invit\n",
      "478 strong\n",
      "479 window\n",
      "480 anonym\n",
      "481 aren\n",
      "482 http\n",
      "483 imgur\n",
      "484 report\n",
      "485 later\n",
      "486 miss\n",
      "487 bore\n",
      "488 kind\n",
      "489 longform\n",
      "490 spastic\n",
      "491 thank\n",
      "492 xoxo\n",
      "493 effort\n",
      "494 master\n",
      "495 thesi\n",
      "496 true\n",
      "497 arguabl\n",
      "498 use\n",
      "499 bullshit\n",
      "500 page\n",
      "501 stop\n",
      "502 unfortun\n",
      "503 mayb\n",
      "504 straight\n",
      "505 student\n",
      "506 texa\n",
      "507 rate\n",
      "508 surpris\n",
      "509 heard\n",
      "510 clove\n",
      "511 forgot\n",
      "512 nofollow\n",
      "513 texas_rang\n",
      "514 phase\n",
      "515 braunfel\n",
      "516 blue\n",
      "517 island\n",
      "518 riddl\n",
      "519 austin\n",
      "520 guy\n",
      "521 zomg\n",
      "522 counti\n",
      "523 dalla\n",
      "524 furthest\n",
      "525 ranger\n",
      "526 wrong\n",
      "527 famili\n",
      "528 addi\n",
      "529 refil\n",
      "530 script\n",
      "531 smoke\n",
      "532 speed\n",
      "533 abus\n",
      "534 excus\n",
      "535 read\n",
      "536 meme\n",
      "537 estat\n",
      "538 real\n",
      "539 burn\n",
      "540 unit\n",
      "541 long\n",
      "542 support\n",
      "543 ahahah\n",
      "544 care\n",
      "545 happi\n",
      "546 hard\n",
      "547 keep\n",
      "548 late\n",
      "549 stuff\n",
      "550 that\n",
      "551 yall\n",
      "552 tldr\n",
      "553 fuckin\n",
      "554 text\n",
      "555 natur\n",
      "556 autist\n",
      "557 crazi\n",
      "558 open\n",
      "559 weekend\n",
      "560 scrub\n",
      "561 asid\n",
      "562 betray\n",
      "563 communism\n",
      "564 counter\n",
      "565 elit\n",
      "566 joke\n",
      "567 major\n",
      "568 monetari\n",
      "569 overst\n",
      "570 polici\n",
      "571 reaction\n",
      "572 rich\n",
      "573 zirp\n",
      "574 choic\n",
      "575 activ\n",
      "576 complain\n",
      "577 engag\n",
      "578 help\n",
      "579 probabl\n",
      "580 blog\n",
      "581 nice\n",
      "582 subscrib\n",
      "583 deep\n",
      "584 stone\n",
      "585 doll\n",
      "586 drug\n",
      "587 spare\n",
      "588 spend\n",
      "589 comrad\n",
      "590 yaaa\n",
      "591 wine\n",
      "592 dynam\n",
      "593 lead\n",
      "594 model\n",
      "595 snapshot\n",
      "596 system\n",
      "597 christ\n",
      "598 gonna\n",
      "599 thanx\n",
      "600 alt\n",
      "601 shoulda\n",
      "602 written\n",
      "603 laugh\n",
      "604 sooooooo\n",
      "605 unionis\n",
      "606 follow\n",
      "607 metaphor\n",
      "608 interest\n",
      "609 regret\n",
      "610 analysi\n",
      "611 brother\n",
      "612 decent\n",
      "613 proletariat\n",
      "614 status\n",
      "615 frank\n",
      "616 insult\n",
      "617 middl\n",
      "618 patrician\n",
      "619 pretend\n",
      "620 season\n",
      "621 sow\n",
      "622 undertak\n",
      "623 action\n",
      "624 carri\n",
      "625 comfort\n",
      "626 expect\n",
      "627 floor\n",
      "628 hous\n",
      "629 role\n",
      "630 slight\n",
      "631 wood\n",
      "632 past\n",
      "633 sentenc\n",
      "634 lmao\n",
      "635 bing\n",
      "636 bong\n",
      "637 chines\n",
      "638 ching\n",
      "639 chong\n",
      "640 communisim\n",
      "641 doesnt\n",
      "642 tldnr\n",
      "643 centuri\n",
      "644 communist\n",
      "645 lenin\n",
      "646 russia\n",
      "647 select\n",
      "648 member\n",
      "649 modicum\n",
      "650 parti\n",
      "651 swanson\n",
      "652 ultim\n",
      "653 overlord\n",
      "654 breath\n",
      "655 healthi\n",
      "656 nerd\n",
      "657 quit\n",
      "658 hammer\n",
      "659 obvious\n",
      "660 stupid\n",
      "661 bloodi\n",
      "662 delet\n",
      "663 entrust\n",
      "664 futur\n",
      "665 intellectu\n",
      "666 lose\n",
      "667 possibl\n",
      "668 prospect\n",
      "669 ruin\n",
      "670 rule\n",
      "671 ten\n",
      "672 thousand\n",
      "673 user\n",
      "674 corrupt\n",
      "675 easili\n",
      "676 wise\n",
      "677 head\n",
      "678 pull\n",
      "679 sand\n",
      "680 aaaand\n",
      "681 self\n",
      "682 slowli\n",
      "683 hahaha\n",
      "684 holi\n",
      "685 humorectomi\n",
      "686 revers\n",
      "687 profil\n",
      "688 weeni\n",
      "689 doggi\n",
      "690 ear\n",
      "691 what\n",
      "692 upgrad\n",
      "693 cramer\n",
      "694 jimmi\n",
      "695 notoneofus\n",
      "696 straddl\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.dictionary.Dictionary(apple_sample.comments)\n",
    "\n",
    "count = 0\n",
    "for key, value in sorted(dictionary.iteritems(),reverse=False):\n",
    "    print key,value\n",
    "    count+=1\n",
    "    if count>1000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out tokens that appear in \n",
    "# a. less than 21 documents\n",
    "# b. above 0.5 documents\n",
    "# c. keep the first 600 most frequent tokens\n",
    "dictionary.filter_extremes(keep_n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 market\n",
      "1 peopl\n",
      "2 look\n",
      "3 call\n",
      "4 time\n",
      "5 appl\n",
      "6 compani\n",
      "7 nike\n",
      "8 right\n",
      "9 work\n",
      "10 go\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for key, value in sorted(dictionary.iteritems(), reverse=False):\n",
    "    print key,value\n",
    "    count+=1\n",
    "    if count>10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>commentsUrl</th>\n",
       "      <th>comments</th>\n",
       "      <th>none</th>\n",
       "      <th>date</th>\n",
       "      <th>date_str</th>\n",
       "      <th>linkFlair</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://old.reddit.com/r/investing/comments/b6...</td>\n",
       "      <td>As a fellow Kiwi  chur bol</td>\n",
       "      <td>NaN</td>\n",
       "      <td>datetime.datetime(2019, 3, 27, 7, 4, 38, 990153)</td>\n",
       "      <td>'2019-03-27'</td>\n",
       "      <td>ne</td>\n",
       "      <td>8</td>\n",
       "      <td>investing</td>\n",
       "      <td>Those looking into investing in New Zealand Sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://old.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>Shooting for  60  this year   I only got to  4...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>datetime.datetime(2019, 3, 27, 9, 5, 32, 423850)</td>\n",
       "      <td>'2019-03-27'</td>\n",
       "      <td>ne</td>\n",
       "      <td>0</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>Research Survey WSB tell us your return expect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://old.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>Returns  What returns</td>\n",
       "      <td>NaN</td>\n",
       "      <td>datetime.datetime(2019, 3, 27, 9, 5, 32, 423850)</td>\n",
       "      <td>'2019-03-27'</td>\n",
       "      <td>ne</td>\n",
       "      <td>0</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>Research Survey WSB tell us your return expect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://old.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>I hope</td>\n",
       "      <td>NaN</td>\n",
       "      <td>datetime.datetime(2019, 3, 27, 12, 20, 32, 15...</td>\n",
       "      <td>'2019-03-27'</td>\n",
       "      <td>ne</td>\n",
       "      <td>0</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>Will the Pound crash after Brexit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://old.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>Worked all summer break from college and saved...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>datetime.datetime(2019, 3, 27, 12, 21, 2, 436...</td>\n",
       "      <td>'2019-03-27'</td>\n",
       "      <td>ne</td>\n",
       "      <td>3</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>2500 Mac Plus or AAPL stock 9 1 1989</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         commentsUrl  \\\n",
       "0  https://old.reddit.com/r/investing/comments/b6...   \n",
       "1  https://old.reddit.com/r/wallstreetbets/commen...   \n",
       "2  https://old.reddit.com/r/wallstreetbets/commen...   \n",
       "3  https://old.reddit.com/r/wallstreetbets/commen...   \n",
       "4  https://old.reddit.com/r/wallstreetbets/commen...   \n",
       "\n",
       "                                            comments none  \\\n",
       "0                      As a fellow Kiwi  chur bol     NaN   \n",
       "1  Shooting for  60  this year   I only got to  4...  NaN   \n",
       "2                          Returns  What returns      NaN   \n",
       "3                                          I hope     NaN   \n",
       "4  Worked all summer break from college and saved...  NaN   \n",
       "\n",
       "                                                date      date_str linkFlair  \\\n",
       "0   datetime.datetime(2019, 3, 27, 7, 4, 38, 990153)  '2019-03-27'        ne   \n",
       "1   datetime.datetime(2019, 3, 27, 9, 5, 32, 423850)  '2019-03-27'        ne   \n",
       "2   datetime.datetime(2019, 3, 27, 9, 5, 32, 423850)  '2019-03-27'        ne   \n",
       "3   datetime.datetime(2019, 3, 27, 12, 20, 32, 15...  '2019-03-27'        ne   \n",
       "4   datetime.datetime(2019, 3, 27, 12, 21, 2, 436...  '2019-03-27'        ne   \n",
       "\n",
       "   score         subreddit                                              title  \n",
       "0      8        investing   Those looking into investing in New Zealand Sh...  \n",
       "1      0   wallstreetbets   Research Survey WSB tell us your return expect...  \n",
       "2      0   wallstreetbets   Research Survey WSB tell us your return expect...  \n",
       "3      0   wallstreetbets                  Will the Pound crash after Brexit   \n",
       "4      3   wallstreetbets               2500 Mac Plus or AAPL stock 9 1 1989   "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Bears of Reddit What is your reasoning to believe in an upcoming bear market                                                                                                                        333\n",
       "Lyft is wildly unprofitable and lost 911 million last year '4 here 's how other unprofitable companies fared after they went public                                                                 322\n",
       "After Two Of The Greatest Bull Markets In U S History Why Are Boomers So Broke                                                                                                                      312\n",
       "Zillow 's House Flipping Averages An Abysmal 0 6 Per Flip                                                                                                                                           296\n",
       "How long have you been holding out for a market crash                                                                                                                                               293\n",
       "The Longest U S Bull Market Has Failed to Fix the Nation 's Public Pensions                                                                                                                         291\n",
       "Apple announced its Apple TV product Shares down 1 21 Meanwhile Roku up 4 68 and Netflix up 1 45                                                                                                    287\n",
       "What company 's stock are you definitely not buying Why                                                                                                                                             281\n",
       "Now that Lyft is planning an IPO of 20 billion Uber 100 120 billion Car rental stocks are in the dumps                                                                                              280\n",
       "Investing in REITs as an alternative to rental properties                                                                                                                                           278\n",
       "CONDOR GANG CAW CAW CAW                                                                                                                                                                             273\n",
       "Pinterest sets IPO terms below last private valuation                                                                                                                                               270\n",
       "Bull Bear Theta Rap Battle                                                                                                                                                                          269\n",
       "Mcdonald s buys Dynamic Yield for 300 million                                                                                                                                                       263\n",
       "Random discussion thread Anything goes                                                                                                                                                              262\n",
       "Nightly Trading Discussion March 21 22                                                                                                                                                              259\n",
       "Personal bankruptcy imminent boys                                                                                                                                                                   259\n",
       "Bank of America says it will make 5 billion in mortgages to low and moderate income borrowers                                                                                                       253\n",
       "Bitcoin up 15 in the last 2 hours                                                                                                                                                                   252\n",
       "AAPL Apple Special Event Announcement Megathread March 25 2019                                                                                                                                      251\n",
       "Nightly Trading Discussion March 25 26                                                                                                                                                              249\n",
       "SoftBank CEO Masayoshi Son says he offered Jeff Bezos 100M for a 30 percent stake in Amazon during its early years Son notes he says he was unable to secure the deal because Bezos wanted 130M     247\n",
       "Warren Buffett Banks will be worth more money 10 years from now                                                                                                                                     246\n",
       "750 000 to invest                                                                                                                                                                                   245\n",
       "The US Treasury is going to add an additional 11B to European products in countermeasures towards Airbus subsidies negatively affecting the US                                                      242\n",
       "Jeff Bezos to keep 75 percent of couple 's Amazon stock after finalizing divorce                                                                                                                    241\n",
       "Daily Discussion April 04                                                                                                                                                                           240\n",
       "Bank of America is raising its minimum wage for employees to 20 an hour                                                                                                                             239\n",
       "Exclusive Uber plans to sell around 10 billion worth of stock in IPO source                                                                                                                         239\n",
       "Why is JPM rapidly declining                                                                                                                                                                        238\n",
       "                                                                                                                                                                                                   ... \n",
       "Boeing issues software fix                                                                                                                                                                            1\n",
       "WSB is top trend on Twitter today YachtCocaineProstitutes                                                                                                                                             1\n",
       "Pulmatrix Inc NASDAQ PULM enters into binding term sheet with Cipla Technologies LLC                                                                                                                  1\n",
       "Thoughts on PRMTX                                                                                                                                                                                     1\n",
       "Leading Cannabis Companies to Embrace Artificial Intelligence AI to Significantly Boost Global Sales                                                                                                  1\n",
       "Hedge funds are loading up to bet against Lyft                                                                                                                                                        1\n",
       "I know DELL is in the news now but MSFT did it first invest                                                                                                                                           1\n",
       "BEMG Reports Record Breaking Annual Revenues of 1 057 837 for 2018 in Comparison to 962 997 for 2017 and 764 342 for 2016 and is Poised for Rapid Expansion in 2019                                   1\n",
       "Discord                                                                                                                                                                                               1\n",
       "Best products for traders Interviews with the world s best traders and investors                                                                                                                      1\n",
       "My Advice On How To Find Penny Stocks For Day Trading Over 100 Return On This Trade                                                                                                                   1\n",
       "DarioHealth Reports Fourth Quarter and Year End 2018 Record Results                                                                                                                                   1\n",
       "The Most Important Quality for an Investment Manager by Warren Buffet                                                                                                                                 1\n",
       "Jack Black is all in on Tesla                                                                                                                                                                         1\n",
       "Looks like they will solve the spiking aircraft problem soon Calls                                                                                                                                    1\n",
       "CERN is on the rise                                                                                                                                                                                   1\n",
       "MVIS                                                                                                                                                                                                  1\n",
       "SODE Acquires First Bitcoin Shopping Center In App Store                                                                                                                                              1\n",
       "Your opinion on eit un                                                                                                                                                                                1\n",
       "SNRG Just uplisted to OTCQB Featured along side CannTrust Holdings Cannabis                                                                                                                           1\n",
       "YouTube Channel that shows Level Two of daily runners                                                                                                                                                 1\n",
       "Looking for offline charts for android                                                                                                                                                                1\n",
       "Gap JCPenney Victoria s Secret Foot Locker 465 stores closures in 48 hours                                                                                                                            1\n",
       "My Stock Tracker                                                                                                                                                                                      1\n",
       "Like a cyclist wearing a blindfold caution rules global stock markets drifting lower this morning                                                                                                     1\n",
       "How GE built up and wrote down 22 billion in assets                                                                                                                                                   1\n",
       "AERGO                                                                                                                                                                                                 1\n",
       "Big medical bet Why this Philly area pharma company 's shares jumped 65 percent in a week                                                                                                             1\n",
       "Technical Analysis of Stocks and Commodities April Added                                                                                                                                              1\n",
       "InteractiveBrokers Monthly Activity Fee and the Average Equity Balance limit of USD 2 000                                                                                                             1\n",
       "Name: title, Length: 2365, dtype: int64"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.title.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>commentsUrl</th>\n",
       "      <th>comments</th>\n",
       "      <th>none</th>\n",
       "      <th>date</th>\n",
       "      <th>date_str</th>\n",
       "      <th>linkFlair</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2759</th>\n",
       "      <td>https://old.reddit.com/r/investing/comments/b6...</td>\n",
       "      <td>Article   a href   https   www reuters com art...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>datetime.datetime(2019, 3, 28, 9, 59, 20, 217...</td>\n",
       "      <td>'2019-03-28'</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>429</td>\n",
       "      <td>investing</td>\n",
       "      <td>Mcdonald s buys Dynamic Yield for 300 million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2760</th>\n",
       "      <td>https://old.reddit.com/r/investing/comments/b6...</td>\n",
       "      <td>It s not mentioned in the article but they wil...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>datetime.datetime(2019, 3, 28, 9, 59, 20, 217...</td>\n",
       "      <td>'2019-03-28'</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>429</td>\n",
       "      <td>investing</td>\n",
       "      <td>Mcdonald s buys Dynamic Yield for 300 million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2761</th>\n",
       "      <td>https://old.reddit.com/r/investing/comments/b6...</td>\n",
       "      <td>I think the big metric to watch to see how wel...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>datetime.datetime(2019, 3, 28, 9, 59, 20, 217...</td>\n",
       "      <td>'2019-03-28'</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>429</td>\n",
       "      <td>investing</td>\n",
       "      <td>Mcdonald s buys Dynamic Yield for 300 million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2762</th>\n",
       "      <td>https://old.reddit.com/r/investing/comments/b6...</td>\n",
       "      <td>Anecdotal  but people feel more comfortable ad...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>datetime.datetime(2019, 3, 28, 9, 59, 20, 217...</td>\n",
       "      <td>'2019-03-28'</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>429</td>\n",
       "      <td>investing</td>\n",
       "      <td>Mcdonald s buys Dynamic Yield for 300 million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2763</th>\n",
       "      <td>https://old.reddit.com/r/investing/comments/b6...</td>\n",
       "      <td>So if its cold and dreary out their drive thro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>datetime.datetime(2019, 3, 28, 9, 59, 20, 217...</td>\n",
       "      <td>'2019-03-28'</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>429</td>\n",
       "      <td>investing</td>\n",
       "      <td>Mcdonald s buys Dynamic Yield for 300 million</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            commentsUrl  \\\n",
       "2759  https://old.reddit.com/r/investing/comments/b6...   \n",
       "2760  https://old.reddit.com/r/investing/comments/b6...   \n",
       "2761  https://old.reddit.com/r/investing/comments/b6...   \n",
       "2762  https://old.reddit.com/r/investing/comments/b6...   \n",
       "2763  https://old.reddit.com/r/investing/comments/b6...   \n",
       "\n",
       "                                               comments none  \\\n",
       "2759  Article   a href   https   www reuters com art...  NaN   \n",
       "2760  It s not mentioned in the article but they wil...  NaN   \n",
       "2761  I think the big metric to watch to see how wel...  NaN   \n",
       "2762  Anecdotal  but people feel more comfortable ad...  NaN   \n",
       "2763  So if its cold and dreary out their drive thro...  NaN   \n",
       "\n",
       "                                                   date      date_str  \\\n",
       "2759   datetime.datetime(2019, 3, 28, 9, 59, 20, 217...  '2019-03-28'   \n",
       "2760   datetime.datetime(2019, 3, 28, 9, 59, 20, 217...  '2019-03-28'   \n",
       "2761   datetime.datetime(2019, 3, 28, 9, 59, 20, 217...  '2019-03-28'   \n",
       "2762   datetime.datetime(2019, 3, 28, 9, 59, 20, 217...  '2019-03-28'   \n",
       "2763   datetime.datetime(2019, 3, 28, 9, 59, 20, 217...  '2019-03-28'   \n",
       "\n",
       "        linkFlair  score    subreddit  \\\n",
       "2759  Discussion     429   investing    \n",
       "2760  Discussion     429   investing    \n",
       "2761  Discussion     429   investing    \n",
       "2762  Discussion     429   investing    \n",
       "2763  Discussion     429   investing    \n",
       "\n",
       "                                               title  \n",
       "2759  Mcdonald s buys Dynamic Yield for 300 million   \n",
       "2760  Mcdonald s buys Dynamic Yield for 300 million   \n",
       "2761  Mcdonald s buys Dynamic Yield for 300 million   \n",
       "2762  Mcdonald s buys Dynamic Yield for 300 million   \n",
       "2763  Mcdonald s buys Dynamic Yield for 300 million   "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data.title.str.contains(\"Mcdonald s buys Dynamic Yield for 300 million\")].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'worth',\n",
       " u'consid',\n",
       " u'appl',\n",
       " u'price',\n",
       " u'multipl',\n",
       " u'compar',\n",
       " u'tech',\n",
       " u'giant',\n",
       " u'especi',\n",
       " u'cook',\n",
       " u'push',\n",
       " u'compani',\n",
       " u'servic',\n",
       " u'proport',\n",
       " u'increas',\n",
       " u'servic',\n",
       " u'subscript',\n",
       " u'base',\n",
       " u'revenu',\n",
       " u'push',\n",
       " u'appl',\n",
       " u'valu',\n",
       " u'multipl',\n",
       " u'higher',\n",
       " u'simpli',\n",
       " u'hardwar',\n",
       " u'base',\n",
       " u'revenu']"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apple_sample.comments.reset_index().comments[23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "161"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dictionary for each document indicating how many times a word appears, save this to 'bow_corpus'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in apple_sample.comments]\n",
    "bow_18445 = bow_corpus[23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 5, appl, appears 2 times.\n",
      "Word 6, compani, appears 1 times.\n"
     ]
    }
   ],
   "source": [
    "# checking how often \n",
    "for i in range(len(bow_18445)):\n",
    "    print \"Word {}, {}, appears {} times.\".format(bow_18445[i][0],\n",
    "                                                  dictionary[bow_18445[i][0]],\n",
    "                                                             bow_18445[i][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a tf-idf model (term frequency - inverted document frequency) model and save it to tf-idf, apply a transformation to the corpus and call it corpus_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tfidf = gensim.models.TfidfModel(bow_corpus) # fitting the model\n",
    "corpus_tfidf = tfidf[bow_corpus] # apply the model to all documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.7238632483756052), (1, 0.6899434742434462)]\n"
     ]
    }
   ],
   "source": [
    "# previewing tf-idf scores for the first document\n",
    "from pprint import pprint\n",
    "\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the LDA Model with Bag of Words dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0,\n",
      " Words:0.279*\"appl\" + 0.237*\"read\" + 0.169*\"compani\" + 0.135*\"work\" + 0.065*\"nike\" + 0.005*\"fuck\" + 0.005*\"good\" + 0.005*\"look\" + 0.005*\"accept\" + 0.005*\"mod\"\n",
      "Topic: 1,\n",
      " Words:0.177*\"appl\" + 0.166*\"accept\" + 0.140*\"href\" + 0.129*\"nike\" + 0.077*\"market\" + 0.077*\"love\" + 0.067*\"right\" + 0.041*\"think\" + 0.041*\"go\" + 0.014*\"compani\"\n",
      "Topic: 2,\n",
      " Words:0.295*\"put\" + 0.187*\"call\" + 0.149*\"love\" + 0.096*\"better\" + 0.058*\"read\" + 0.050*\"go\" + 0.050*\"want\" + 0.018*\"href\" + 0.005*\"nike\" + 0.005*\"good\"\n",
      "Topic: 3,\n",
      " Words:0.166*\"compani\" + 0.145*\"go\" + 0.096*\"nike\" + 0.093*\"power\" + 0.068*\"look\" + 0.047*\"appl\" + 0.028*\"think\" + 0.028*\"like\" + 0.027*\"peopl\" + 0.027*\"mod\"\n",
      "Topic: 4,\n",
      " Words:0.137*\"post\" + 0.107*\"power\" + 0.093*\"mod\" + 0.088*\"need\" + 0.083*\"fuck\" + 0.082*\"peopl\" + 0.057*\"want\" + 0.051*\"like\" + 0.038*\"better\" + 0.038*\"abl\"\n",
      "Topic: 5,\n",
      " Words:0.247*\"mod\" + 0.189*\"like\" + 0.120*\"time\" + 0.096*\"mean\" + 0.072*\"peopl\" + 0.048*\"accept\" + 0.040*\"right\" + 0.038*\"think\" + 0.038*\"better\" + 0.023*\"nike\"\n",
      "Topic: 6,\n",
      " Words:0.361*\"look\" + 0.274*\"nike\" + 0.145*\"think\" + 0.027*\"read\" + 0.027*\"love\" + 0.007*\"appl\" + 0.007*\"good\" + 0.007*\"mod\" + 0.007*\"href\" + 0.007*\"put\"\n",
      "Topic: 7,\n",
      " Words:0.156*\"mod\" + 0.137*\"power\" + 0.103*\"href\" + 0.103*\"mean\" + 0.064*\"read\" + 0.047*\"post\" + 0.047*\"time\" + 0.047*\"peopl\" + 0.037*\"good\" + 0.037*\"abl\"\n",
      "Topic: 8,\n",
      " Words:0.247*\"good\" + 0.181*\"work\" + 0.064*\"fuck\" + 0.064*\"market\" + 0.064*\"post\" + 0.064*\"abl\" + 0.040*\"nike\" + 0.040*\"appl\" + 0.030*\"call\" + 0.026*\"put\"\n",
      "Topic: 9,\n",
      " Words:0.246*\"money\" + 0.111*\"need\" + 0.108*\"right\" + 0.100*\"want\" + 0.072*\"market\" + 0.072*\"compani\" + 0.055*\"good\" + 0.044*\"abl\" + 0.044*\"go\" + 0.016*\"read\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print \"Topic: {},\\n Words:{}\".format(idx,topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The topics using Bag of Words technique are a bit hard to determine, in this case we'll attempt to train LDA using TF-IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0,\n",
      " Words:0.210*\"money\" + 0.155*\"appl\" + 0.124*\"compani\" + 0.123*\"like\" + 0.083*\"need\" + 0.071*\"want\" + 0.055*\"put\" + 0.044*\"market\" + 0.044*\"abl\" + 0.016*\"read\"\n",
      "Topic: 1,\n",
      " Words:0.203*\"power\" + 0.178*\"mod\" + 0.103*\"href\" + 0.068*\"love\" + 0.060*\"time\" + 0.053*\"post\" + 0.053*\"peopl\" + 0.035*\"good\" + 0.035*\"mean\" + 0.035*\"read\"\n",
      "Topic: 2,\n",
      " Words:0.152*\"look\" + 0.133*\"post\" + 0.113*\"good\" + 0.086*\"market\" + 0.086*\"peopl\" + 0.058*\"like\" + 0.058*\"mod\" + 0.030*\"better\" + 0.030*\"think\" + 0.030*\"accept\"\n",
      "Topic: 3,\n",
      " Words:0.201*\"mod\" + 0.164*\"mean\" + 0.097*\"time\" + 0.097*\"think\" + 0.073*\"like\" + 0.073*\"peopl\" + 0.058*\"look\" + 0.043*\"read\" + 0.038*\"accept\" + 0.038*\"right\"\n",
      "Topic: 4,\n",
      " Words:0.227*\"href\" + 0.206*\"good\" + 0.102*\"read\" + 0.102*\"post\" + 0.081*\"money\" + 0.050*\"fuck\" + 0.029*\"work\" + 0.029*\"nike\" + 0.029*\"compani\" + 0.007*\"mod\"\n",
      "Topic: 5,\n",
      " Words:0.178*\"need\" + 0.141*\"compani\" + 0.078*\"post\" + 0.078*\"mod\" + 0.078*\"want\" + 0.078*\"power\" + 0.054*\"nike\" + 0.052*\"better\" + 0.043*\"mean\" + 0.041*\"right\"\n",
      "Topic: 6,\n",
      " Words:0.317*\"nike\" + 0.278*\"appl\" + 0.076*\"market\" + 0.066*\"work\" + 0.051*\"fuck\" + 0.040*\"think\" + 0.040*\"right\" + 0.040*\"abl\" + 0.014*\"mod\" + 0.009*\"read\"\n",
      "Topic: 7,\n",
      " Words:0.260*\"read\" + 0.223*\"fuck\" + 0.133*\"love\" + 0.095*\"better\" + 0.058*\"good\" + 0.058*\"want\" + 0.058*\"work\" + 0.005*\"appl\" + 0.005*\"put\" + 0.005*\"href\"\n",
      "Topic: 8,\n",
      " Words:0.164*\"put\" + 0.160*\"call\" + 0.156*\"accept\" + 0.113*\"go\" + 0.075*\"right\" + 0.066*\"work\" + 0.044*\"want\" + 0.035*\"like\" + 0.035*\"market\" + 0.034*\"need\"\n",
      "Topic: 9,\n",
      " Words:0.322*\"right\" + 0.196*\"need\" + 0.018*\"read\" + 0.018*\"nike\" + 0.018*\"appl\" + 0.018*\"put\" + 0.018*\"fuck\" + 0.018*\"href\" + 0.018*\"love\" + 0.018*\"look\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print \"Topic: {},\\n Words:{}\".format(idx,topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['title'][384]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents['comments'][357]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It s worth considering Apple s price multiple compared to other tech giants   Especially now that Cook is pushing the company into services   A proportional increase in services subscription based revenue should push Apple s value multiple higher simply because it s non hardware based revenue     '"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apple_sample_raw.iloc[23].comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance evaluation of LDA models on processed documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.774991154671 \n",
      "Topic: 0.279*\"appl\" + 0.237*\"read\" + 0.169*\"compani\" + 0.135*\"work\" + 0.065*\"nike\" + 0.005*\"fuck\" + 0.005*\"good\" + 0.005*\"look\" + 0.005*\"accept\" + 0.005*\"mod\"\n",
      "\n",
      "Score: 0.0250041279942 \n",
      "Topic: 0.177*\"appl\" + 0.166*\"accept\" + 0.140*\"href\" + 0.129*\"nike\" + 0.077*\"market\" + 0.077*\"love\" + 0.067*\"right\" + 0.041*\"think\" + 0.041*\"go\" + 0.014*\"compani\"\n",
      "\n",
      "Score: 0.0250030867755 \n",
      "Topic: 0.166*\"compani\" + 0.145*\"go\" + 0.096*\"nike\" + 0.093*\"power\" + 0.068*\"look\" + 0.047*\"appl\" + 0.028*\"think\" + 0.028*\"like\" + 0.027*\"peopl\" + 0.027*\"mod\"\n",
      "\n",
      "Score: 0.0250010490417 \n",
      "Topic: 0.246*\"money\" + 0.111*\"need\" + 0.108*\"right\" + 0.100*\"want\" + 0.072*\"market\" + 0.072*\"compani\" + 0.055*\"good\" + 0.044*\"abl\" + 0.044*\"go\" + 0.016*\"read\"\n",
      "\n",
      "Score: 0.0250003878027 \n",
      "Topic: 0.247*\"good\" + 0.181*\"work\" + 0.064*\"fuck\" + 0.064*\"market\" + 0.064*\"post\" + 0.064*\"abl\" + 0.040*\"nike\" + 0.040*\"appl\" + 0.030*\"call\" + 0.026*\"put\"\n",
      "\n",
      "Score: 0.0250001251698 \n",
      "Topic: 0.247*\"mod\" + 0.189*\"like\" + 0.120*\"time\" + 0.096*\"mean\" + 0.072*\"peopl\" + 0.048*\"accept\" + 0.040*\"right\" + 0.038*\"think\" + 0.038*\"better\" + 0.023*\"nike\"\n",
      "\n",
      "Score: 0.0250000637025 \n",
      "Topic: 0.156*\"mod\" + 0.137*\"power\" + 0.103*\"href\" + 0.103*\"mean\" + 0.064*\"read\" + 0.047*\"post\" + 0.047*\"time\" + 0.047*\"peopl\" + 0.037*\"good\" + 0.037*\"abl\"\n",
      "\n",
      "Score: 0.0250000171363 \n",
      "Topic: 0.295*\"put\" + 0.187*\"call\" + 0.149*\"love\" + 0.096*\"better\" + 0.058*\"read\" + 0.050*\"go\" + 0.050*\"want\" + 0.018*\"href\" + 0.005*\"nike\" + 0.005*\"good\"\n",
      "\n",
      "Score: 0.0250000171363 \n",
      "Topic: 0.137*\"post\" + 0.107*\"power\" + 0.093*\"mod\" + 0.088*\"need\" + 0.083*\"fuck\" + 0.082*\"peopl\" + 0.057*\"want\" + 0.051*\"like\" + 0.038*\"better\" + 0.038*\"abl\"\n",
      "\n",
      "Score: 0.0250000171363 \n",
      "Topic: 0.361*\"look\" + 0.274*\"nike\" + 0.145*\"think\" + 0.027*\"read\" + 0.027*\"love\" + 0.007*\"appl\" + 0.007*\"good\" + 0.007*\"mod\" + 0.007*\"href\" + 0.007*\"put\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model[bow_corpus[23]], key=lambda tupl: -1*tupl[1]):\n",
    "    print \"Score: {} \\nTopic: {}\\n\".format(score, lda_model.print_topic(index,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance evaluation of LDA model using TF-IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Somebody needs to teach her how to blink  '"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents['comments'][23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'worth',\n",
       " u'consid',\n",
       " u'appl',\n",
       " u'price',\n",
       " u'multipl',\n",
       " u'compar',\n",
       " u'tech',\n",
       " u'giant',\n",
       " u'especi',\n",
       " u'cook',\n",
       " u'push',\n",
       " u'compani',\n",
       " u'servic',\n",
       " u'proport',\n",
       " u'increas',\n",
       " u'servic',\n",
       " u'subscript',\n",
       " u'base',\n",
       " u'revenu',\n",
       " u'push',\n",
       " u'appl',\n",
       " u'valu',\n",
       " u'multipl',\n",
       " u'higher',\n",
       " u'simpli',\n",
       " u'hardwar',\n",
       " u'base',\n",
       " u'revenu']"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apple_sample.reset_index().comments[23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"What 's the better buy Nike or apple \""
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apple_sample.reset_index().title[23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.774984657764 \n",
      "Topic: 0.210*\"money\" + 0.155*\"appl\" + 0.124*\"compani\" + 0.123*\"like\" + 0.083*\"need\" + 0.071*\"want\" + 0.055*\"put\" + 0.044*\"market\" + 0.044*\"abl\" + 0.016*\"read\"\n",
      "\n",
      "Score: 0.0250110812485 \n",
      "Topic: 0.317*\"nike\" + 0.278*\"appl\" + 0.076*\"market\" + 0.066*\"work\" + 0.051*\"fuck\" + 0.040*\"think\" + 0.040*\"right\" + 0.040*\"abl\" + 0.014*\"mod\" + 0.009*\"read\"\n",
      "\n",
      "Score: 0.0250033438206 \n",
      "Topic: 0.178*\"need\" + 0.141*\"compani\" + 0.078*\"post\" + 0.078*\"mod\" + 0.078*\"want\" + 0.078*\"power\" + 0.054*\"nike\" + 0.052*\"better\" + 0.043*\"mean\" + 0.041*\"right\"\n",
      "\n",
      "Score: 0.0250004287809 \n",
      "Topic: 0.201*\"mod\" + 0.164*\"mean\" + 0.097*\"time\" + 0.097*\"think\" + 0.073*\"like\" + 0.073*\"peopl\" + 0.058*\"look\" + 0.043*\"read\" + 0.038*\"accept\" + 0.038*\"right\"\n",
      "\n",
      "Score: 0.0250001735985 \n",
      "Topic: 0.227*\"href\" + 0.206*\"good\" + 0.102*\"read\" + 0.102*\"post\" + 0.081*\"money\" + 0.050*\"fuck\" + 0.029*\"work\" + 0.029*\"nike\" + 0.029*\"compani\" + 0.007*\"mod\"\n",
      "\n",
      "Score: 0.0250001419336 \n",
      "Topic: 0.164*\"put\" + 0.160*\"call\" + 0.156*\"accept\" + 0.113*\"go\" + 0.075*\"right\" + 0.066*\"work\" + 0.044*\"want\" + 0.035*\"like\" + 0.035*\"market\" + 0.034*\"need\"\n",
      "\n",
      "Score: 0.0250001102686 \n",
      "Topic: 0.152*\"look\" + 0.133*\"post\" + 0.113*\"good\" + 0.086*\"market\" + 0.086*\"peopl\" + 0.058*\"like\" + 0.058*\"mod\" + 0.030*\"better\" + 0.030*\"think\" + 0.030*\"accept\"\n",
      "\n",
      "Score: 0.0250000264496 \n",
      "Topic: 0.203*\"power\" + 0.178*\"mod\" + 0.103*\"href\" + 0.068*\"love\" + 0.060*\"time\" + 0.053*\"post\" + 0.053*\"peopl\" + 0.035*\"good\" + 0.035*\"mean\" + 0.035*\"read\"\n",
      "\n",
      "Score: 0.0250000264496 \n",
      "Topic: 0.260*\"read\" + 0.223*\"fuck\" + 0.133*\"love\" + 0.095*\"better\" + 0.058*\"good\" + 0.058*\"want\" + 0.058*\"work\" + 0.005*\"appl\" + 0.005*\"put\" + 0.005*\"href\"\n",
      "\n",
      "Score: 0.0250000264496 \n",
      "Topic: 0.322*\"right\" + 0.196*\"need\" + 0.018*\"read\" + 0.018*\"nike\" + 0.018*\"appl\" + 0.018*\"put\" + 0.018*\"fuck\" + 0.018*\"href\" + 0.018*\"love\" + 0.018*\"look\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model_tfidf[bow_corpus[23]], key=lambda tupl: -1*tupl[1]):\n",
    "    print \"Score: {} \\nTopic: {}\\n\".format(score, lda_model_tfidf.print_topic(index,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing LDA TF-IDF on unprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.549987435341\t Topic: 0.317*\"nike\" + 0.278*\"appl\" + 0.076*\"market\" + 0.066*\"work\" + 0.051*\"fuck\"\n",
      "Score: 0.050011895597\t Topic: 0.210*\"money\" + 0.155*\"appl\" + 0.124*\"compani\" + 0.123*\"like\" + 0.083*\"need\"\n",
      "Score: 0.0500003285706\t Topic: 0.201*\"mod\" + 0.164*\"mean\" + 0.097*\"time\" + 0.097*\"think\" + 0.073*\"like\"\n",
      "Score: 0.0500001981854\t Topic: 0.152*\"look\" + 0.133*\"post\" + 0.113*\"good\" + 0.086*\"market\" + 0.086*\"peopl\"\n",
      "Score: 0.0500000230968\t Topic: 0.203*\"power\" + 0.178*\"mod\" + 0.103*\"href\" + 0.068*\"love\" + 0.060*\"time\"\n",
      "Score: 0.0500000230968\t Topic: 0.227*\"href\" + 0.206*\"good\" + 0.102*\"read\" + 0.102*\"post\" + 0.081*\"money\"\n",
      "Score: 0.0500000230968\t Topic: 0.178*\"need\" + 0.141*\"compani\" + 0.078*\"post\" + 0.078*\"mod\" + 0.078*\"want\"\n",
      "Score: 0.0500000230968\t Topic: 0.260*\"read\" + 0.223*\"fuck\" + 0.133*\"love\" + 0.095*\"better\" + 0.058*\"good\"\n",
      "Score: 0.0500000230968\t Topic: 0.164*\"put\" + 0.160*\"call\" + 0.156*\"accept\" + 0.113*\"go\" + 0.075*\"right\"\n",
      "Score: 0.0500000230968\t Topic: 0.322*\"right\" + 0.196*\"need\" + 0.018*\"read\" + 0.018*\"nike\" + 0.018*\"appl\"\n"
     ]
    }
   ],
   "source": [
    "unseen_document = \"i'm selling apple\"\n",
    "bow_vector = dictionary.doc2bow(tokenization(unseen_document))\n",
    "for index, score in sorted(lda_model_tfidf[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model_tfidf.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_document = \"Microsoft\"\n",
    "bow_vector = dictionary.doc2bow(tokenization(unseen_document))\n",
    "for index, score in sorted(lda_model_tfidf[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model_tfidf.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unseen_document = \"Gold\"\n",
    "bow_vector = dictionary.doc2bow(tokenization(unseen_document))\n",
    "for index, score in sorted(lda_model_tfidf[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model_tfidf.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_document = \"Apple\"\n",
    "bow_vector = dictionary.doc2bow(tokenization(unseen_document))\n",
    "for index, score in sorted(lda_model_tfidf[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model_tfidf.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
