{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.corpora.Dictionary import Dictionary as d\n",
    "from gensim.corpora import MmCorpus\n",
    "from gensim.models.ldamodel import LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/selenacordona/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"training-data-2019-04-05.csv\")\n",
    "data_text= data_text.drop_duplicates(subset='comments', keep=\"last\").reset_index()\n",
    "data_text = data[['comments']]\n",
    "data_text['index'] = data_text.index\n",
    "documents = data_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>commentsUrl</th>\n",
       "      <th>comments</th>\n",
       "      <th>date</th>\n",
       "      <th>date_str</th>\n",
       "      <th>linkFlair</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://old.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>I think this selloff will bleed into next week...</td>\n",
       "      <td>datetime.datetime(2019, 3, 22, 23, 10, 1, 823...</td>\n",
       "      <td>'2019-03-22'</td>\n",
       "      <td>Gain</td>\n",
       "      <td>14</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>UPDATE it worked out</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://old.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>What if the sell off was    them    selling of...</td>\n",
       "      <td>datetime.datetime(2019, 3, 22, 23, 10, 1, 823...</td>\n",
       "      <td>'2019-03-22'</td>\n",
       "      <td>Gain</td>\n",
       "      <td>14</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>UPDATE it worked out</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://old.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>Nope  Saw someone buy puts at the ask on futur...</td>\n",
       "      <td>datetime.datetime(2019, 3, 22, 23, 10, 1, 823...</td>\n",
       "      <td>'2019-03-22'</td>\n",
       "      <td>Gain</td>\n",
       "      <td>14</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>UPDATE it worked out</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://old.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>This interface is super confusing  Just tell u...</td>\n",
       "      <td>datetime.datetime(2019, 3, 22, 23, 10, 1, 823...</td>\n",
       "      <td>'2019-03-22'</td>\n",
       "      <td>Gain</td>\n",
       "      <td>14</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>UPDATE it worked out</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://old.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>Yolo Microsoft put  Made another 13k on RH</td>\n",
       "      <td>datetime.datetime(2019, 3, 22, 23, 10, 1, 823...</td>\n",
       "      <td>'2019-03-22'</td>\n",
       "      <td>Gain</td>\n",
       "      <td>14</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>UPDATE it worked out</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         commentsUrl  \\\n",
       "0  https://old.reddit.com/r/wallstreetbets/commen...   \n",
       "1  https://old.reddit.com/r/wallstreetbets/commen...   \n",
       "2  https://old.reddit.com/r/wallstreetbets/commen...   \n",
       "3  https://old.reddit.com/r/wallstreetbets/commen...   \n",
       "4  https://old.reddit.com/r/wallstreetbets/commen...   \n",
       "\n",
       "                                            comments  \\\n",
       "0  I think this selloff will bleed into next week...   \n",
       "1  What if the sell off was    them    selling of...   \n",
       "2  Nope  Saw someone buy puts at the ask on futur...   \n",
       "3  This interface is super confusing  Just tell u...   \n",
       "4      Yolo Microsoft put  Made another 13k on RH      \n",
       "\n",
       "                                                date      date_str linkFlair  \\\n",
       "0   datetime.datetime(2019, 3, 22, 23, 10, 1, 823...  '2019-03-22'     Gain    \n",
       "1   datetime.datetime(2019, 3, 22, 23, 10, 1, 823...  '2019-03-22'     Gain    \n",
       "2   datetime.datetime(2019, 3, 22, 23, 10, 1, 823...  '2019-03-22'     Gain    \n",
       "3   datetime.datetime(2019, 3, 22, 23, 10, 1, 823...  '2019-03-22'     Gain    \n",
       "4   datetime.datetime(2019, 3, 22, 23, 10, 1, 823...  '2019-03-22'     Gain    \n",
       "\n",
       "   score         subreddit                  title  \n",
       "0     14   wallstreetbets   UPDATE it worked out   \n",
       "1     14   wallstreetbets   UPDATE it worked out   \n",
       "2     14   wallstreetbets   UPDATE it worked out   \n",
       "3     14   wallstreetbets   UPDATE it worked out   \n",
       "4     14   wallstreetbets   UPDATE it worked out   "
      ]
     },
     "execution_count": 691,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27961\n",
      "                                            comments  index\n",
      "0  I think this selloff will bleed into next week...      0\n",
      "1  What if the sell off was    them    selling of...      1\n",
      "2  Nope  Saw someone buy puts at the ask on futur...      2\n",
      "3  This interface is super confusing  Just tell u...      3\n",
      "4      Yolo Microsoft put  Made another 13k on RH         4\n"
     ]
    }
   ],
   "source": [
    "print len(documents)\n",
    "print documents[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-Processing:  Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/selenacordona/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 627,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2019)\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I thought this would make a difference, but it just adds random words to the topics...\n",
    "data_text.title = \\\n",
    "data_text.title.apply(lambda x: re.sub(r\"(\\d+)\", \\\n",
    "                                          lambda x: num2words.num2words(int(x.group(0))), x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function to run lemmatization and stemming on a given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization + lemmatization step\n",
    "def lemmatization(token):\n",
    "    w = WordNetLemmatizer()\n",
    "    sanitized_token = w.lemmatize(token)\n",
    "    #SnowballStemmer()\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    return stemmer.stem(sanitized_token)\n",
    "\n",
    "def tokenization(text):\n",
    "    result=[]\n",
    "    tokens = gensim.utils.simple_preprocess(text)\n",
    "    for token in tokens:\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and \\\n",
    "        len(token) > 3:\n",
    "            result.append(lemmatization(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_text = data_text.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_text = data_text[['comments']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'similar',\n",
       " u'general',\n",
       " u'posit',\n",
       " u'sentiment',\n",
       " u'knew',\n",
       " u'googl',\n",
       " u'game',\n",
       " u'stream',\n",
       " u'keynot',\n",
       " u'come',\n",
       " u'talk',\n",
       " u'server',\n",
       " u'hardwar',\n",
       " u'detail',\n",
       " u'pretti',\n",
       " u'strong',\n",
       " u'convict',\n",
       " u'involv',\n",
       " u'bought',\n",
       " u'heavi',\n",
       " u'easi']"
      ]
     },
     "execution_count": 634,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenization(data_text.comments[1035])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "['I', 'did', 'something', 'similar', 'with', 'AMD', '', 'it', '', 'generally', '', 'has', 'positive', 'sentiment', 'and', 'I', 'knew', 'Google', 's', 'game', 'streaming', 'keynote', 'was', 'coming', 'up', 'and', 'would', 'talk', 'server', 'hardware', 'details', '', 'since', 'I', 'had', 'a', 'pretty', 'strong', 'conviction', 'that', 'AMD', 'would', 'be', 'involved', 'I', 'bought', 'in', 'heavy', 'and', 'made', 'an', 'easy', '10', '', '', '', '']\n",
      " \n",
      "tokenized and lemmatized document: \n",
      "[u'similar', u'general', u'posit', u'sentiment', u'knew', u'googl', u'game', u'stream', u'keynot', u'come', u'talk', u'server', u'hardwar', u'detail', u'pretti', u'strong', u'convict', u'involv', u'bought', u'heavi', u'easi']\n"
     ]
    }
   ],
   "source": [
    "sample = documents[documents['index'] == 1035].values[0][0]\n",
    "\n",
    "print \"original document: \"\n",
    "words = []\n",
    "for word in sample.split(\" \"):\n",
    "    words.append(word)\n",
    "print words\n",
    "print \" \"\n",
    "print \"tokenized and lemmatized document: \"\n",
    "print tokenization(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    I think this selloff will bleed into next week...\n",
       "1    What if the sell off was    them    selling of...\n",
       "2    Nope  Saw someone buy puts at the ask on futur...\n",
       "3    This interface is super confusing  Just tell u...\n",
       "4        Yolo Microsoft put  Made another 13k on RH   \n",
       "5                                    nice  bro        \n",
       "6    What do you think of APPL calls before their M...\n",
       "7    Puts on everything  Down Friday means gap down...\n",
       "8                             Yolo puts  Seriously    \n",
       "9    I will close all calls monday morning and put ...\n",
       "Name: comments, dtype: object"
      ]
     },
     "execution_count": 636,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents['comments'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [think, selloff, bleed, week, look, selloff, l...\n",
       "1     [sell, sell, news, hour, later, mueller, report]\n",
       "2                  [nope, put, futur, think, contract]\n",
       "3    [interfac, super, confus, tell, posit, total, ...\n",
       "4                                    [yolo, microsoft]\n",
       "5                                               [nice]\n",
       "6    [think, appl, call, march, announc, servic, play]\n",
       "7    [put, friday, mean, monday, close, monday, ope...\n",
       "8                                 [yolo, put, serious]\n",
       "9                     [close, call, monday, morn, put]\n",
       "Name: comments, dtype: object"
      ]
     },
     "execution_count": 637,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_documents = documents['comments'].map(tokenization)\n",
    "processed_documents[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words dataset on documents "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the BoW dataset to return number of times a word appears in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 bleed\n",
      "1 come\n",
      "2 dumb\n",
      "3 fear\n",
      "4 gonna\n",
      "5 know\n",
      "6 like\n",
      "7 long\n",
      "8 look\n",
      "9 loom\n",
      "10 money\n",
      "11 recess\n",
      "12 right\n",
      "13 selloff\n",
      "14 think\n",
      "15 time\n",
      "16 week\n",
      "17 hour\n",
      "18 later\n",
      "19 mueller\n",
      "20 news\n",
      "21 report\n",
      "22 sell\n",
      "23 contract\n",
      "24 futur\n",
      "25 nope\n",
      "26 put\n",
      "27 confus\n",
      "28 gain\n",
      "29 interfac\n",
      "30 posit\n",
      "31 super\n",
      "32 tell\n",
      "33 total\n",
      "34 microsoft\n",
      "35 yolo\n",
      "36 nice\n",
      "37 announc\n",
      "38 appl\n",
      "39 call\n",
      "40 march\n",
      "41 play\n",
      "42 servic\n",
      "43 close\n",
      "44 friday\n",
      "45 low\n",
      "46 lower\n",
      "47 mean\n",
      "48 monday\n",
      "49 open\n",
      "50 test\n",
      "51 serious\n",
      "52 morn\n",
      "53 day\n",
      "54 flip\n",
      "55 indic\n",
      "56 peopl\n",
      "57 reliabl\n",
      "58 yeah\n",
      "59 gap\n",
      "60 technic\n",
      "61 comfort\n",
      "62 complac\n",
      "63 fomo\n",
      "64 histor\n",
      "65 pig\n",
      "66 real\n",
      "67 slaughter\n",
      "68 sold\n",
      "69 thought\n",
      "70 today\n",
      "71 chart\n",
      "72 csco\n",
      "73 cvna\n",
      "74 daili\n",
      "75 disgust\n",
      "76 grmn\n",
      "77 liter\n",
      "78 market\n",
      "79 overextend\n",
      "80 predict\n",
      "81 shop\n",
      "82 short\n",
      "83 way\n",
      "84 broke\n",
      "85 follow\n",
      "86 wrong\n",
      "87 set\n",
      "88 broseph\n",
      "89 rememb\n",
      "90 commiss\n",
      "91 lose\n",
      "92 correct\n",
      "93 etrad\n",
      "94 need\n",
      "95 option\n",
      "96 switch\n",
      "97 ameritrad\n",
      "98 better\n",
      "99 cost\n",
      "100 cover\n",
      "101 stock\n",
      "102 trade\n",
      "103 usual\n",
      "104 forward\n",
      "105 good\n",
      "106 manag\n",
      "107 msft\n",
      "108 pretti\n",
      "109 see\n",
      "110 spread\n",
      "111 condor\n",
      "112 iron\n",
      "113 sideway\n",
      "114 stay\n",
      "115 airlin\n",
      "116 boe\n",
      "117 cancel\n",
      "118 chick\n",
      "119 conspiraci\n",
      "120 govt\n",
      "121 hand\n",
      "122 hold\n",
      "123 indonesian\n",
      "124 order\n",
      "125 pocket\n",
      "126 price\n",
      "127 step\n",
      "128 joke\n",
      "129 mayb\n",
      "130 take\n",
      "131 tinfoil\n",
      "132 said\n",
      "133 yasssssssssss\n",
      "134 second\n",
      "135 titl\n",
      "136 loss\n",
      "137 took\n",
      "138 hope\n",
      "139 shit\n",
      "140 work\n",
      "141 missl\n",
      "142 want\n",
      "143 final\n",
      "144 page\n",
      "145 articl\n",
      "146 cnew\n",
      "147 differ\n",
      "148 gotta\n",
      "149 read\n",
      "150 summari\n",
      "151 terribl\n",
      "152 whistleblow\n",
      "153 autopilot\n",
      "154 document\n",
      "155 dysfunct\n",
      "156 fall\n",
      "157 leak\n",
      "158 major\n",
      "159 show\n",
      "160 fuck\n",
      "161 mother\n",
      "162 blow\n",
      "163 meant\n",
      "164 missil\n",
      "165 damn\n",
      "166 aapl\n",
      "167 bot\n",
      "168 bother\n",
      "169 headlin\n",
      "170 mark\n",
      "171 medium\n",
      "172 post\n",
      "173 question\n",
      "174 start\n",
      "175 stellar\n",
      "176 sure\n",
      "177 arriv\n",
      "178 bear\n",
      "179 buy\n",
      "180 instead\n",
      "181 bought\n",
      "182 combat\n",
      "183 drop\n",
      "184 quick\n",
      "185 strike\n",
      "186 wave\n",
      "187 worthless\n",
      "188 fella\n",
      "189 glad\n",
      "190 help\n",
      "191 horribl\n",
      "192 load\n",
      "193 offset\n",
      "194 uvxi\n",
      "195 tvix\n",
      "196 earth\n",
      "197 moon\n",
      "198 dota\n",
      "199 miss\n",
      "200 clever\n",
      "201 alien\n",
      "202 destroy\n",
      "203 documentari\n",
      "204 dude\n",
      "205 kill\n",
      "206 netflix\n",
      "207 night\n",
      "208 nuke\n",
      "209 sent\n",
      "210 shock\n",
      "211 smart\n",
      "212 space\n",
      "213 wait\n",
      "214 watch\n",
      "215 whaaaaaa\n",
      "216 wouldn\n",
      "217 interest\n",
      "218 sound\n",
      "219 action\n",
      "220 actual\n",
      "221 cater\n",
      "222 desir\n",
      "223 disast\n",
      "224 doom\n",
      "225 entertain\n",
      "226 fiction\n",
      "227 honest\n",
      "228 intent\n",
      "229 movi\n",
      "230 porn\n",
      "231 respect\n",
      "232 suppos\n",
      "233 theori\n",
      "234 thrill\n",
      "235 thriller\n",
      "236 unacknowledg\n",
      "237 udffb\n",
      "238 advanc\n",
      "239 appar\n",
      "240 autism\n",
      "241 form\n",
      "242 understand\n",
      "243 batteri\n",
      "244 mobil\n",
      "245 adida\n",
      "246 nike\n",
      "247 shoe\n",
      "248 sock\n",
      "249 wear\n",
      "250 blasphemi\n",
      "251 dump\n",
      "252 etf\n",
      "253 leverag\n",
      "254 stop\n",
      "255 assum\n",
      "256 bet\n",
      "257 lotto\n",
      "258 stupid\n",
      "259 ticket\n",
      "260 half\n",
      "261 high\n",
      "262 lost\n",
      "263 portfolio\n",
      "264 seen\n",
      "265 dip\n",
      "266 issu\n",
      "267 limit\n",
      "268 profit\n",
      "269 simpl\n",
      "270 make\n",
      "271 sens\n",
      "272 behavior\n",
      "273 ticker\n",
      "274 ntqqq\n",
      "275 nudow\n",
      "276 upro\n",
      "277 xdow\n",
      "278 xqqq\n",
      "279 xspi\n",
      "280 xltt\n",
      "281 dgaz\n",
      "282 month\n",
      "283 ugaz\n",
      "284 yinn\n",
      "285 held\n",
      "286 overnight\n",
      "287 panick\n",
      "288 say\n",
      "289 thing\n",
      "290 tripl\n",
      "291 matter\n",
      "292 big\n",
      "293 shouldv\n",
      "294 middl\n",
      "295 wasn\n",
      "296 feelsbad\n",
      "297 bread\n",
      "298 butter\n",
      "299 cyanid\n",
      "300 pain\n",
      "301 slow\n",
      "302 experi\n",
      "303 learn\n",
      "304 fundament\n",
      "305 deserv\n",
      "306 lmoa\n",
      "307 hat\n",
      "308 bank\n",
      "309 cap\n",
      "310 cheap\n",
      "311 chines\n",
      "312 commit\n",
      "313 href\n",
      "314 maga\n",
      "315 piec\n",
      "316 setup\n",
      "317 site\n",
      "318 wholesal\n",
      "319 downi\n",
      "320 uppi\n",
      "321 camp\n",
      "322 death\n",
      "323 fema\n",
      "324 thrown\n",
      "325 bore\n",
      "326 compani\n",
      "327 origin\n",
      "328 vintag\n",
      "329 capit\n",
      "330 paper\n",
      "331 plate\n",
      "332 tabl\n",
      "333 arrest\n",
      "334 crisi\n",
      "335 hous\n",
      "336 caught\n",
      "337 dumbass\n",
      "338 mail\n",
      "339 problem\n",
      "340 room\n",
      "341 smoke\n",
      "342 weed\n",
      "343 berni\n",
      "344 haha\n",
      "345 break\n",
      "346 possibl\n",
      "347 turn\n",
      "348 yesterday\n",
      "349 amazon\n",
      "350 cliff\n",
      "351 dog\n",
      "352 earn\n",
      "353 get\n",
      "354 huge\n",
      "355 trend\n",
      "356 opposit\n",
      "357 funni\n",
      "358 laugh\n",
      "359 direct\n",
      "360 weak\n",
      "361 exact\n",
      "362 haven\n",
      "363 kept\n",
      "364 singl\n",
      "365 strategi\n",
      "366 thank\n",
      "367 best\n",
      "368 wish\n",
      "369 abstract\n",
      "370 cash\n",
      "371 comeback\n",
      "372 hard\n",
      "373 number\n",
      "374 previous\n",
      "375 stori\n",
      "376 chees\n",
      "377 chucki\n",
      "378 feel\n",
      "379 robinhood\n",
      "380 token\n",
      "381 true\n",
      "382 eat\n",
      "383 microwav\n",
      "384 ramen\n",
      "385 concept\n",
      "386 head\n",
      "387 wrap\n",
      "388 affect\n",
      "389 certain\n",
      "390 date\n",
      "391 decay\n",
      "392 equiti\n",
      "393 gist\n",
      "394 go\n",
      "395 movement\n",
      "396 negat\n",
      "397 stuff\n",
      "398 that\n",
      "399 there\n",
      "400 under\n",
      "401 valu\n",
      "402 volatil\n",
      "403 exampl\n",
      "404 allow\n",
      "405 closer\n",
      "406 current\n",
      "407 eventu\n",
      "408 expir\n",
      "409 greater\n",
      "410 hedg\n",
      "411 intrins\n",
      "412 june\n",
      "413 lock\n",
      "414 move\n",
      "415 past\n",
      "416 purchas\n",
      "417 share\n",
      "418 somebodi\n",
      "419 tri\n",
      "420 wigt\n",
      "421 morningn\n",
      "422 term\n",
      "423 belief\n",
      "424 given\n",
      "425 lemm\n",
      "426 okay\n",
      "427 ownership\n",
      "428 period\n",
      "429 rise\n",
      "430 yield\n",
      "431 dollar\n",
      "432 exercis\n",
      "433 idea\n",
      "434 cours\n",
      "435 equal\n",
      "436 excersis\n",
      "437 expens\n",
      "438 gambl\n",
      "439 imgain\n",
      "440 lot\n",
      "441 resel\n",
      "442 trader\n",
      "443 coupl\n",
      "444 caus\n",
      "445 extrins\n",
      "446 longer\n",
      "447 doin\n",
      "448 abil\n",
      "449 belfort\n",
      "450 born\n",
      "451 guess\n",
      "452 innat\n",
      "453 jordan\n",
      "454 prodig\n",
      "455 walk\n",
      "456 magic\n",
      "457 risk\n",
      "458 pennystock\n",
      "459 went\n",
      "460 book\n",
      "461 deceiv\n",
      "462 favor\n",
      "463 underyli\n",
      "464 dipshit\n",
      "465 entitl\n",
      "466 googl\n",
      "467 account\n",
      "468 blew\n",
      "469 happen\n",
      "470 luckili\n",
      "471 commenc\n",
      "472 god\n",
      "473 plus\n",
      "474 prais\n",
      "475 return\n",
      "476 mind\n",
      "477 upsid\n",
      "478 autist\n",
      "479 cake\n",
      "480 happi\n",
      "481 gave\n",
      "482 continu\n",
      "483 decid\n",
      "484 deep\n",
      "485 excit\n",
      "486 gold\n",
      "487 ground\n",
      "488 infam\n",
      "489 insan\n",
      "490 irrate\n",
      "491 jackpot\n",
      "492 mega\n",
      "493 throw\n",
      "494 wonder\n",
      "495 fine\n",
      "496 rate\n",
      "497 bobbi\n",
      "498 dont\n",
      "499 evil\n",
      "500 ricki\n",
      "501 beer\n",
      "502 spit\n",
      "503 american\n",
      "504 inflat\n",
      "505 level\n",
      "506 overvaul\n",
      "507 prime\n",
      "508 stabl\n",
      "509 sweden\n",
      "510 grant\n",
      "511 loan\n",
      "512 love\n",
      "513 base\n",
      "514 central\n",
      "515 lord\n",
      "516 stapl\n",
      "517 world\n",
      "518 year\n",
      "519 free\n",
      "520 dead\n",
      "521 economi\n",
      "522 fest\n",
      "523 girl\n",
      "524 immigr\n",
      "525 mass\n",
      "526 rape\n",
      "527 sausag\n",
      "528 suitcas\n",
      "529 gtfo\n",
      "530 polit\n",
      "531 retard\n",
      "532 answer\n",
      "533 cuck\n",
      "534 dillema\n",
      "535 excus\n",
      "536 globalist\n",
      "537 liber\n",
      "538 mention\n",
      "539 moral\n",
      "540 race\n",
      "541 racism\n",
      "542 racist\n",
      "543 remain\n",
      "544 shore\n",
      "545 stuf\n",
      "546 vote\n",
      "547 wors\n",
      "548 imagin\n",
      "549 germani\n",
      "550 rang\n",
      "551 basic\n",
      "552 invest\n",
      "553 nmoney\n",
      "554 studi\n",
      "555 crash\n",
      "556 rough\n",
      "557 spike\n",
      "558 tank\n",
      "559 variabl\n",
      "560 fix\n",
      "561 fuckin\n",
      "562 chang\n",
      "563 state\n",
      "564 afford\n",
      "565 build\n",
      "566 citi\n",
      "567 construct\n",
      "568 extrem\n",
      "569 govern\n",
      "570 land\n",
      "571 point\n",
      "572 practic\n",
      "573 ridicul\n",
      "574 surpris\n",
      "575 taller\n",
      "576 unafford\n",
      "577 vancouv\n",
      "578 blame\n",
      "579 seattl\n",
      "580 silicon\n",
      "581 spin\n",
      "582 valley\n",
      "583 wheel\n",
      "584 approxim\n",
      "585 aren\n",
      "586 averag\n",
      "587 buyer\n",
      "588 condo\n",
      "589 constitut\n",
      "590 detach\n",
      "591 develop\n",
      "592 fact\n",
      "593 fee\n",
      "594 financ\n",
      "595 foreign\n",
      "596 industri\n",
      "597 involv\n",
      "598 natur\n",
      "599 permit\n",
      "600 place\n",
      "601 player\n",
      "602 project\n",
      "603 rental\n",
      "604 residenti\n",
      "605 speak\n",
      "606 stage\n",
      "607 stall\n",
      "608 subject\n",
      "609 tax\n",
      "610 tend\n",
      "611 unit\n",
      "612 whimsic\n",
      "613 canada\n",
      "614 main\n",
      "615 pauper\n",
      "616 reason\n",
      "617 rich\n",
      "618 suck\n",
      "619 tape\n",
      "620 zone\n",
      "621 quantum\n",
      "622 china\n",
      "623 gainz\n",
      "624 launder\n",
      "625 outsid\n",
      "626 area\n",
      "627 hong\n",
      "628 houston\n",
      "629 kong\n",
      "630 nimbi\n",
      "631 regul\n",
      "632 result\n",
      "633 urban\n",
      "634 apart\n",
      "635 bid\n",
      "636 core\n",
      "637 cram\n",
      "638 cycl\n",
      "639 downtown\n",
      "640 effect\n",
      "641 exponenti\n",
      "642 increas\n",
      "643 near\n",
      "644 properti\n",
      "645 roof\n",
      "646 toronto\n",
      "647 vicious\n",
      "648 will\n",
      "649 bedroom\n",
      "650 rent\n",
      "651 shitshow\n",
      "652 stuck\n",
      "653 thousand\n",
      "654 tini\n",
      "655 lawyer\n",
      "656 live\n",
      "657 salari\n",
      "658 sourc\n",
      "659 deposit\n",
      "660 benefit\n",
      "661 borrow\n",
      "662 bunch\n",
      "663 charg\n",
      "664 control\n",
      "665 decis\n",
      "666 econom\n",
      "667 lawn\n",
      "668 mess\n",
      "669 mow\n",
      "670 offer\n",
      "671 popul\n",
      "672 small\n",
      "673 timmi\n",
      "674 unfortun\n",
      "675 consid\n",
      "676 lesson\n",
      "677 life\n",
      "678 macro\n",
      "679 parent\n",
      "680 save\n",
      "681 sorri\n",
      "682 explain\n",
      "683 forc\n",
      "684 holder\n",
      "685 incentiv\n",
      "686 individu\n",
      "687 pas\n",
      "688 reserv\n",
      "689 store\n",
      "690 theoret\n",
      "691 shouldn\n",
      "692 stonk\n",
      "693 introduc\n",
      "694 probabl\n",
      "695 accur\n",
      "696 ledger\n",
      "697 monopoli\n",
      "698 produc\n",
      "699 stroke\n",
      "700 holi\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.dictionary.Dictionary(processed_documents)\n",
    "\n",
    "count = 0\n",
    "for key, value in sorted(dictionary.iteritems()):\n",
    "    print key,value\n",
    "    count+=1\n",
    "    if count>700:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out tokens that appear in \n",
    "# a. less than 21 documents\n",
    "# b. above 0.5 documents\n",
    "# c. keep the first 600 most frequent tokens\n",
    "dictionary.filter_extremes(no_below=21, no_above=0.5, keep_n=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "599 brand\n",
      "598 roth\n",
      "597 beat\n",
      "596 estat\n",
      "595 premium\n",
      "594 typic\n",
      "593 april\n",
      "592 boomer\n",
      "591 phone\n",
      "590 advic\n",
      "589 contribut\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for key, value in sorted(dictionary.iteritems(), reverse=True):\n",
    "    print key,value\n",
    "    count+=1\n",
    "    if count>10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Daily advice thread All questions about your personal situation should be asked here                                                                                                                                                                                                               614\n",
       "After Two Of The Greatest Bull Markets In U S History Why Are Boomers So Broke                                                                                                                                                                                                                     551\n",
       "Random discussion thread Anything goes                                                                                                                                                                                                                                                             301\n",
       "Zillow 's House Flipping Averages An Abysmal 0 6 Per Flip                                                                                                                                                                                                                                          299\n",
       "Apple announced its Apple TV product Shares down 1 21 Meanwhile Roku up 4 68 and Netflix up 1 45                                                                                                                                                                                                   289\n",
       "CONDOR GANG CAW CAW CAW                                                                                                                                                                                                                                                                            282\n",
       "What company 's stock are you definitely not buying Why                                                                                                                                                                                                                                            281\n",
       "Mcdonald s buys Dynamic Yield for 300 million                                                                                                                                                                                                                                                      265\n",
       "Nightly Trading Discussion March 25 26                                                                                                                                                                                                                                                             263\n",
       "Nightly Trading Discussion March 21 22                                                                                                                                                                                                                                                             259\n",
       "AAPL Apple Special Event Announcement Megathread March 25 2019                                                                                                                                                                                                                                     252\n",
       "Why is JPM rapidly declining                                                                                                                                                                                                                                                                       246\n",
       "Weekend Discussion Thread March 22 24 2019                                                                                                                                                                                                                                                         234\n",
       "Nightly Trading Discussion March 20 21                                                                                                                                                                                                                                                             232\n",
       "Daily Discussion March 18                                                                                                                                                                                                                                                                          228\n",
       "How would Steve Jobs react if he found out that 8 years after his death Apple product annoucement events will be used to launch a credit card                                                                                                                                                      228\n",
       "Currently filing 2018 tax return Because of the capital net loss of 13345 in 2018 I won t have to pay any additional federal taxes                                                                                                                                                                 227\n",
       "Apple Announces New Credit Card and Subscription Services                                                                                                                                                                                                                                          224\n",
       "Daily Discussion March 22                                                                                                                                                                                                                                                                          224\n",
       "Federal prosecutors announce charges against Michael Avenatti alleging he tried to extort Nike threatening to release damaging info about the company and told Nike attorneys if his demands were not met I ll go take ten billion dollars off your client s market cap I m not fucking around     219\n",
       "Weed stocks taking a downturn What are your opinions on its future prices                                                                                                                                                                                                                          219\n",
       "Daily Discussion March 27                                                                                                                                                                                                                                                                          218\n",
       "Daily Discussion Thread March 27 2019                                                                                                                                                                                                                                                              216\n",
       "Nightly Trading Discussion March 24 25                                                                                                                                                                                                                                                             214\n",
       "Daily Discussion March 25                                                                                                                                                                                                                                                                          213\n",
       "Daily Discussion March 19                                                                                                                                                                                                                                                                          211\n",
       "Many of you know me as the 900 to 55k guy I 've been missing since I went to college Now that I 'm about to graduate I 'm more free to yolo                                                                                                                                                        207\n",
       "What Are Your Moves Tomorrow March 27                                                                                                                                                                                                                                                              206\n",
       "Daily Discussion March 20                                                                                                                                                                                                                                                                          202\n",
       "Daily Discussion Thread March 25 2019                                                                                                                                                                                                                                                              201\n",
       "                                                                                                                                                                                                                                                                                                  ... \n",
       "Momentum Traders Watch List for 3 25 2019                                                                                                                                                                                                                                                            1\n",
       "The Most Important Quality for an Investment Manager by Warren Buffet                                                                                                                                                                                                                                1\n",
       "Will the Pound crash after Brexit                                                                                                                                                                                                                                                                    1\n",
       "Anyone know why ABMD is dipping                                                                                                                                                                                                                                                                      1\n",
       "Discord                                                                                                                                                                                                                                                                                              1\n",
       "PSIQ Huge News PSIQ to Receive Assignment of License to Prepare Land in Israel With the Intent to Grow Cultivate Distribute and Export Medical Cannabis                                                                                                                                              1\n",
       "UK broker recommendations that offer ISA                                                                                                                                                                                                                                                             1\n",
       "Investor Howard Marks discusses the market cycle and how to master it                                                                                                                                                                                                                                1\n",
       "Those looking into investing in New Zealand Share market NZX I have analysed the companies with wide economic moats to help you on your journey                                                                                                                                                      1\n",
       "Gap JCPenney Victoria s Secret Foot Locker 465 stores closures in 48 hours                                                                                                                                                                                                                           1\n",
       "Aurora Cannabis Aims To Exploit Mexico 's Recent Marijuana Legislation Other US Companies Join                                                                                                                                                                                                       1\n",
       "Fidelity Brokerage Link                                                                                                                                                                                                                                                                              1\n",
       "My Stock Tracker                                                                                                                                                                                                                                                                                     1\n",
       "Deviations POC and Value Area for Tuesday March 26 2019                                                                                                                                                                                                                                              1\n",
       "This is a really helpful site in terms of understanding a business s financial under the scope of a value investor                                                                                                                                                                                   1\n",
       "Looks like they will solve the spiking aircraft problem soon Calls                                                                                                                                                                                                                                   1\n",
       "ETFM                                                                                                                                                                                                                                                                                                 1\n",
       "Exclusive Uber plans to kick off IPO in April sources                                                                                                                                                                                                                                                1\n",
       "Looking for offline charts for android                                                                                                                                                                                                                                                               1\n",
       "Manage 2 000 000 Competitive Stock Market game Overnight Billionaire 2019 '                                                                                                                                                                                                                          1\n",
       "DarioHealth Reports Fourth Quarter and Year End 2018 Record Results                                                                                                                                                                                                                                  1\n",
       "I know DELL is in the news now but MSFT did it first invest                                                                                                                                                                                                                                          1\n",
       "Can anyone help with a good app or website they 'd recommend for accurate dividend history upcoming earnings or just stock data in general I have an iPhone If anyone could recommend something that 'd be awesome                                                                                   1\n",
       "Daily SPX TPO s 03 20 2019                                                                                                                                                                                                                                                                           1\n",
       "CNC Acquisition Powermove or not                                                                                                                                                                                                                                                                     1\n",
       "My buddy left the family business started a Value Investing Podcast Here s some stock analysis of Newell Brands NWL                                                                                                                                                                                  1\n",
       "How GE built up and wrote down 22 billion in assets                                                                                                                                                                                                                                                  1\n",
       "Deviations POC and Value Area for Monday March 25 2019                                                                                                                                                                                                                                               1\n",
       "TradeZero Pro V3 How to maximise the main window                                                                                                                                                                                                                                                     1\n",
       "Nasper s spinning off Tencent other holdings into Euronext listing                                                                                                                                                                                                                                   1\n",
       "Name: title, Length: 746, dtype: int64"
      ]
     },
     "execution_count": 641,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.title.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>commentsUrl</th>\n",
       "      <th>comments</th>\n",
       "      <th>date</th>\n",
       "      <th>date_str</th>\n",
       "      <th>linkFlair</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26437</th>\n",
       "      <td>https://old.reddit.com/r/investing/comments/b6...</td>\n",
       "      <td>Article   a href   https   www reuters com art...</td>\n",
       "      <td>datetime.datetime(2019, 3, 28, 9, 59, 20, 217...</td>\n",
       "      <td>'2019-03-28'</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>429</td>\n",
       "      <td>investing</td>\n",
       "      <td>Mcdonald s buys Dynamic Yield for 300 million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26438</th>\n",
       "      <td>https://old.reddit.com/r/investing/comments/b6...</td>\n",
       "      <td>It s not mentioned in the article but they wil...</td>\n",
       "      <td>datetime.datetime(2019, 3, 28, 9, 59, 20, 217...</td>\n",
       "      <td>'2019-03-28'</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>429</td>\n",
       "      <td>investing</td>\n",
       "      <td>Mcdonald s buys Dynamic Yield for 300 million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26439</th>\n",
       "      <td>https://old.reddit.com/r/investing/comments/b6...</td>\n",
       "      <td>I think the big metric to watch to see how wel...</td>\n",
       "      <td>datetime.datetime(2019, 3, 28, 9, 59, 20, 217...</td>\n",
       "      <td>'2019-03-28'</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>429</td>\n",
       "      <td>investing</td>\n",
       "      <td>Mcdonald s buys Dynamic Yield for 300 million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26440</th>\n",
       "      <td>https://old.reddit.com/r/investing/comments/b6...</td>\n",
       "      <td>Anecdotal  but people feel more comfortable ad...</td>\n",
       "      <td>datetime.datetime(2019, 3, 28, 9, 59, 20, 217...</td>\n",
       "      <td>'2019-03-28'</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>429</td>\n",
       "      <td>investing</td>\n",
       "      <td>Mcdonald s buys Dynamic Yield for 300 million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26441</th>\n",
       "      <td>https://old.reddit.com/r/investing/comments/b6...</td>\n",
       "      <td>So if its cold and dreary out their drive thro...</td>\n",
       "      <td>datetime.datetime(2019, 3, 28, 9, 59, 20, 217...</td>\n",
       "      <td>'2019-03-28'</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>429</td>\n",
       "      <td>investing</td>\n",
       "      <td>Mcdonald s buys Dynamic Yield for 300 million</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             commentsUrl  \\\n",
       "26437  https://old.reddit.com/r/investing/comments/b6...   \n",
       "26438  https://old.reddit.com/r/investing/comments/b6...   \n",
       "26439  https://old.reddit.com/r/investing/comments/b6...   \n",
       "26440  https://old.reddit.com/r/investing/comments/b6...   \n",
       "26441  https://old.reddit.com/r/investing/comments/b6...   \n",
       "\n",
       "                                                comments  \\\n",
       "26437  Article   a href   https   www reuters com art...   \n",
       "26438  It s not mentioned in the article but they wil...   \n",
       "26439  I think the big metric to watch to see how wel...   \n",
       "26440  Anecdotal  but people feel more comfortable ad...   \n",
       "26441  So if its cold and dreary out their drive thro...   \n",
       "\n",
       "                                                    date      date_str  \\\n",
       "26437   datetime.datetime(2019, 3, 28, 9, 59, 20, 217...  '2019-03-28'   \n",
       "26438   datetime.datetime(2019, 3, 28, 9, 59, 20, 217...  '2019-03-28'   \n",
       "26439   datetime.datetime(2019, 3, 28, 9, 59, 20, 217...  '2019-03-28'   \n",
       "26440   datetime.datetime(2019, 3, 28, 9, 59, 20, 217...  '2019-03-28'   \n",
       "26441   datetime.datetime(2019, 3, 28, 9, 59, 20, 217...  '2019-03-28'   \n",
       "\n",
       "         linkFlair  score    subreddit  \\\n",
       "26437  Discussion     429   investing    \n",
       "26438  Discussion     429   investing    \n",
       "26439  Discussion     429   investing    \n",
       "26440  Discussion     429   investing    \n",
       "26441  Discussion     429   investing    \n",
       "\n",
       "                                                title  \n",
       "26437  Mcdonald s buys Dynamic Yield for 300 million   \n",
       "26438  Mcdonald s buys Dynamic Yield for 300 million   \n",
       "26439  Mcdonald s buys Dynamic Yield for 300 million   \n",
       "26440  Mcdonald s buys Dynamic Yield for 300 million   \n",
       "26441  Mcdonald s buys Dynamic Yield for 300 million   "
      ]
     },
     "execution_count": 690,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data.title.str.contains(\"Mcdonald s buys Dynamic Yield for 300 million\")].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You ve lost money in stocks since Oct  2016    '"
      ]
     },
     "execution_count": 647,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents.comments[1843]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10093"
      ]
     },
     "execution_count": 643,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bow_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dictionary for each document indicating how many times a word appears, save this to 'bow_corpus'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_documents]\n",
    "bow_18445 = bow_corpus[1843]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 8, money, appears 1 times.\n",
      "Word 70, stock, appears 1 times.\n",
      "Word 140, lost, appears 1 times.\n"
     ]
    }
   ],
   "source": [
    "# checking how often \n",
    "for i in range(len(bow_18445)):\n",
    "    print \"Word {}, {}, appears {} times.\".format(bow_18445[i][0],\n",
    "                                                  dictionary[bow_18445[i][0]],\n",
    "                                                             bow_18445[i][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a tf-idf model (term frequency - inverted document frequency) model and save it to tf-idf, apply a transformation to the corpus and call it corpus_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tfidf = gensim.models.TfidfModel(bow_corpus) # fitting the model\n",
    "corpus_tfidf = tfidf[bow_corpus] # apply the model to all documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.2460471416858584),\n",
      " (1, 0.3743501837965858),\n",
      " (2, 0.3809654829088901),\n",
      " (3, 0.31328859695201416),\n",
      " (4, 0.20895227905708416),\n",
      " (5, 0.15448071769055557),\n",
      " (6, 0.21467153700736602),\n",
      " (7, 0.19464745234114117),\n",
      " (8, 0.190681302932228),\n",
      " (9, 0.270261835801707),\n",
      " (10, 0.22571363377346496),\n",
      " (11, 0.18355526665480057),\n",
      " (12, 0.3844783278201842),\n",
      " (13, 0.2503554854921157)]\n"
     ]
    }
   ],
   "source": [
    "# previewing tf-idf scores for the first document\n",
    "from pprint import pprint\n",
    "\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the LDA Model with Bag of Words dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0,\n",
      " Words:0.027*\"money\" + 0.024*\"compani\" + 0.024*\"like\" + 0.019*\"great\" + 0.018*\"good\" + 0.017*\"peopl\" + 0.014*\"hope\" + 0.012*\"start\" + 0.012*\"hous\" + 0.012*\"point\"\n",
      "Topic: 1,\n",
      " Words:0.071*\"like\" + 0.030*\"work\" + 0.016*\"peopl\" + 0.016*\"busi\" + 0.015*\"hard\" + 0.015*\"yeah\" + 0.015*\"idea\" + 0.014*\"think\" + 0.014*\"know\" + 0.014*\"mean\"\n",
      "Topic: 2,\n",
      " Words:0.054*\"year\" + 0.034*\"think\" + 0.030*\"compani\" + 0.028*\"market\" + 0.022*\"go\" + 0.017*\"month\" + 0.016*\"stock\" + 0.015*\"happen\" + 0.011*\"buy\" + 0.010*\"need\"\n",
      "Topic: 3,\n",
      " Words:0.043*\"thank\" + 0.038*\"time\" + 0.033*\"put\" + 0.032*\"call\" + 0.026*\"post\" + 0.019*\"hold\" + 0.018*\"bought\" + 0.016*\"think\" + 0.016*\"contract\" + 0.016*\"nice\"\n",
      "Topic: 4,\n",
      " Words:0.036*\"price\" + 0.023*\"make\" + 0.022*\"right\" + 0.018*\"valu\" + 0.017*\"loan\" + 0.013*\"invest\" + 0.013*\"option\" + 0.012*\"sens\" + 0.012*\"rate\" + 0.012*\"check\"\n",
      "Topic: 5,\n",
      " Words:0.039*\"long\" + 0.037*\"term\" + 0.027*\"short\" + 0.024*\"strong\" + 0.023*\"recess\" + 0.022*\"yield\" + 0.019*\"bond\" + 0.019*\"year\" + 0.017*\"loss\" + 0.017*\"rate\"\n",
      "Topic: 6,\n",
      " Words:0.046*\"money\" + 0.038*\"fuck\" + 0.025*\"account\" + 0.025*\"stock\" + 0.022*\"shit\" + 0.017*\"card\" + 0.017*\"news\" + 0.016*\"get\" + 0.016*\"want\" + 0.016*\"market\"\n",
      "Topic: 7,\n",
      " Words:0.048*\"appl\" + 0.036*\"peopl\" + 0.034*\"good\" + 0.023*\"thing\" + 0.023*\"like\" + 0.016*\"think\" + 0.014*\"servic\" + 0.013*\"probabl\" + 0.012*\"want\" + 0.012*\"edit\"\n",
      "Topic: 8,\n",
      " Words:0.068*\"trade\" + 0.030*\"invest\" + 0.027*\"href\" + 0.027*\"http\" + 0.027*\"market\" + 0.025*\"stock\" + 0.024*\"time\" + 0.021*\"read\" + 0.020*\"know\" + 0.017*\"nofollow\"\n",
      "Topic: 9,\n",
      " Words:0.058*\"look\" + 0.030*\"like\" + 0.023*\"share\" + 0.023*\"fund\" + 0.023*\"dividend\" + 0.018*\"market\" + 0.018*\"go\" + 0.017*\"stock\" + 0.017*\"risk\" + 0.014*\"come\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print \"Topic: {},\\n Words:{}\".format(idx,topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The topics using Bag of Words technique are a bit hard to determine, in this case we'll attempt to train LDA using TF-IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0,\n",
      " Words:0.025*\"fuck\" + 0.018*\"like\" + 0.018*\"market\" + 0.016*\"think\" + 0.016*\"year\" + 0.016*\"dividend\" + 0.014*\"loan\" + 0.013*\"return\" + 0.012*\"work\" + 0.012*\"better\"\n",
      "Topic: 1,\n",
      " Words:0.032*\"market\" + 0.032*\"stock\" + 0.025*\"long\" + 0.024*\"sell\" + 0.019*\"week\" + 0.017*\"hous\" + 0.016*\"today\" + 0.013*\"time\" + 0.012*\"go\" + 0.012*\"know\"\n",
      "Topic: 2,\n",
      " Words:0.051*\"compani\" + 0.025*\"know\" + 0.024*\"stock\" + 0.018*\"peopl\" + 0.015*\"think\" + 0.015*\"point\" + 0.013*\"thought\" + 0.013*\"mean\" + 0.012*\"like\" + 0.012*\"make\"\n",
      "Topic: 3,\n",
      " Words:0.029*\"money\" + 0.028*\"short\" + 0.027*\"http\" + 0.027*\"like\" + 0.024*\"href\" + 0.018*\"cash\" + 0.017*\"nofollow\" + 0.017*\"market\" + 0.017*\"fund\" + 0.016*\"hold\"\n",
      "Topic: 4,\n",
      " Words:0.029*\"year\" + 0.028*\"term\" + 0.023*\"long\" + 0.019*\"price\" + 0.019*\"call\" + 0.019*\"option\" + 0.018*\"go\" + 0.017*\"right\" + 0.016*\"thank\" + 0.016*\"like\"\n",
      "Topic: 5,\n",
      " Words:0.027*\"think\" + 0.026*\"like\" + 0.022*\"peopl\" + 0.015*\"go\" + 0.014*\"card\" + 0.013*\"price\" + 0.013*\"pretti\" + 0.012*\"market\" + 0.011*\"look\" + 0.011*\"rate\"\n",
      "Topic: 6,\n",
      " Words:0.052*\"time\" + 0.027*\"strong\" + 0.022*\"year\" + 0.013*\"like\" + 0.013*\"close\" + 0.013*\"post\" + 0.012*\"thank\" + 0.012*\"month\" + 0.011*\"sell\" + 0.011*\"mayb\"\n",
      "Topic: 7,\n",
      " Words:0.054*\"trade\" + 0.041*\"good\" + 0.024*\"bond\" + 0.022*\"market\" + 0.021*\"stock\" + 0.019*\"put\" + 0.017*\"look\" + 0.015*\"real\" + 0.014*\"time\" + 0.012*\"peopl\"\n",
      "Topic: 8,\n",
      " Words:0.036*\"like\" + 0.032*\"money\" + 0.015*\"month\" + 0.014*\"need\" + 0.013*\"market\" + 0.013*\"rate\" + 0.012*\"sell\" + 0.012*\"look\" + 0.011*\"stock\" + 0.011*\"think\"\n",
      "Topic: 9,\n",
      " Words:0.045*\"invest\" + 0.030*\"appl\" + 0.024*\"thing\" + 0.016*\"need\" + 0.015*\"like\" + 0.014*\"money\" + 0.012*\"read\" + 0.012*\"good\" + 0.011*\"year\" + 0.011*\"trade\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print \"Topic: {},\\n Words:{}\".format(idx,topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Glad I sold these yesterday before close Would ve been worthless if I still held '"
      ]
     },
     "execution_count": 656,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['title'][384]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'So  AMZN was at 1712 at Monday open and then acquired more than 100 points before Friday    '"
      ]
     },
     "execution_count": 657,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents['comments'][357]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance evaluation of LDA models on processed documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.819945335388 \n",
      "Topic: 0.043*\"thank\" + 0.038*\"time\" + 0.033*\"put\" + 0.032*\"call\" + 0.026*\"post\" + 0.019*\"hold\" + 0.018*\"bought\" + 0.016*\"think\" + 0.016*\"contract\" + 0.016*\"nice\"\n",
      "\n",
      "Score: 0.0200093649328 \n",
      "Topic: 0.027*\"money\" + 0.024*\"compani\" + 0.024*\"like\" + 0.019*\"great\" + 0.018*\"good\" + 0.017*\"peopl\" + 0.014*\"hope\" + 0.012*\"start\" + 0.012*\"hous\" + 0.012*\"point\"\n",
      "\n",
      "Score: 0.0200089477003 \n",
      "Topic: 0.048*\"appl\" + 0.036*\"peopl\" + 0.034*\"good\" + 0.023*\"thing\" + 0.023*\"like\" + 0.016*\"think\" + 0.014*\"servic\" + 0.013*\"probabl\" + 0.012*\"want\" + 0.012*\"edit\"\n",
      "\n",
      "Score: 0.0200087055564 \n",
      "Topic: 0.046*\"money\" + 0.038*\"fuck\" + 0.025*\"account\" + 0.025*\"stock\" + 0.022*\"shit\" + 0.017*\"card\" + 0.017*\"news\" + 0.016*\"get\" + 0.016*\"want\" + 0.016*\"market\"\n",
      "\n",
      "Score: 0.0200071353465 \n",
      "Topic: 0.039*\"long\" + 0.037*\"term\" + 0.027*\"short\" + 0.024*\"strong\" + 0.023*\"recess\" + 0.022*\"yield\" + 0.019*\"bond\" + 0.019*\"year\" + 0.017*\"loss\" + 0.017*\"rate\"\n",
      "\n",
      "Score: 0.0200047716498 \n",
      "Topic: 0.054*\"year\" + 0.034*\"think\" + 0.030*\"compani\" + 0.028*\"market\" + 0.022*\"go\" + 0.017*\"month\" + 0.016*\"stock\" + 0.015*\"happen\" + 0.011*\"buy\" + 0.010*\"need\"\n",
      "\n",
      "Score: 0.0200046170503 \n",
      "Topic: 0.068*\"trade\" + 0.030*\"invest\" + 0.027*\"href\" + 0.027*\"http\" + 0.027*\"market\" + 0.025*\"stock\" + 0.024*\"time\" + 0.021*\"read\" + 0.020*\"know\" + 0.017*\"nofollow\"\n",
      "\n",
      "Score: 0.0200043041259 \n",
      "Topic: 0.058*\"look\" + 0.030*\"like\" + 0.023*\"share\" + 0.023*\"fund\" + 0.023*\"dividend\" + 0.018*\"market\" + 0.018*\"go\" + 0.017*\"stock\" + 0.017*\"risk\" + 0.014*\"come\"\n",
      "\n",
      "Score: 0.0200036261231 \n",
      "Topic: 0.036*\"price\" + 0.023*\"make\" + 0.022*\"right\" + 0.018*\"valu\" + 0.017*\"loan\" + 0.013*\"invest\" + 0.013*\"option\" + 0.012*\"sens\" + 0.012*\"rate\" + 0.012*\"check\"\n",
      "\n",
      "Score: 0.0200031623244 \n",
      "Topic: 0.071*\"like\" + 0.030*\"work\" + 0.016*\"peopl\" + 0.016*\"busi\" + 0.015*\"hard\" + 0.015*\"yeah\" + 0.015*\"idea\" + 0.014*\"think\" + 0.014*\"know\" + 0.014*\"mean\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model[bow_corpus[357]], key=lambda tupl: -1*tupl[1]):\n",
    "    print \"Score: {} \\nTopic: {}\\n\".format(score, lda_model.print_topic(index,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance evaluation of LDA model using TF-IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'He bought them at the beginning of the week   '"
      ]
     },
     "execution_count": 659,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents['comments'][356]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Glad I sold these yesterday before close Would ve been worthless if I still held '"
      ]
     },
     "execution_count": 660,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['title'][359]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.774913132191 \n",
      "Topic: 0.032*\"market\" + 0.032*\"stock\" + 0.025*\"long\" + 0.024*\"sell\" + 0.019*\"week\" + 0.017*\"hous\" + 0.016*\"today\" + 0.013*\"time\" + 0.012*\"go\" + 0.012*\"know\"\n",
      "\n",
      "Score: 0.0250198002905 \n",
      "Topic: 0.052*\"time\" + 0.027*\"strong\" + 0.022*\"year\" + 0.013*\"like\" + 0.013*\"close\" + 0.013*\"post\" + 0.012*\"thank\" + 0.012*\"month\" + 0.011*\"sell\" + 0.011*\"mayb\"\n",
      "\n",
      "Score: 0.0250167082995 \n",
      "Topic: 0.029*\"money\" + 0.028*\"short\" + 0.027*\"http\" + 0.027*\"like\" + 0.024*\"href\" + 0.018*\"cash\" + 0.017*\"nofollow\" + 0.017*\"market\" + 0.017*\"fund\" + 0.016*\"hold\"\n",
      "\n",
      "Score: 0.0250149760395 \n",
      "Topic: 0.027*\"think\" + 0.026*\"like\" + 0.022*\"peopl\" + 0.015*\"go\" + 0.014*\"card\" + 0.013*\"price\" + 0.013*\"pretti\" + 0.012*\"market\" + 0.011*\"look\" + 0.011*\"rate\"\n",
      "\n",
      "Score: 0.0250102430582 \n",
      "Topic: 0.029*\"year\" + 0.028*\"term\" + 0.023*\"long\" + 0.019*\"price\" + 0.019*\"call\" + 0.019*\"option\" + 0.018*\"go\" + 0.017*\"right\" + 0.016*\"thank\" + 0.016*\"like\"\n",
      "\n",
      "Score: 0.0250065661967 \n",
      "Topic: 0.036*\"like\" + 0.032*\"money\" + 0.015*\"month\" + 0.014*\"need\" + 0.013*\"market\" + 0.013*\"rate\" + 0.012*\"sell\" + 0.012*\"look\" + 0.011*\"stock\" + 0.011*\"think\"\n",
      "\n",
      "Score: 0.0250061433762 \n",
      "Topic: 0.025*\"fuck\" + 0.018*\"like\" + 0.018*\"market\" + 0.016*\"think\" + 0.016*\"year\" + 0.016*\"dividend\" + 0.014*\"loan\" + 0.013*\"return\" + 0.012*\"work\" + 0.012*\"better\"\n",
      "\n",
      "Score: 0.0250050053 \n",
      "Topic: 0.051*\"compani\" + 0.025*\"know\" + 0.024*\"stock\" + 0.018*\"peopl\" + 0.015*\"think\" + 0.015*\"point\" + 0.013*\"thought\" + 0.013*\"mean\" + 0.012*\"like\" + 0.012*\"make\"\n",
      "\n",
      "Score: 0.0250041484833 \n",
      "Topic: 0.045*\"invest\" + 0.030*\"appl\" + 0.024*\"thing\" + 0.016*\"need\" + 0.015*\"like\" + 0.014*\"money\" + 0.012*\"read\" + 0.012*\"good\" + 0.011*\"year\" + 0.011*\"trade\"\n",
      "\n",
      "Score: 0.0250032320619 \n",
      "Topic: 0.054*\"trade\" + 0.041*\"good\" + 0.024*\"bond\" + 0.022*\"market\" + 0.021*\"stock\" + 0.019*\"put\" + 0.017*\"look\" + 0.015*\"real\" + 0.014*\"time\" + 0.012*\"peopl\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model_tfidf[bow_corpus[356]], key=lambda tupl: -1*tupl[1]):\n",
    "    print \"Score: {} \\nTopic: {}\\n\".format(score, lda_model_tfidf.print_topic(index,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing LDA TF-IDF on unprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.549956798553\t Topic: 0.027*\"think\" + 0.026*\"like\" + 0.022*\"peopl\" + 0.015*\"go\" + 0.014*\"card\"\n",
      "Score: 0.0500101149082\t Topic: 0.036*\"like\" + 0.032*\"money\" + 0.015*\"month\" + 0.014*\"need\" + 0.013*\"market\"\n",
      "Score: 0.0500078015029\t Topic: 0.051*\"compani\" + 0.025*\"know\" + 0.024*\"stock\" + 0.018*\"peopl\" + 0.015*\"think\"\n",
      "Score: 0.0500065423548\t Topic: 0.052*\"time\" + 0.027*\"strong\" + 0.022*\"year\" + 0.013*\"like\" + 0.013*\"close\"\n",
      "Score: 0.0500062070787\t Topic: 0.045*\"invest\" + 0.030*\"appl\" + 0.024*\"thing\" + 0.016*\"need\" + 0.015*\"like\"\n",
      "Score: 0.0500052757561\t Topic: 0.032*\"market\" + 0.032*\"stock\" + 0.025*\"long\" + 0.024*\"sell\" + 0.019*\"week\"\n",
      "Score: 0.0500024147332\t Topic: 0.054*\"trade\" + 0.041*\"good\" + 0.024*\"bond\" + 0.022*\"market\" + 0.021*\"stock\"\n",
      "Score: 0.0500018335879\t Topic: 0.029*\"money\" + 0.028*\"short\" + 0.027*\"http\" + 0.027*\"like\" + 0.024*\"href\"\n",
      "Score: 0.0500016361475\t Topic: 0.029*\"year\" + 0.028*\"term\" + 0.023*\"long\" + 0.019*\"price\" + 0.019*\"call\"\n",
      "Score: 0.0500013828278\t Topic: 0.025*\"fuck\" + 0.018*\"like\" + 0.018*\"market\" + 0.016*\"think\" + 0.016*\"year\"\n"
     ]
    }
   ],
   "source": [
    "unseen_document = \"Amazon\"\n",
    "bow_vector = dictionary.doc2bow(tokenization(unseen_document))\n",
    "for index, score in sorted(lda_model_tfidf[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model_tfidf.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.10000000149\t Topic: 0.025*\"fuck\" + 0.018*\"like\" + 0.018*\"market\" + 0.016*\"think\" + 0.016*\"year\"\n",
      "Score: 0.10000000149\t Topic: 0.032*\"market\" + 0.032*\"stock\" + 0.025*\"long\" + 0.024*\"sell\" + 0.019*\"week\"\n",
      "Score: 0.10000000149\t Topic: 0.051*\"compani\" + 0.025*\"know\" + 0.024*\"stock\" + 0.018*\"peopl\" + 0.015*\"think\"\n",
      "Score: 0.10000000149\t Topic: 0.029*\"money\" + 0.028*\"short\" + 0.027*\"http\" + 0.027*\"like\" + 0.024*\"href\"\n",
      "Score: 0.10000000149\t Topic: 0.029*\"year\" + 0.028*\"term\" + 0.023*\"long\" + 0.019*\"price\" + 0.019*\"call\"\n",
      "Score: 0.10000000149\t Topic: 0.027*\"think\" + 0.026*\"like\" + 0.022*\"peopl\" + 0.015*\"go\" + 0.014*\"card\"\n",
      "Score: 0.10000000149\t Topic: 0.052*\"time\" + 0.027*\"strong\" + 0.022*\"year\" + 0.013*\"like\" + 0.013*\"close\"\n",
      "Score: 0.10000000149\t Topic: 0.054*\"trade\" + 0.041*\"good\" + 0.024*\"bond\" + 0.022*\"market\" + 0.021*\"stock\"\n",
      "Score: 0.10000000149\t Topic: 0.036*\"like\" + 0.032*\"money\" + 0.015*\"month\" + 0.014*\"need\" + 0.013*\"market\"\n",
      "Score: 0.10000000149\t Topic: 0.045*\"invest\" + 0.030*\"appl\" + 0.024*\"thing\" + 0.016*\"need\" + 0.015*\"like\"\n"
     ]
    }
   ],
   "source": [
    "unseen_document = \"Microsoft\"\n",
    "bow_vector = dictionary.doc2bow(tokenization(unseen_document))\n",
    "for index, score in sorted(lda_model_tfidf[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model_tfidf.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.549921274185\t Topic: 0.027*\"think\" + 0.026*\"like\" + 0.022*\"peopl\" + 0.015*\"go\" + 0.014*\"card\"\n",
      "Score: 0.0500220023096\t Topic: 0.052*\"time\" + 0.027*\"strong\" + 0.022*\"year\" + 0.013*\"like\" + 0.013*\"close\"\n",
      "Score: 0.0500118546188\t Topic: 0.054*\"trade\" + 0.041*\"good\" + 0.024*\"bond\" + 0.022*\"market\" + 0.021*\"stock\"\n",
      "Score: 0.050010945648\t Topic: 0.045*\"invest\" + 0.030*\"appl\" + 0.024*\"thing\" + 0.016*\"need\" + 0.015*\"like\"\n",
      "Score: 0.0500089451671\t Topic: 0.032*\"market\" + 0.032*\"stock\" + 0.025*\"long\" + 0.024*\"sell\" + 0.019*\"week\"\n",
      "Score: 0.0500074774027\t Topic: 0.029*\"year\" + 0.028*\"term\" + 0.023*\"long\" + 0.019*\"price\" + 0.019*\"call\"\n",
      "Score: 0.0500049926341\t Topic: 0.051*\"compani\" + 0.025*\"know\" + 0.024*\"stock\" + 0.018*\"peopl\" + 0.015*\"think\"\n",
      "Score: 0.0500045754015\t Topic: 0.025*\"fuck\" + 0.018*\"like\" + 0.018*\"market\" + 0.016*\"think\" + 0.016*\"year\"\n",
      "Score: 0.0500042065978\t Topic: 0.029*\"money\" + 0.028*\"short\" + 0.027*\"http\" + 0.027*\"like\" + 0.024*\"href\"\n",
      "Score: 0.0500036925077\t Topic: 0.036*\"like\" + 0.032*\"money\" + 0.015*\"month\" + 0.014*\"need\" + 0.013*\"market\"\n"
     ]
    }
   ],
   "source": [
    "unseen_document = \"Gold\"\n",
    "bow_vector = dictionary.doc2bow(tokenization(unseen_document))\n",
    "for index, score in sorted(lda_model_tfidf[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model_tfidf.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.549983978271\t Topic: 0.045*\"invest\" + 0.030*\"appl\" + 0.024*\"thing\" + 0.016*\"need\" + 0.015*\"like\"\n",
      "Score: 0.0500056371093\t Topic: 0.027*\"think\" + 0.026*\"like\" + 0.022*\"peopl\" + 0.015*\"go\" + 0.014*\"card\"\n",
      "Score: 0.0500030145049\t Topic: 0.051*\"compani\" + 0.025*\"know\" + 0.024*\"stock\" + 0.018*\"peopl\" + 0.015*\"think\"\n",
      "Score: 0.0500019192696\t Topic: 0.032*\"market\" + 0.032*\"stock\" + 0.025*\"long\" + 0.024*\"sell\" + 0.019*\"week\"\n",
      "Score: 0.0500012598932\t Topic: 0.036*\"like\" + 0.032*\"money\" + 0.015*\"month\" + 0.014*\"need\" + 0.013*\"market\"\n",
      "Score: 0.0500011965632\t Topic: 0.029*\"year\" + 0.028*\"term\" + 0.023*\"long\" + 0.019*\"price\" + 0.019*\"call\"\n",
      "Score: 0.0500009134412\t Topic: 0.025*\"fuck\" + 0.018*\"like\" + 0.018*\"market\" + 0.016*\"think\" + 0.016*\"year\"\n",
      "Score: 0.050000756979\t Topic: 0.052*\"time\" + 0.027*\"strong\" + 0.022*\"year\" + 0.013*\"like\" + 0.013*\"close\"\n",
      "Score: 0.0500007234514\t Topic: 0.029*\"money\" + 0.028*\"short\" + 0.027*\"http\" + 0.027*\"like\" + 0.024*\"href\"\n",
      "Score: 0.050000615418\t Topic: 0.054*\"trade\" + 0.041*\"good\" + 0.024*\"bond\" + 0.022*\"market\" + 0.021*\"stock\"\n"
     ]
    }
   ],
   "source": [
    "unseen_document = \"Apple\"\n",
    "bow_vector = dictionary.doc2bow(tokenization(unseen_document))\n",
    "for index, score in sorted(lda_model_tfidf[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model_tfidf.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model has a strong preference for APPL, Gold, Amazon but little to say about Microsoft due to the lack of data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
