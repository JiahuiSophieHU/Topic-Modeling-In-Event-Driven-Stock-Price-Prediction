commentsUrl,date,date_str,linkFlair,score,subreddit,title,post_url
https://old.reddit.com/r/investing/comments/b475gs/trump_says_he_will_nominate_fed_critic_stephen/'," datetime.datetime(2019, 3, 22, 22, 0, 4, 148168)",'2019-03-22',u'News',u'8','investing',u'Trump says he will nominate Fed critic Stephen Moore for central bank appointment'," u'https://old.reddit.com/r/investing/comments/b475gs/trump_says_he_will_nominate_fed_critic_stephen/'}
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-03-22 22:00:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
DEBUG:root:Post added to MongoDB
2019-03-22 22:00:04 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/ValueInvesting/new/>
"
https://old.reddit.com/r/wallstreetbets/comments/b495s1/many_of_you_know_me_as_the_900_to_55k_guy_ive/'," datetime.datetime(2019, 3, 22, 22, 5, 1, 593499)",'2019-03-22',u'Gain',u'428','wallstreetbets',"u'Many of you know me as the 900 to 55k guy, I\u2019ve been missing since I went to college. Now that I\u2019m about to graduate I\u2019m more free to yolo.'"," u'https://old.reddit.com/r/wallstreetbets/comments/b495s1/many_of_you_know_me_as_the_900_to_55k_guy_ive/'}
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
2019-03-22 22:05:01 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
2019-03-22 22:05:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
DEBUG:root:Post added to MongoDB
2019-03-22 22:05:01 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/stocks/new/>
"
https://old.reddit.com/r/investing/comments/b477cn/what_are_the_best_practices_and_investment/'," datetime.datetime(2019, 3, 22, 22, 40, 2, 697080)",'2019-03-22',None,u'6','investing',u'What are the best practices and investment strategies for a recession?'," u'https://old.reddit.com/r/investing/comments/b477cn/what_are_the_best_practices_and_investment/'}
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Daytrading/new/> from <GET https://old.reddit.com/r/daytrading/new/>
2019-03-22 22:40:02 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Daytrading/new/> from <GET https://old.reddit.com/r/daytrading/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-03-22 22:40:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
DEBUG:root:Post added to MongoDB
2019-03-22 22:40:03 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/ValueInvesting/new/>
"
https://old.reddit.com/r/stocks/comments/b44a1z/rstocks_daily_discussion_friday_mar_22_2019/'," datetime.datetime(2019, 3, 22, 22, 50, 2, 32440)",'2019-03-22',None,u'3','stocks',"u'r/Stocks Daily Discussion Friday - Mar 22, 2019'"," u'https://old.reddit.com/r/stocks/comments/b44a1z/rstocks_daily_discussion_friday_mar_22_2019/'}
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
2019-03-22 22:50:02 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
2019-03-22 22:50:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
DEBUG:root:Post added to MongoDB
2019-03-22 22:50:02 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/stockaday/new/>
"
https://old.reddit.com/r/investing/comments/b4780e/the_10y_03m_yield_spread_inversion_has_predicted/'," datetime.datetime(2019, 3, 22, 23, 5, 3, 258049)",'2019-03-22',None,u'19','investing',u'The 10y - 03m yield spread inversion has predicted almost all recessions except 1998'," u'https://old.reddit.com/r/investing/comments/b4780e/the_10y_03m_yield_spread_inversion_has_predicted/'}
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Daytrading/new/> from <GET https://old.reddit.com/r/daytrading/new/>
2019-03-22 23:05:03 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Daytrading/new/> from <GET https://old.reddit.com/r/daytrading/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-03-22 23:05:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
DEBUG:root:Post added to MongoDB
2019-03-22 23:05:04 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/ValueInvesting/new/>
"
https://old.reddit.com/r/wallstreetbets/comments/b4cuz1/potentiometer_short_play_ford_carbon_monoxide/'," datetime.datetime(2019, 3, 22, 23, 10, 1, 813963)",'2019-03-22',u'DD',u'4','wallstreetbets',u'Potentiometer Short play: Ford. Carbon Monoxide leaks in SUVs'," u'https://old.reddit.com/r/wallstreetbets/comments/b4cuz1/potentiometer_short_play_ford_carbon_monoxide/'}
DEBUG:root:Post added to MongoDB
2019-03-22 23:10:01 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/wallstreetbets/new/>
"
https://old.reddit.com/r/wallstreetbets/comments/b4cus7/photoshop_request_new_banner_images/'," datetime.datetime(2019, 3, 22, 23, 10, 1, 819081)",'2019-03-22',u'Mods',u'15','wallstreetbets',u'[Photoshop Request] New banner images'," u'https://old.reddit.com/r/wallstreetbets/comments/b4cus7/photoshop_request_new_banner_images/'}
DEBUG:root:Post added to MongoDB
2019-03-22 23:10:01 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/wallstreetbets/new/>
"
https://old.reddit.com/r/wallstreetbets/comments/b4cnmu/but_the_yield_spread_inversion_has_predicted_all/'," datetime.datetime(2019, 3, 22, 23, 10, 1, 821487)",'2019-03-22',None,u'0','wallstreetbets',"u'""BuT ThE yIeLd SpREaD iNvErSiOn hAs PrEDiCtED ALl pAsT ReCeSsiONs""'"," u'https://old.reddit.com/r/wallstreetbets/comments/b4cnmu/but_the_yield_spread_inversion_has_predicted_all/'}
DEBUG:root:Post added to MongoDB
2019-03-22 23:10:01 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/wallstreetbets/new/>
"
https://old.reddit.com/r/wallstreetbets/comments/b4c6ly/update_it_worked_out/'," datetime.datetime(2019, 3, 22, 23, 10, 1, 823722)",'2019-03-22',u'Gain',u'14','wallstreetbets',u'UPDATE: it worked out'," u'https://old.reddit.com/r/wallstreetbets/comments/b4c6ly/update_it_worked_out/'}
DEBUG:root:Post added to MongoDB
2019-03-22 23:10:01 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/wallstreetbets/new/>
"
https://old.reddit.com/r/wallstreetbets/comments/b4biqj/what_does_everyone_think_about_this_news/'," datetime.datetime(2019, 3, 22, 23, 10, 1, 826007)",'2019-03-22',u'Discussion',u'3','wallstreetbets',u'What does everyone think about this news?'," u'https://old.reddit.com/r/wallstreetbets/comments/b4biqj/what_does_everyone_think_about_this_news/'}
DEBUG:root:Post added to MongoDB
2019-03-22 23:10:01 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/wallstreetbets/new/>
"
https://old.reddit.com/r/wallstreetbets/comments/b4bhyd/financial_markets_around_the_world_drop_sharply/'," datetime.datetime(2019, 3, 22, 23, 10, 1, 828405)",'2019-03-22',u'Discussion',u'3','wallstreetbets',u'Financial markets around the world drop sharply as fears of global slowdown intensify'," u'https://www.theguardian.com/business/2019/mar/22/eurozone-suffers-sharp-decline-manufacturing-activity'}
DEBUG:root:Post added to MongoDB
2019-03-22 23:10:01 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/wallstreetbets/new/>
"
https://old.reddit.com/r/wallstreetbets/comments/b4bcgm/and_you_people_wonder_why_the_aliens_havent/'," datetime.datetime(2019, 3, 22, 23, 10, 1, 830916)",'2019-03-22',u'Shitpost',u'22','wallstreetbets',"u""and you people wonder why the aliens haven't contacted us yet"""," u'https://old.reddit.com/r/wallstreetbets/comments/b4bcgm/and_you_people_wonder_why_the_aliens_havent/'}
DEBUG:root:Post added to MongoDB
2019-03-22 23:10:01 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/wallstreetbets/new/>
"
https://old.reddit.com/r/wallstreetbets/comments/b4b83o/oops/'," datetime.datetime(2019, 3, 22, 23, 10, 1, 833406)",'2019-03-22',u'Loss',u'19','wallstreetbets',u'Oops'," u'https://old.reddit.com/r/wallstreetbets/comments/b4b83o/oops/'}
DEBUG:root:Post added to MongoDB
2019-03-22 23:10:01 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/wallstreetbets/new/>
"
https://old.reddit.com/r/wallstreetbets/comments/b4aws7/that_one_banker_from_08_lol/'," datetime.datetime(2019, 3, 22, 23, 10, 1, 836014)",'2019-03-22',u'Shitpost',u'44','wallstreetbets',u'That one banker from 08 lol'," u'https://old.reddit.com/r/wallstreetbets/comments/b4aws7/that_one_banker_from_08_lol/'}
DEBUG:root:Post added to MongoDB
2019-03-22 23:10:01 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/wallstreetbets/new/>
"
https://old.reddit.com/r/wallstreetbets/comments/b4atsq/things_are_not_going_well/'," datetime.datetime(2019, 3, 22, 23, 10, 1, 838939)",'2019-03-22',u'Loss',u'41','wallstreetbets',u'Things are not going well'," u'https://old.reddit.com/r/wallstreetbets/comments/b4atsq/things_are_not_going_well/'}
DEBUG:root:Post added to MongoDB
2019-03-22 23:10:01 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/wallstreetbets/new/>
"
https://old.reddit.com/r/wallstreetbets/comments/b4assm/powell_after_deciding_to_hold_interest_rates/'," datetime.datetime(2019, 3, 22, 23, 10, 1, 841335)",'2019-03-22',u'Shitpost',u'595','wallstreetbets',u'Powell after deciding to hold interest rates'," u'https://old.reddit.com/r/wallstreetbets/comments/b4assm/powell_after_deciding_to_hold_interest_rates/'}
DEBUG:root:Post added to MongoDB
2019-03-22 23:10:01 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/wallstreetbets/new/>
"
https://old.reddit.com/r/wallstreetbets/comments/b4946x/glad_i_sold_these_yesterday_before_close_wouldve/'," datetime.datetime(2019, 3, 22, 23, 10, 1, 843717)",'2019-03-22',u'Gain',u'108','wallstreetbets',"u""Glad I sold these yesterday before close. Would've been worthless if I still held"""," u'https://old.reddit.com/r/wallstreetbets/comments/b4946x/glad_i_sold_these_yesterday_before_close_wouldve/'}
DEBUG:root:Post added to MongoDB
2019-03-22 23:10:01 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/wallstreetbets/new/>
"
https://old.reddit.com/r/wallstreetbets/comments/b4a6d7/dow_falls_460_points_as_us_recession_indicator/'," datetime.datetime(2019, 3, 22, 23, 10, 1, 846092)",'2019-03-22',u'Fundamentals',u'10','wallstreetbets',u'Dow falls 460 points as US recession indicator flashes red'," u'https://amp.cnn.com/cnn/2019/03/22/investing/dow-stock-market-yield-curve/index.html?__twitter_impression=true'}
DEBUG:root:Post added to MongoDB
2019-03-22 23:10:01 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/wallstreetbets/new/>
"
https://old.reddit.com/r/wallstreetbets/comments/b4a5tr/pinterest_files_to_go_public_booked_756_million/'," datetime.datetime(2019, 3, 22, 23, 10, 1, 848397)",'2019-03-22',u'Stocks',u'24','wallstreetbets',u'Pinterest files to go public: Booked $756 million last year and claims 250 million monthly users'," u'https://www.cnbc.com/2019/03/22/pinterest-releases-s-1-for-ipo.html'}
DEBUG:root:Post added to MongoDB
2019-03-22 23:10:01 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/wallstreetbets/new/>
"
https://old.reddit.com/r/wallstreetbets/comments/b4a4qi/ea_to_the_moon_92mm_in_first_month_before/'," datetime.datetime(2019, 3, 22, 23, 10, 1, 850821)",'2019-03-22',u'DD',u'2','wallstreetbets',u'EA to the moon? 92MM in first month before battlepass'," u'https://www.gamerevolution.com/news/514153-apex-legends-sales'}
DEBUG:root:Post added to MongoDB
2019-03-22 23:10:01 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/wallstreetbets/new/>
"
https://old.reddit.com/r/wallstreetbets/comments/b49z3k/i_guess_shaq_underperforms_the_market_just_like/'," datetime.datetime(2019, 3, 22, 23, 10, 1, 853389)",'2019-03-22',u'Shitpost',u'64','wallstreetbets',u'I guess Shaq underperforms the market just like everyone on WSB'," u'https://old.reddit.com/r/wallstreetbets/comments/b49z3k/i_guess_shaq_underperforms_the_market_just_like/'}
DEBUG:root:Post added to MongoDB
2019-03-22 23:10:01 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/wallstreetbets/new/>
"
https://old.reddit.com/r/wallstreetbets/comments/b49xvf/cocaine_on_wall_street_confirmed/'," datetime.datetime(2019, 3, 22, 23, 10, 1, 855703)",'2019-03-22',u'Shitpost',u'315','wallstreetbets',u'Cocaine on Wall Street confirmed:'," u'https://old.reddit.com/r/wallstreetbets/comments/b49xvf/cocaine_on_wall_street_confirmed/'}
DEBUG:root:Post added to MongoDB
2019-03-22 23:10:01 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/wallstreetbets/new/>
"
https://old.reddit.com/r/wallstreetbets/comments/b49x5s/journey_to_whatever_is_possible/'," datetime.datetime(2019, 3, 22, 23, 10, 1, 857999)",'2019-03-22',u'Gain',u'68','wallstreetbets',u'Journey to whatever is possible'," u'https://old.reddit.com/r/wallstreetbets/comments/b49x5s/journey_to_whatever_is_possible/'}
DEBUG:root:Post added to MongoDB
2019-03-22 23:10:01 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/wallstreetbets/new/>
"
https://old.reddit.com/r/wallstreetbets/comments/b49p5w/weekend_discussion_thread_march_2224_2019/'," datetime.datetime(2019, 3, 22, 23, 10, 1, 860416)",'2019-03-22',u'Daily Discussion',u'35','wallstreetbets',"u'Weekend Discussion Thread - March 22-24, 2019'"," u'https://old.reddit.com/r/wallstreetbets/comments/b49p5w/weekend_discussion_thread_march_2224_2019/'}
DEBUG:root:Post added to MongoDB
2019-03-22 23:10:01 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/wallstreetbets/new/>
"
https://old.reddit.com/r/wallstreetbets/comments/b49fq3/he_needs_our_support/'," datetime.datetime(2019, 3, 22, 23, 10, 1, 862680)",'2019-03-22',u'Discussion',u'6','wallstreetbets',u'He needs our support'," u'https://www.wsj.com/articles/ex-enron-ceo-skilling-plots-second-act-11553282732?emailToken=d0093a295fe70d2cffb1f217bb3968efJvjyOI9wjagewtjeayLwtN0k5mezlRluXlcn+6bXnDdgInG2XbnJS9SNu1sO/K6T7WplZXnSTVBgJY4rbU1tksWymbBbNUJD8fj08Td97pU%3D&reflink=article_copyURL_share'}
DEBUG:root:Post added to MongoDB
2019-03-22 23:10:01 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/wallstreetbets/new/>
"
https://old.reddit.com/r/wallstreetbets/comments/b499is/43k_in_10_minutes_on_options_expiring_today/'," datetime.datetime(2019, 3, 22, 23, 10, 1, 866087)",'2019-03-22',u'Gain',u'142','wallstreetbets',u'$43k in 10 minutes on options expiring today'," u'https://old.reddit.com/r/wallstreetbets/comments/b499is/43k_in_10_minutes_on_options_expiring_today/'}
DEBUG:root:Post added to MongoDB
2019-03-22 23:10:01 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/wallstreetbets/new/>
"
https://old.reddit.com/r/wallstreetbets/comments/b497so/til_i_dont_have_any_idea_what_i_am_doing_i/'," datetime.datetime(2019, 3, 22, 23, 10, 1, 868710)",'2019-03-22',u'Options',u'23','wallstreetbets',u'TIL I don\u2019t have any idea what I am doing. I mistakenly thought even losers win sometimes \U0001f629'," u'https://old.reddit.com/r/wallstreetbets/comments/b497so/til_i_dont_have_any_idea_what_i_am_doing_i/'}
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
2019-03-22 23:10:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
2019-03-22 23:10:01 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
DEBUG:root:Post added to MongoDB
2019-03-22 23:10:02 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/stocks/new/>
"
https://old.reddit.com/r/stocks/comments/b49hs1/stockbond_certificate_replicas/'," datetime.datetime(2019, 3, 22, 23, 10, 2, 88065)",'2019-03-22',None,u'0','stocks',u'Stock/Bond certificate replicas'," u'https://old.reddit.com/r/stocks/comments/b49hs1/stockbond_certificate_replicas/'}
DEBUG:root:Post added to MongoDB
2019-03-22 23:10:02 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/stocks/new/>
"
https://old.reddit.com/r/stocks/comments/b49aty/videoconferencing_company_zoom_files_to_go_public/'," datetime.datetime(2019, 3, 22, 23, 10, 2, 90296)",'2019-03-22',None,u'201','stocks',"u""Video-conferencing company Zoom files to go public with over $300 million in revenue \u2014 and it's even profitable"""," u'https://old.reddit.com/r/stocks/comments/b49aty/videoconferencing_company_zoom_files_to_go_public/'}
DEBUG:root:Post added to MongoDB
2019-03-22 23:10:02 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/stocks/new/>
"
https://old.reddit.com/r/stocks/comments/b493hp/looking_for_offline_charts_for_android/'," datetime.datetime(2019, 3, 22, 23, 10, 2, 92604)",'2019-03-22',None,u'0','stocks',u'Looking for offline charts for android'," u'https://old.reddit.com/r/stocks/comments/b493hp/looking_for_offline_charts_for_android/'}
DEBUG:root:Post added to MongoDB
2019-03-22 23:10:02 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/stocks/new/>
"
https://old.reddit.com/r/stocks/comments/b4930o/apple_is_keeping_partners_in_the_dark_about_how/'," datetime.datetime(2019, 3, 22, 23, 10, 2, 94908)",'2019-03-22',None,u'19','stocks',u'Apple is keeping partners in the dark about how it plans to package and price its video service'," u'https://old.reddit.com/r/stocks/comments/b4930o/apple_is_keeping_partners_in_the_dark_about_how/'}
DEBUG:root:Post added to MongoDB
2019-03-22 23:10:02 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/stocks/new/>
"
https://old.reddit.com/r/stocks/comments/b47out/opinions_about_nasdaq_100/'," datetime.datetime(2019, 3, 22, 23, 10, 2, 97185)",'2019-03-22',None,u'2','stocks',u'Opinions about Nasdaq 100'," u'https://old.reddit.com/r/stocks/comments/b47out/opinions_about_nasdaq_100/'}
DEBUG:root:Post added to MongoDB
2019-03-22 23:10:02 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/stocks/new/>
"
https://old.reddit.com/r/stocks/comments/b47nz9/is_bioscrip_bios_a_buy_right_now/'," datetime.datetime(2019, 3, 22, 23, 10, 2, 99429)",'2019-03-22',None,u'0','stocks',u'Is BioScrip (BIOS) a buy right now?'," u'https://old.reddit.com/r/stocks/comments/b47nz9/is_bioscrip_bios_a_buy_right_now/'}
DEBUG:root:Post added to MongoDB
2019-03-22 23:10:02 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/stocks/new/>
"
https://old.reddit.com/r/stocks/comments/b47kkb/rstocks_fundamentals_friday_mar_22_2019/'," datetime.datetime(2019, 3, 22, 23, 10, 2, 101633)",'2019-03-22',None,u'2','stocks',"u'r/Stocks Fundamentals Friday Mar 22, 2019'"," u'https://old.reddit.com/r/stocks/comments/b47kkb/rstocks_fundamentals_friday_mar_22_2019/'}
DEBUG:root:Post added to MongoDB
2019-03-22 23:10:02 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/stocks/new/>
"
https://old.reddit.com/r/stocks/comments/b47bdg/wall_street_analysts_this_week_are_increasingly/'," datetime.datetime(2019, 3, 22, 23, 10, 2, 103959)",'2019-03-22',None,u'12','stocks',u'Wall Street analysts this week are increasingly worried about Amazon hurting the stocks they cover'," u'https://old.reddit.com/r/stocks/comments/b47bdg/wall_street_analysts_this_week_are_increasingly/'}
DEBUG:root:Post added to MongoDB
2019-03-22 23:10:02 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/stocks/new/>
"
https://old.reddit.com/r/stocks/comments/b476b1/ladybaybee_recommended_stock_picks_20190322/'," datetime.datetime(2019, 3, 22, 23, 10, 2, 106190)",'2019-03-22',None,u'5','stocks',u'Ladybaybee recommended stock picks 2019-03-22'," u'https://old.reddit.com/r/stocks/comments/b476b1/ladybaybee_recommended_stock_picks_20190322/'}
DEBUG:root:Post added to MongoDB
2019-03-22 23:10:02 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/stocks/new/>
"
https://old.reddit.com/r/stocks/comments/b46wq4/what_is_going_on_with_wrk/'," datetime.datetime(2019, 3, 22, 23, 10, 2, 110769)",'2019-03-22',None,u'0','stocks',u'What is going on with WRK?'," u'https://old.reddit.com/r/stocks/comments/b46wq4/what_is_going_on_with_wrk/'}
DEBUG:root:Post added to MongoDB
2019-03-22 23:10:02 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/stocks/new/>
"
https://old.reddit.com/r/stocks/comments/b46vpg/any_good_way_to_get_into_an_ipo_before_it_rockets/'," datetime.datetime(2019, 3, 22, 23, 10, 2, 113048)",'2019-03-22',None,u'0','stocks',u'Any good way to get into an IPO before it rockets?'," u'https://old.reddit.com/r/stocks/comments/b46vpg/any_good_way_to_get_into_an_ipo_before_it_rockets/'}
DEBUG:root:Post added to MongoDB
2019-03-22 23:10:02 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/stocks/new/>
"
https://old.reddit.com/r/stocks/comments/b46t64/vix_up_20/'," datetime.datetime(2019, 3, 22, 23, 10, 2, 115427)",'2019-03-22',None,u'0','stocks',u'$VIX up 20%'," u'https://old.reddit.com/r/stocks/comments/b46t64/vix_up_20/'}
DEBUG:root:Post added to MongoDB
2019-03-22 23:10:02 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/stocks/new/>
"
https://old.reddit.com/r/stocks/comments/b46s3u/any_websites_that_you_can_find_company_news_for_a/'," datetime.datetime(2019, 3, 22, 23, 10, 2, 117687)",'2019-03-22',u'Question',u'1','stocks',u'Any websites that you can find company news for a specific date? I can\u2019t seem to do it on google anymore.'," u'https://old.reddit.com/r/stocks/comments/b46s3u/any_websites_that_you_can_find_company_news_for_a/'}
DEBUG:root:Post added to MongoDB
2019-03-22 23:10:02 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/stocks/new/>
"
https://old.reddit.com/r/stocks/comments/b46r9g/will_zuora_zuo_achieve_liftoff_any_time_soon/'," datetime.datetime(2019, 3, 22, 23, 10, 2, 119930)",'2019-03-22',None,u'1','stocks',u'Will Zuora ZUO achieve lift-off any time soon?'," u'https://old.reddit.com/r/stocks/comments/b46r9g/will_zuora_zuo_achieve_liftoff_any_time_soon/'}
DEBUG:root:Post added to MongoDB
2019-03-22 23:10:02 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/stocks/new/>
"
https://old.reddit.com/r/stocks/comments/b46dmj/any_ideas_why_avaya_avya_keeps_falling/'," datetime.datetime(2019, 3, 22, 23, 10, 2, 122208)",'2019-03-22',None,u'3','stocks',u'Any ideas why Avaya (AVYA) keeps falling?'," u'https://old.reddit.com/r/stocks/comments/b46dmj/any_ideas_why_avaya_avya_keeps_falling/'}
DEBUG:root:Post added to MongoDB
2019-03-22 23:10:02 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/stocks/new/>
"
https://old.reddit.com/r/stocks/comments/b45rmb/what_was_the_mean_value_of_a_share_of_amazon_on/'," datetime.datetime(2019, 3, 22, 23, 10, 2, 124444)",'2019-03-22',None,u'0','stocks',u'What was the mean value of a share of amazon on December 20th 2018?'," u'https://old.reddit.com/r/stocks/comments/b45rmb/what_was_the_mean_value_of_a_share_of_amazon_on/'}
DEBUG:root:Post added to MongoDB
2019-03-22 23:10:02 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/stocks/new/>
"
https://old.reddit.com/r/stocks/comments/b45o5f/alibaba_class_action_worth_it/'," datetime.datetime(2019, 3, 22, 23, 10, 2, 126747)",'2019-03-22',None,u'6','stocks',u'Alibaba Class Action - Worth It?'," u'https://old.reddit.com/r/stocks/comments/b45o5f/alibaba_class_action_worth_it/'}
DEBUG:root:Post added to MongoDB
2019-03-22 23:10:02 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/stocks/new/>
"
https://old.reddit.com/r/stocks/comments/b456da/will_brexit_have_effect_on_big_it_companies_noted/'," datetime.datetime(2019, 3, 22, 23, 10, 2, 129010)",'2019-03-22',u'Question',u'3','stocks',"u'Will Brexit have effect on big IT companies, noted on US stock markets like SAP, MS etc?'"," u'https://old.reddit.com/r/stocks/comments/b456da/will_brexit_have_effect_on_big_it_companies_noted/'}
DEBUG:root:Post added to MongoDB
2019-03-22 23:10:02 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/stocks/new/>
"
https://old.reddit.com/r/investing/comments/b4ccq3/money_market_etf/'," datetime.datetime(2019, 3, 22, 23, 10, 3, 42425)",'2019-03-22',None,u'6','investing',u'Money market ETF'," u'https://old.reddit.com/r/investing/comments/b4ccq3/money_market_etf/'}
DEBUG:root:Post added to MongoDB
2019-03-22 23:10:03 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/investing/new/>
"
https://old.reddit.com/r/investing/comments/b4bkjw/how_many_of_you_are_buying_with_the_inverted/'," datetime.datetime(2019, 3, 22, 23, 10, 3, 44877)",'2019-03-22',None,u'11','investing',u'How many of you are buying with the inverted yield curve?'," u'https://old.reddit.com/r/investing/comments/b4bkjw/how_many_of_you_are_buying_with_the_inverted/'}
DEBUG:root:Post added to MongoDB
2019-03-22 23:10:03 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/investing/new/>
"
https://old.reddit.com/r/investing/comments/b4bhyt/pinterest_files_for_ipo_but_avoids_calling_itself/'," datetime.datetime(2019, 3, 22, 23, 10, 3, 47634)",'2019-03-22',None,u'1','investing',"u'Pinterest files for IPO, but avoids calling itself a social network (The Verge)'"," u'https://old.reddit.com/r/investing/comments/b4bhyt/pinterest_files_for_ipo_but_avoids_calling_itself/'}
DEBUG:root:Post added to MongoDB
2019-03-22 23:10:03 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/investing/new/>
"
https://old.reddit.com/r/investing/comments/b4b8c0/what_if_ownership_of_stocks_is_a_big_scam/'," datetime.datetime(2019, 3, 22, 23, 10, 3, 49938)",'2019-03-22',None,u'0','investing',u'What if ownership of stocks is a big scam?'," u'https://old.reddit.com/r/investing/comments/b4b8c0/what_if_ownership_of_stocks_is_a_big_scam/'}
DEBUG:root:Post added to MongoDB
2019-03-22 23:10:03 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/investing/new/>
"
https://old.reddit.com/r/investing/comments/b4aziu/stash_invest_yea_or_nay/'," datetime.datetime(2019, 3, 22, 23, 10, 3, 52177)",'2019-03-22',None,u'0','investing',u'Stash Invest: Yea or Nay?'," u'https://old.reddit.com/r/investing/comments/b4aziu/stash_invest_yea_or_nay/'}
DEBUG:root:Post added to MongoDB
2019-03-22 23:10:03 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/investing/new/>
"
https://old.reddit.com/r/investing/comments/b4as8n/lyft_ipo_prospectus/'," datetime.datetime(2019, 3, 22, 23, 10, 3, 54436)",'2019-03-22',None,u'0','investing',u'Lyft IPO prospectus'," u'https://old.reddit.com/r/investing/comments/b4as8n/lyft_ipo_prospectus/'}
DEBUG:root:Post added to MongoDB
2019-03-22 23:10:03 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/investing/new/>
"
https://old.reddit.com/r/investing/comments/b4ao36/fidelity_brokerage_link/'," datetime.datetime(2019, 3, 22, 23, 10, 3, 56695)",'2019-03-22',None,u'0','investing',u'Fidelity Brokerage Link?'," u'https://old.reddit.com/r/investing/comments/b4ao36/fidelity_brokerage_link/'}
DEBUG:root:Post added to MongoDB
2019-03-22 23:10:03 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/investing/new/>
"
https://old.reddit.com/r/investing/comments/b4am5r/i_maxed_out_my_ira_contribution_but_have_more/'," datetime.datetime(2019, 3, 22, 23, 10, 3, 59004)",'2019-03-22',None,u'1','investing',"u'I maxed out my IRA contribution but have more money to contribute, what type of account should I put it into?'"," u'https://old.reddit.com/r/investing/comments/b4am5r/i_maxed_out_my_ira_contribution_but_have_more/'}
DEBUG:root:Post added to MongoDB
2019-03-22 23:10:03 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/investing/new/>
"
https://old.reddit.com/r/investing/comments/b4a9t4/eli5_inverted_yield_curve_and_housing_market/'," datetime.datetime(2019, 3, 22, 23, 10, 3, 61302)",'2019-03-22',None,u'0','investing',u'ELI5: Inverted Yield Curve and Housing Market'," u'https://old.reddit.com/r/investing/comments/b4a9t4/eli5_inverted_yield_curve_and_housing_market/'}
DEBUG:root:Post added to MongoDB
2019-03-22 23:10:03 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/investing/new/>
"
https://old.reddit.com/r/investing/comments/b49zla/pinterest_just_dropped_its_filing_to_go_public/'," datetime.datetime(2019, 3, 22, 23, 10, 3, 63531)",'2019-03-22',None,u'44','investing',"u'Pinterest just dropped its filing to go public, revealing financials for the first time'"," u'https://old.reddit.com/r/investing/comments/b49zla/pinterest_just_dropped_its_filing_to_go_public/'}
DEBUG:root:Post added to MongoDB
2019-03-22 23:10:03 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/investing/new/>
"
https://old.reddit.com/r/investing/comments/b49lay/stay_calm_during_yield_inversion/'," datetime.datetime(2019, 3, 22, 23, 10, 3, 65773)",'2019-03-22',None,u'68','investing',u'Stay Calm During Yield Inversion'," u'https://old.reddit.com/r/investing/comments/b49lay/stay_calm_during_yield_inversion/'}
DEBUG:root:Post added to MongoDB
2019-03-22 23:10:03 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/investing/new/>
"
https://old.reddit.com/r/investing/comments/b49din/zoom_files_to_go_public_is_profitable/'," datetime.datetime(2019, 3, 22, 23, 10, 3, 68182)",'2019-03-22',None,u'9','investing',"u'Zoom Files to Go Public, Is Profitable'"," u'https://old.reddit.com/r/investing/comments/b49din/zoom_files_to_go_public_is_profitable/'}
DEBUG:root:Post added to MongoDB
2019-03-22 23:10:03 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/investing/new/>
"
https://old.reddit.com/r/investing/comments/b49byy/10_yr_yield_leading_global_pmis/'," datetime.datetime(2019, 3, 22, 23, 10, 3, 70438)",'2019-03-22',u'Education',u'1','investing',u'10 Yr Yield leading Global PMIs'," u'https://old.reddit.com/r/investing/comments/b49byy/10_yr_yield_leading_global_pmis/'}
DEBUG:root:Post added to MongoDB
2019-03-22 23:10:03 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/investing/new/>
"
https://old.reddit.com/r/investing/comments/b496r1/retiring_early_with_401k_question/'," datetime.datetime(2019, 3, 22, 23, 10, 3, 72707)",'2019-03-22',u'Help',u'1','investing',u'Retiring Early with 401K Question'," u'https://old.reddit.com/r/investing/comments/b496r1/retiring_early_with_401k_question/'}
DEBUG:root:Post added to MongoDB
2019-03-22 23:10:03 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/investing/new/>
"
https://old.reddit.com/r/investing/comments/b48dts/how_did_you_get_started_in_investing/'," datetime.datetime(2019, 3, 22, 23, 10, 3, 74965)",'2019-03-22',u'Education',u'1','investing',u'How did you get started in investing?'," u'https://old.reddit.com/r/investing/comments/b48dts/how_did_you_get_started_in_investing/'}
DEBUG:root:Post added to MongoDB
2019-03-22 23:10:03 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/investing/new/>
"
https://old.reddit.com/r/investing/comments/b48au0/my_thoughts_a_recession_spending_gdp_investors/'," datetime.datetime(2019, 3, 22, 23, 10, 3, 77169)",'2019-03-22',None,u'1','investing',"u'My thoughts - A recession, spending, GDP, investors push to bonds.'"," u'https://old.reddit.com/r/investing/comments/b48au0/my_thoughts_a_recession_spending_gdp_investors/'}
DEBUG:root:Post added to MongoDB
2019-03-22 23:10:03 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/investing/new/>
"
https://old.reddit.com/r/investing/comments/b488bg/potential_us_recession_looming/'," datetime.datetime(2019, 3, 22, 23, 10, 3, 79439)",'2019-03-22',u'Discussion',u'2','investing',u'Potential US Recession Looming'," u'https://old.reddit.com/r/investing/comments/b488bg/potential_us_recession_looming/'}
DEBUG:root:Post added to MongoDB
2019-03-22 23:10:03 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/investing/new/>
"
https://old.reddit.com/r/investing/comments/b485rt/it_is_possible_to_understand_based_on_data_what/'," datetime.datetime(2019, 3, 22, 23, 10, 3, 81805)",'2019-03-22',None,u'2','investing',"u'It is possible to understand, based on data, what is causing market fluctuations?'"," u'https://old.reddit.com/r/investing/comments/b485rt/it_is_possible_to_understand_based_on_data_what/'}
DEBUG:root:Post added to MongoDB
2019-03-22 23:10:03 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/investing/new/>
"
https://old.reddit.com/r/investing/comments/b47de9/is_spyg_spyv_or_spyd_better_options_to_spy_if_i/'," datetime.datetime(2019, 3, 22, 23, 10, 3, 84075)",'2019-03-22',None,u'1','investing',"u'Is SPYG, SPYV or SPYD better options to SPY if I can trade them without commision'"," u'https://old.reddit.com/r/investing/comments/b47de9/is_spyg_spyv_or_spyd_better_options_to_spy_if_i/'}
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-03-22 23:10:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
DEBUG:root:Post added to MongoDB
2019-03-22 23:10:03 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/ValueInvesting/new/>
"
https://old.reddit.com/r/thewallstreet/comments/b2fmik/weekly_question_thread_week_11_2019/'," datetime.datetime(2019, 3, 22, 23, 10, 3, 893842)",'2019-03-22',None,u'9','thewallstreet',"u'Weekly Question Thread - Week 11, 2019'"," u'https://old.reddit.com/r/thewallstreet/comments/b2fmik/weekly_question_thread_week_11_2019/'}
DEBUG:root:Post added to MongoDB
2019-03-22 23:10:04 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/Trading/new/>
"
https://old.reddit.com/r/Trading/comments/axfads/short_term_vs_long_term_capital_gains_taxes/'," datetime.datetime(2019, 3, 22, 23, 10, 4, 67903)",'2019-03-22',None,u'1','Trading',u'Short Term vs Long Term Capital Gains Taxes'," u'https://old.reddit.com/r/Trading/comments/axfads/short_term_vs_long_term_capital_gains_taxes/'}
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/StockMarket/new/> (referer: None)
2019-03-22 23:10:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/StockMarket/new/> (referer: None)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/Stock_Picks/new/> (referer: None)
2019-03-22 23:10:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/Stock_Picks/new/> (referer: None)
DEBUG:root:Post added to MongoDB
2019-03-22 23:10:04 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/StockMarket/new/>
"
https://old.reddit.com/r/StockMarket/comments/b465k6/do_you_expect_to_see_a_recovery_from_the/'," datetime.datetime(2019, 3, 22, 23, 10, 4, 685411)",'2019-03-22',None,u'0','StockMarket',u'Do you expect to see a recovery from the financial stocks?'," u'https://old.reddit.com/r/StockMarket/comments/b465k6/do_you_expect_to_see_a_recovery_from_the/'}
DEBUG:root:Post added to MongoDB
2019-03-22 23:10:04 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/Stock_Picks/new/>
"
https://old.reddit.com/r/StockMarket/comments/b45ht1/aurora_cannabis_aims_to_exploit_mexicos_recent/'," datetime.datetime(2019, 3, 22, 23, 10, 4, 708019)",'2019-03-22',None,u'6','StockMarket',u'Aurora Cannabis Aims To Exploit Mexico\u2019s Recent Marijuana Legislation; Other US Companies Join'," u'https://old.reddit.com/r/StockMarket/comments/b45ht1/aurora_cannabis_aims_to_exploit_mexicos_recent/'}
DEBUG:root:Post added to MongoDB
2019-03-22 23:10:04 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/StockMarket/new/>
"
https://old.reddit.com/r/StockMarket/comments/b44my5/rstockmarket_march_2019_contest_update_as_market/'," datetime.datetime(2019, 3, 22, 23, 10, 4, 711016)",'2019-03-22',u'Contest',u'85','StockMarket',"u'r/StockMarket March 2019 Contest update as market close March 21st, 2019'"," u'https://old.reddit.com/r/StockMarket/comments/b44my5/rstockmarket_march_2019_contest_update_as_market/'}
DEBUG:root:Post added to MongoDB
2019-03-22 23:10:04 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/StockMarket/new/>
"
https://old.reddit.com/r/StockMarket/comments/b44k41/relationship_between_asian_stock_market_and_us/'," datetime.datetime(2019, 3, 22, 23, 10, 4, 713558)",'2019-03-22',None,u'1','StockMarket',u'Relationship between Asian stock market and US equities'," u'https://old.reddit.com/r/StockMarket/comments/b44k41/relationship_between_asian_stock_market_and_us/'}
DEBUG:root:Post added to MongoDB
2019-03-22 23:10:04 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/StockMarket/new/>
"
https://old.reddit.com/r/StockMarket/comments/b44gf6/todays_premarket_news_friday_march_22nd_2019/'," datetime.datetime(2019, 3, 22, 23, 10, 4, 715877)",'2019-03-22',u'News',u'16','StockMarket',"u""Today's Pre-Market News [Friday, March 22nd, 2019]"""," u'https://old.reddit.com/r/StockMarket/comments/b44gf6/todays_premarket_news_friday_march_22nd_2019/'}
DEBUG:root:Post added to MongoDB
2019-03-22 23:10:04 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/Stock_Picks/new/>
"
https://old.reddit.com/r/StockMarket/comments/b43rzu/most_anticipated_earnings_releases_for_the_week/'," datetime.datetime(2019, 3, 22, 23, 10, 4, 729689)",'2019-03-22',u'Earnings',u'53','StockMarket',"u'Most Anticipated Earnings Releases for the week beginning March 25th, 2019'"," u'https://old.reddit.com/r/StockMarket/comments/b43rzu/most_anticipated_earnings_releases_for_the_week/'}
DEBUG:root:Post added to MongoDB
2019-03-22 23:10:04 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/StockMarket/new/>
"
https://old.reddit.com/r/StockMarket/comments/b413im/all_index_funds/'," datetime.datetime(2019, 3, 22, 23, 10, 4, 731996)",'2019-03-22',None,u'1','StockMarket',u'All Index Funds?'," u'https://old.reddit.com/r/StockMarket/comments/b413im/all_index_funds/'}
DEBUG:root:Post added to MongoDB
2019-03-22 23:10:04 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/StockMarket/new/>
"
https://old.reddit.com/r/StockMarket/comments/b3zfep/what_are_the_best_free_financial_news_websites_to/'," datetime.datetime(2019, 3, 22, 23, 10, 4, 734221)",'2019-03-22',None,u'116','StockMarket',u'What are the best FREE financial news websites to use to get your news on companies you\u2019re watching?'," u'https://old.reddit.com/r/StockMarket/comments/b3zfep/what_are_the_best_free_financial_news_websites_to/'}
DEBUG:root:Post added to MongoDB
2019-03-22 23:10:04 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/StockMarket/new/>
"
https://old.reddit.com/r/StockMarket/comments/b3z9ue/msft/'," datetime.datetime(2019, 3, 22, 23, 10, 4, 736468)",'2019-03-22',None,u'24','StockMarket',u'MSFT'," u'https://old.reddit.com/r/StockMarket/comments/b3z9ue/msft/'}
DEBUG:root:Post added to MongoDB
2019-03-22 23:10:04 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/StockMarket/new/>
"
https://old.reddit.com/r/StockMarket/comments/b3w8p1/tim_sykes_dvds_read_text/'," datetime.datetime(2019, 3, 22, 23, 10, 4, 738729)",'2019-03-22',None,u'0','StockMarket',u'tim sykes dvds read text'," u'https://old.reddit.com/r/StockMarket/comments/b3w8p1/tim_sykes_dvds_read_text/'}
DEBUG:root:Post added to MongoDB
2019-03-22 23:10:04 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/Stock_Picks/new/>
"
https://old.reddit.com/r/StockMarket/comments/b3vqpo/ndra_and_coda_if_you_want_to_sit_on_fat_stacks/'," datetime.datetime(2019, 3, 22, 23, 10, 4, 752767)",'2019-03-22',None,u'2','StockMarket',u'NDRA and CODA if you want to sit on fat stacks!'," u'https://old.reddit.com/r/StockMarket/comments/b3vqpo/ndra_and_coda_if_you_want_to_sit_on_fat_stacks/'}
DEBUG:root:Post added to MongoDB
2019-03-22 23:10:04 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/StockMarket/new/>
"
https://old.reddit.com/r/StockMarket/comments/b3unqc/free_stock_price_alert_app/'," datetime.datetime(2019, 3, 22, 23, 10, 4, 755053)",'2019-03-22',None,u'3','StockMarket',u'Free stock price alert app?'," u'https://old.reddit.com/r/StockMarket/comments/b3unqc/free_stock_price_alert_app/'}
DEBUG:root:Post added to MongoDB
2019-03-22 23:10:04 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/StockMarket/new/>
"
https://old.reddit.com/r/StockMarket/comments/b3tpab/can_a_single_person_move_a_market/'," datetime.datetime(2019, 3, 22, 23, 10, 4, 757303)",'2019-03-22',None,u'3','StockMarket',u'Can a single person move a market?'," u'https://old.reddit.com/r/StockMarket/comments/b3tpab/can_a_single_person_move_a_market/'}
DEBUG:root:Post added to MongoDB
2019-03-22 23:10:04 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/Stock_Picks/new/>
"
https://old.reddit.com/r/Daytrading/comments/b388m1/how_to_trade_cryptocurrency_2019/'," datetime.datetime(2019, 3, 22, 23, 10, 5, 80116)",'2019-03-22',None,u'1','Daytrading',u'How to trade cryptocurrency 2019'," u'https://www.youtube.com/watch?v=vsbKASVJydI&feature=youtu.be'}
DEBUG:root:Post added to MongoDB
2019-03-22 23:10:05 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/Daytrading/new/>
"
https://old.reddit.com/r/Daytrading/comments/b35qnj/how_did_steven_dux_test_his_strategies_on_1000s/'," datetime.datetime(2019, 3, 22, 23, 10, 5, 83084)",'2019-03-22',None,u'10','Daytrading',"u""How did Steven Dux test his strategies on 1000's of Penny Stocks?"""," u'https://old.reddit.com/r/Daytrading/comments/b35qnj/how_did_steven_dux_test_his_strategies_on_1000s/'}
DEBUG:root:Post added to MongoDB
2019-03-22 23:10:05 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/Daytrading/new/>
"
https://old.reddit.com/r/Daytrading/comments/b3301n/newb_trader_question/'," datetime.datetime(2019, 3, 22, 23, 10, 5, 85383)",'2019-03-22',None,u'2','Daytrading',u'Newb Trader Question'," u'https://old.reddit.com/r/Daytrading/comments/b3301n/newb_trader_question/'}
INFO:scrapy.core.engine:Closing spider (finished)
2019-03-22 23:10:05 [scrapy.core.engine] INFO: Closing spider (finished)
INFO:scrapy.statscollectors:Dumping Scrapy stats:
"
https://old.reddit.com/r/wallstreetbets/comments/b4wcfd/sorry_guys_they_wont_start_you_out_with_box/'," datetime.datetime(2019, 3, 24, 11, 30, 1, 565625)",'2019-03-24',u'Shitpost',u'4','wallstreetbets',"u""Sorry guys, they won't start you out with box spreads"""," u'https://old.reddit.com/r/wallstreetbets/comments/b4wcfd/sorry_guys_they_wont_start_you_out_with_box/'}
DEBUG:root:Post added to MongoDB
2019-03-24 11:30:01 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/wallstreetbets/new/>
"
https://old.reddit.com/r/investing/comments/b4vn9q/what_are_the_odds_the_next_recession_is_big_or/'," datetime.datetime(2019, 3, 24, 11, 35, 2, 534654)",'2019-03-24',None,u'0','investing',u'What are the odds the next recession is big or small one?'," u'https://old.reddit.com/r/investing/comments/b4vn9q/what_are_the_odds_the_next_recession_is_big_or/'}
DEBUG:root:Post added to MongoDB
2019-03-24 11:35:02 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/investing/new/>
"
https://old.reddit.com/r/investing/comments/b4q4ia/post_investing_for_dummies_educational_resources/'," datetime.datetime(2019, 3, 24, 11, 50, 3, 345224)",'2019-03-24',None,u'1','investing',u'Post \u201cinvesting for dummies\u201d educational resources'," u'https://old.reddit.com/r/investing/comments/b4q4ia/post_investing_for_dummies_educational_resources/'}
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-03-24 11:50:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
DEBUG:root:Post added to MongoDB
2019-03-24 11:50:03 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/ValueInvesting/new/>
"
https://old.reddit.com/r/stocks/comments/b4b5eu/high_interest_paid_by_banks_in_india/'," datetime.datetime(2019, 3, 24, 12, 0, 1, 488548)",'2019-03-24',None,u'3','stocks',u'High interest paid by banks in India'," u'https://old.reddit.com/r/stocks/comments/b4b5eu/high_interest_paid_by_banks_in_india/'}
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
2019-03-24 12:00:01 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
2019-03-24 12:00:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
DEBUG:root:Post added to MongoDB
2019-03-24 12:00:02 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/stockaday/new/>
"
https://old.reddit.com/r/StockMarket/comments/b46c1y/does_buying_bullish_and_bearish_sp_etfs_and/'," datetime.datetime(2019, 3, 24, 12, 5, 5, 519046)",'2019-03-24',None,u'12','StockMarket',u'Does buying bullish and bearish s&p ETFs and trying to play them through daily market movements make sense?'," u'https://old.reddit.com/r/StockMarket/comments/b46c1y/does_buying_bullish_and_bearish_sp_etfs_and/'}
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/Daytrading/new/> (referer: None)
2019-03-24 12:05:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/Daytrading/new/> (referer: None)
DEBUG:root:Post added to MongoDB
2019-03-24 12:05:05 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/Daytrading/new/>
"
https://old.reddit.com/r/investing/comments/b4q4nq/how_to_access_earnings_calls_as_an/'," datetime.datetime(2019, 3, 24, 12, 20, 3, 263261)",'2019-03-24',None,u'1','investing',u'How to access earnings calls as an individual/average investor?'," u'https://old.reddit.com/r/investing/comments/b4q4nq/how_to_access_earnings_calls_as_an/'}
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-03-24 12:20:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
DEBUG:root:Post added to MongoDB
2019-03-24 12:20:03 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/ValueInvesting/new/>
"
https://old.reddit.com/r/wallstreetbets/comments/b4wllp/how_do_i_make_money_from_this/'," datetime.datetime(2019, 3, 24, 12, 30, 1, 940682)",'2019-03-24',u'Shitpost',u'0','wallstreetbets',u'How do I make money from this?'," u'https://youtu.be/NOqXgO0MFbQ'}
DEBUG:root:Post added to MongoDB
2019-03-24 12:30:01 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/wallstreetbets/new/>
"
https://old.reddit.com/r/wallstreetbets/comments/b4vdkn/i_know_dell_is_in_the_news_now_but_msft_did_it/'," datetime.datetime(2019, 3, 24, 12, 30, 1, 958581)",'2019-03-24',None,u'8','wallstreetbets',u'I know $DELL is in the news now but $MSFT did it first - invest'," u'https://www.youtube.com/watch?v=XUAsU_zQVMo'}
DEBUG:root:Post added to MongoDB
2019-03-24 12:30:01 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/wallstreetbets/new/>
"
https://old.reddit.com/r/wallstreetbets/comments/b4tj37/this_article_gave_me_aids/'," datetime.datetime(2019, 3, 24, 12, 30, 1, 970023)",'2019-03-24',u'Shitpost',u'5','wallstreetbets',u'This article gave me aids.'," u'https://www.forbes.com/sites/petercohan/2018/08/23/why-stocks-would-soar-on-trump-impeachment/'}
DEBUG:root:Post added to MongoDB
2019-03-24 12:30:01 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/wallstreetbets/new/>
"
https://old.reddit.com/r/investing/comments/b4qa3b/investment_returns_defined/'," datetime.datetime(2019, 3, 24, 12, 30, 3, 362257)",'2019-03-24',None,u'11','investing',u'Investment returns defined'," u'https://old.reddit.com/r/investing/comments/b4qa3b/investment_returns_defined/'}
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-03-24 12:30:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
DEBUG:root:Post added to MongoDB
2019-03-24 12:30:03 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/ValueInvesting/new/>
"
https://old.reddit.com/r/Daytrading/comments/b39tzw/uk_broker_recommendations_that_offer_isa/'," datetime.datetime(2019, 3, 24, 12, 30, 5, 472729)",'2019-03-24',None,u'1','Daytrading',u'UK broker recommendations that offer ISA?'," u'https://old.reddit.com/r/Daytrading/comments/b39tzw/uk_broker_recommendations_that_offer_isa/'}
INFO:scrapy.core.engine:Closing spider (finished)
2019-03-24 12:30:05 [scrapy.core.engine] INFO: Closing spider (finished)
INFO:scrapy.statscollectors:Dumping Scrapy stats:
"
https://old.reddit.com/r/stocks/comments/b4bb90/selling_on_thursdays/'," datetime.datetime(2019, 3, 24, 12, 35, 1, 877979)",'2019-03-24',u'Advice',u'1','stocks',u'Selling on Thursdays.'," u'https://old.reddit.com/r/stocks/comments/b4bb90/selling_on_thursdays/'}
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
2019-03-24 12:35:01 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
2019-03-24 12:35:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
DEBUG:root:Post added to MongoDB
2019-03-24 12:35:02 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/stockaday/new/>
"
https://old.reddit.com/r/investing/comments/b4r43r/roth_ira_question/'," datetime.datetime(2019, 3, 24, 12, 40, 2, 781342)",'2019-03-24',u'Help',u'1','investing',u'Roth IRA question'," u'https://old.reddit.com/r/investing/comments/b4r43r/roth_ira_question/'}
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Daytrading/new/> from <GET https://old.reddit.com/r/daytrading/new/>
2019-03-24 12:40:02 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Daytrading/new/> from <GET https://old.reddit.com/r/daytrading/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-03-24 12:40:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
DEBUG:root:Post added to MongoDB
2019-03-24 12:40:03 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/ValueInvesting/new/>
"
https://old.reddit.com/r/wallstreetbets/comments/b4qmi5/former_us_trade_representative_says_a_china_trade/'," datetime.datetime(2019, 3, 24, 12, 45, 2, 350251)",'2019-03-24',u'Fundamentals',u'30','wallstreetbets',"u'Former US Trade Representative says a China trade deal is probably weeks away, not months'"," u'https://www.cnbc.com/2019/03/23/former-ustr-barshefsky-sees-china-trade-deal-as-probably-weeks-away.html'}
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/investing/new/> (referer: None)
2019-03-24 12:45:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/investing/new/> (referer: None)
DEBUG:root:Post added to MongoDB
2019-03-24 12:45:02 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/investing/new/>
"
https://old.reddit.com/r/StockMarket/comments/b46kog/whats_the_better_buy_nike_or_apple/'," datetime.datetime(2019, 3, 24, 12, 45, 14, 19085)",'2019-03-24',None,u'3','StockMarket',"u'What\u2019s the better buy, Nike or apple?'"," u'https://old.reddit.com/r/StockMarket/comments/b46kog/whats_the_better_buy_nike_or_apple/'}
INFO:scrapy.core.engine:Closing spider (finished)
2019-03-24 12:45:14 [scrapy.core.engine] INFO: Closing spider (finished)
INFO:scrapy.statscollectors:Dumping Scrapy stats:
"
https://old.reddit.com/r/StockMarket/comments/b46rab/assets_worth_1tn_leave_the_uk_since_brexit/'," datetime.datetime(2019, 3, 24, 12, 50, 5, 416436)",'2019-03-24',None,u'6','StockMarket',u'Assets worth \xa31tn leave the UK since Brexit referendum'," u'https://old.reddit.com/r/StockMarket/comments/b46rab/assets_worth_1tn_leave_the_uk_since_brexit/'}
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/Daytrading/new/> (referer: None)
2019-03-24 12:50:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/Daytrading/new/> (referer: None)
DEBUG:root:Post added to MongoDB
2019-03-24 12:50:05 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/Daytrading/new/>
"
https://old.reddit.com/r/ValueInvesting/comments/auqgza/warren_buffet_responds_to_elon_musk_on_moat/'," datetime.datetime(2019, 3, 24, 13, 0, 3, 596592)",'2019-03-24',u'buffett',u'2','ValueInvesting',u'Warren Buffet Responds to Elon Musk on Moat'," u'https://old.reddit.com/r/ValueInvesting/comments/auqgza/warren_buffet_responds_to_elon_musk_on_moat/'}
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/thewallstreet/new/> (referer: None)
2019-03-24 13:00:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/thewallstreet/new/> (referer: None)
DEBUG:root:Post added to MongoDB
2019-03-24 13:00:03 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/thewallstreet/new/>
"
https://old.reddit.com/r/wallstreetbets/comments/b4r17b/the_aapl_and_msft_dilemma_how_to_trade_monday/'," datetime.datetime(2019, 3, 24, 13, 5, 1, 303670)",'2019-03-24',u'Options',u'21','wallstreetbets',"u'The $AAPL and $MSFT Dilemma: How to trade Monday with recession fears, Trump investigation news, etc?'"," u'https://old.reddit.com/r/wallstreetbets/comments/b4r17b/the_aapl_and_msft_dilemma_how_to_trade_monday/'}
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
2019-03-24 13:05:01 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
DEBUG:root:Post added to MongoDB
2019-03-24 13:05:01 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/stocks/new/>
"
https://old.reddit.com/r/StockMarket/comments/b486rc/should_i_buy_apple_now_or_will_it_go_lower_give/'," datetime.datetime(2019, 3, 24, 13, 20, 5, 316268)",'2019-03-24',None,u'0','StockMarket',"u'Should I buy apple now, or will it go lower? Give me an in depth answer if you can'"," u'https://old.reddit.com/r/StockMarket/comments/b486rc/should_i_buy_apple_now_or_will_it_go_lower_give/'}
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/Daytrading/new/> (referer: None)
2019-03-24 13:20:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/Daytrading/new/> (referer: None)
DEBUG:root:Post added to MongoDB
2019-03-24 13:20:05 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/Daytrading/new/>
"
https://old.reddit.com/r/stocks/comments/b4bio6/etfm/'," datetime.datetime(2019, 3, 24, 13, 25, 1, 955900)",'2019-03-24',u'Ticker Discussion',u'1','stocks',u'ETFM'," u'https://old.reddit.com/r/stocks/comments/b4bio6/etfm/'}
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
2019-03-24 13:25:02 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
2019-03-24 13:25:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
DEBUG:root:Post added to MongoDB
2019-03-24 13:25:02 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/stockaday/new/>
"
https://old.reddit.com/r/investing/comments/b4rmx9/looking_for_advestment_advice_regarding/'," datetime.datetime(2019, 3, 24, 13, 25, 2, 944621)",'2019-03-24',u'Discussion',u'1','investing',u'Looking for advestment advice regarding diversification.'," u'https://old.reddit.com/r/investing/comments/b4rmx9/looking_for_advestment_advice_regarding/'}
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-03-24 13:25:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
DEBUG:root:Post added to MongoDB
2019-03-24 13:25:03 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/ValueInvesting/new/>
"
https://old.reddit.com/r/investing/comments/b4rpla/was_there_many_bears_in_08/'," datetime.datetime(2019, 3, 24, 13, 50, 3, 193338)",'2019-03-24',None,u'3','investing',u'Was there many bears in 08?'," u'https://old.reddit.com/r/investing/comments/b4rpla/was_there_many_bears_in_08/'}
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Daytrading/new/> from <GET https://old.reddit.com/r/daytrading/new/>
2019-03-24 13:50:03 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Daytrading/new/> from <GET https://old.reddit.com/r/daytrading/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-03-24 13:50:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
DEBUG:root:Post added to MongoDB
2019-03-24 13:50:03 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/ValueInvesting/new/>
"
https://old.reddit.com/r/wallstreetbets/comments/b4r8q7/in_2018_5500_retail_store_closures_were_announced/'," datetime.datetime(2019, 3, 24, 14, 5, 1, 945955)",'2019-03-24',u'Fundamentals',u'34','wallstreetbets',"u'In 2018, 5,500 retail store closures were announced. We have not finished Q1 for 2019 and already over 5,800 store closures have been announced'"," u'https://www.businessinsider.com/stores-closing-in-2019-list-2019-3'}
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
2019-03-24 14:05:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
2019-03-24 14:05:02 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
DEBUG:root:Post added to MongoDB
2019-03-24 14:05:02 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/stocks/new/>
"
https://old.reddit.com/r/wallstreetbets/comments/b4r8t4/urban_trade_breakdowns_charlie_lee_calls_the/'," datetime.datetime(2019, 3, 24, 14, 10, 1, 492472)",'2019-03-24',u'Discussion',u'88','wallstreetbets',u'Urban Trade Breakdowns: Charlie Lee Calls the Crypto Bubble Top'," u'https://old.reddit.com/r/wallstreetbets/comments/b4r8t4/urban_trade_breakdowns_charlie_lee_calls_the/'}
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
2019-03-24 14:10:01 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
2019-03-24 14:10:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
DEBUG:root:Post added to MongoDB
2019-03-24 14:10:01 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/stocks/new/>
"
https://old.reddit.com/r/investing/comments/b4rxah/can_anyone_recommend_good_readings_on_investing/'," datetime.datetime(2019, 3, 24, 14, 15, 3, 262084)",'2019-03-24',None,u'1','investing',u'Can anyone recommend good readings on investing in cyclical stocks? Specifically industrials and transportation? Or top investors in the space?'," u'https://old.reddit.com/r/investing/comments/b4rxah/can_anyone_recommend_good_readings_on_investing/'}
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-03-24 14:15:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
DEBUG:root:Post added to MongoDB
2019-03-24 14:15:03 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/ValueInvesting/new/>
"
https://old.reddit.com/r/stocks/comments/b4c5qe/whats_this_about_a_recession/'," datetime.datetime(2019, 3, 24, 14, 20, 1, 855068)",'2019-03-24',None,u'0','stocks',"u""What's this about a recession?"""," u'https://old.reddit.com/r/stocks/comments/b4c5qe/whats_this_about_a_recession/'}
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
2019-03-24 14:20:01 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
2019-03-24 14:20:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
DEBUG:root:Post added to MongoDB
2019-03-24 14:20:02 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/stockaday/new/>
"
https://old.reddit.com/r/wallstreetbets/comments/b4rn6n/rant_on_tesla_favorite_company/'," datetime.datetime(2019, 3, 24, 14, 30, 1, 702931)",'2019-03-24',u'Discussion',u'2','wallstreetbets',u'Rant on Tesla. Favorite company'," u'https://old.reddit.com/r/wallstreetbets/comments/b4rn6n/rant_on_tesla_favorite_company/'}
DEBUG:root:Post added to MongoDB
2019-03-24 14:30:01 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/wallstreetbets/new/>
"
https://old.reddit.com/r/wallstreetbets/comments/b4rb20/monday_morning_open_get_ready_bulls_we_riding/'," datetime.datetime(2019, 3, 24, 14, 30, 1, 705330)",'2019-03-24',u'Shitpost',u'257','wallstreetbets',"u'Monday morning open, get ready bulls! We riding this bull run to heaven'"," u'https://gfycat.com/adorablekaleidoscopicdrake'}
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
2019-03-24 14:30:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
DEBUG:root:Post added to MongoDB
2019-03-24 14:30:01 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/stocks/new/>
"
https://old.reddit.com/r/wallstreetbets/comments/b4rsld/levi_dd/'," datetime.datetime(2019, 3, 24, 14, 35, 1, 368705)",'2019-03-24',u'Stocks',u'12','wallstreetbets',u'LEVI DD'," u'https://old.reddit.com/r/wallstreetbets/comments/b4rsld/levi_dd/'}
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
2019-03-24 14:35:01 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
2019-03-24 14:35:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
DEBUG:root:Post added to MongoDB
2019-03-24 14:35:01 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/stocks/new/>
"
https://old.reddit.com/r/investing/comments/b4sk2c/how_much_do_you_have_in_stocks_and_how_much_are/'," datetime.datetime(2019, 3, 24, 14, 35, 2, 630220)",'2019-03-24',None,u'31','investing',"u'How much do you have in stocks, and how much are you paid in dividends per month?'"," u'https://old.reddit.com/r/investing/comments/b4sk2c/how_much_do_you_have_in_stocks_and_how_much_are/'}
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Daytrading/new/> from <GET https://old.reddit.com/r/daytrading/new/>
2019-03-24 14:35:02 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Daytrading/new/> from <GET https://old.reddit.com/r/daytrading/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-03-24 14:35:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
DEBUG:root:Post added to MongoDB
2019-03-24 14:35:03 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/ValueInvesting/new/>
"
https://old.reddit.com/r/StockMarket/comments/b487de/stockmarket_today/'," datetime.datetime(2019, 3, 24, 14, 50, 4, 323624)",'2019-03-24',None,u'14','StockMarket',u'StockMarket Today'," u'https://old.reddit.com/r/StockMarket/comments/b487de/stockmarket_today/'}
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/Daytrading/new/> (referer: None)
2019-03-24 14:50:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/Daytrading/new/> (referer: None)
DEBUG:root:Post added to MongoDB
2019-03-24 14:50:04 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/Daytrading/new/>
"
https://old.reddit.com/r/investing/comments/b4sovh/what_do_you_think_will_happen_to_gbp_510_years/'," datetime.datetime(2019, 3, 24, 14, 55, 2, 820148)",'2019-03-24',None,u'1','investing',u'What do you think will happen to GBP 5-10 years after Brexit? Assuming a deal is made with the EU'," u'https://old.reddit.com/r/investing/comments/b4sovh/what_do_you_think_will_happen_to_gbp_510_years/'}
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Daytrading/new/> from <GET https://old.reddit.com/r/daytrading/new/>
2019-03-24 14:55:02 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Daytrading/new/> from <GET https://old.reddit.com/r/daytrading/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-03-24 14:55:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
DEBUG:root:Post added to MongoDB
2019-03-24 14:55:03 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/ValueInvesting/new/>
"
https://old.reddit.com/r/ValueInvesting/comments/av3blc/bloody_beginner/'," datetime.datetime(2019, 3, 24, 14, 55, 3, 520959)",'2019-03-24',None,u'1','ValueInvesting',u'Bloody Beginner'," u'https://old.reddit.com/r/ValueInvesting/comments/av3blc/bloody_beginner/'}
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/thewallstreet/new/> (referer: None)
2019-03-24 14:55:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/thewallstreet/new/> (referer: None)
DEBUG:root:Post added to MongoDB
2019-03-24 14:55:03 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/thewallstreet/new/>
"
https://old.reddit.com/r/stocks/comments/b4e1n9/what_the_yield_curve_inversion_really_means/'," datetime.datetime(2019, 3, 24, 15, 0, 1, 494855)",'2019-03-24',None,u'2','stocks',"u'What the Yield Curve Inversion Really Means, According to the Professor Who Discovered It'"," u'https://old.reddit.com/r/stocks/comments/b4e1n9/what_the_yield_curve_inversion_really_means/'}
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
2019-03-24 15:00:01 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
2019-03-24 15:00:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
DEBUG:root:Post added to MongoDB
2019-03-24 15:00:02 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/stockaday/new/>
"
https://old.reddit.com/r/investing/comments/b4sw3m/has_anyone_played_around_with_investopedia_stock/'," datetime.datetime(2019, 3, 24, 15, 0, 2, 576326)",'2019-03-24',None,u'16','investing',u'Has anyone played around with investopedia stock simulator ?'," u'https://old.reddit.com/r/investing/comments/b4sw3m/has_anyone_played_around_with_investopedia_stock/'}
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Daytrading/new/> from <GET https://old.reddit.com/r/daytrading/new/>
2019-03-24 15:00:02 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Daytrading/new/> from <GET https://old.reddit.com/r/daytrading/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-03-24 15:00:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
DEBUG:root:Post added to MongoDB
2019-03-24 15:00:03 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/ValueInvesting/new/>
"
https://old.reddit.com/r/stocks/comments/b4fu22/stocks/'," datetime.datetime(2019, 3, 24, 15, 10, 1, 637429)",'2019-03-24',None,u'0','stocks',u'Stocks'," u'https://old.reddit.com/r/stocks/comments/b4fu22/stocks/'}
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
2019-03-24 15:10:01 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
2019-03-24 15:10:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
DEBUG:root:Post added to MongoDB
2019-03-24 15:10:02 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/stockaday/new/>
"
https://old.reddit.com/r/wallstreetbets/comments/b4ygxv/general_electric_ge/'," datetime.datetime(2019, 3, 24, 15, 25, 3, 472322)",'2019-03-24',u'Discussion',u'0','wallstreetbets',u'General Electric (GE)?'," u'https://old.reddit.com/r/wallstreetbets/comments/b4ygxv/general_electric_ge/'}
DEBUG:root:Post added to MongoDB
2019-03-24 15:25:03 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/wallstreetbets/new/>
"
https://old.reddit.com/r/investing/comments/b4th2a/investing_in_canadian_cannabis_stocks/'," datetime.datetime(2019, 3, 24, 15, 30, 5, 256606)",'2019-03-24',None,u'0','investing',u'Investing in canadian cannabis stocks.'," u'https://old.reddit.com/r/investing/comments/b4th2a/investing_in_canadian_cannabis_stocks/'}
DEBUG:root:Post added to MongoDB
2019-03-24 15:30:05 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/investing/new/>
"
https://old.reddit.com/r/investing/comments/b4tffr/us_markets_news_stocks_slip_as_the_federal/'," datetime.datetime(2019, 3, 24, 15, 30, 5, 298938)",'2019-03-24',None,u'50','investing',u'US Markets News: Stocks Slip as the Federal Reserve Expresses Caution on Economic Slowdown'," u'https://old.reddit.com/r/investing/comments/b4tffr/us_markets_news_stocks_slip_as_the_federal/'}
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-03-24 15:30:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
DEBUG:root:Post added to MongoDB
2019-03-24 15:30:05 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/ValueInvesting/new/>
"
https://old.reddit.com/r/Daytrading/comments/b3aapo/importance_of_psychological_factors_in_day_trading/'," datetime.datetime(2019, 3, 24, 15, 30, 7, 906507)",'2019-03-24',None,u'31','Daytrading',u'Importance of Psychological Factors in Day Trading'," u'https://www.tradingsitereviews.com/day-trading/importance-of-psychological-factors-in-day-trading/'}
INFO:scrapy.core.engine:Closing spider (finished)
2019-03-24 15:30:07 [scrapy.core.engine] INFO: Closing spider (finished)
INFO:scrapy.statscollectors:Dumping Scrapy stats:
"
https://old.reddit.com/r/wallstreetbets/comments/b4s999/investors_shun_mylan_as_sales_misses_overshadow/'," datetime.datetime(2019, 3, 24, 15, 35, 3, 289779)",'2019-03-24',u'Stocks',u'0','wallstreetbets',u'Investors Shun Mylan as Sales Misses Overshadow Cheap Shares'," u'https://www.bloomberg.com/news/articles/2019-03-22/investors-shun-mylan-as-forecast-misses-overshadow-cheap-shares'}
DEBUG:root:Post added to MongoDB
2019-03-24 15:35:03 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/stocks/new/>
"
https://old.reddit.com/r/wallstreetbets/comments/b4xvnk/unpopular_opinion_those_who_constantly_voice/'," datetime.datetime(2019, 3, 24, 15, 40, 3, 583417)",'2019-03-24',u'Discussion',u'1516','wallstreetbets',"u'Unpopular Opinion : Those who constantly voice opinions on which way the market goes, needs to post positions showing they got the balls to follow their own direction.'"," u'https://old.reddit.com/r/wallstreetbets/comments/b4xvnk/unpopular_opinion_those_who_constantly_voice/'}
DEBUG:root:Post added to MongoDB
2019-03-24 15:40:03 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/wallstreetbets/new/>
"
https://old.reddit.com/r/wallstreetbets/comments/b4w82s/i_had_planned_to_hold_mongodb_at_3065_for_the/'," datetime.datetime(2019, 3, 24, 15, 40, 3, 671282)",'2019-03-24',u'Stocks',u'32','wallstreetbets',"u'I had planned to hold MongoDB at $30.65 for the long term, but went went for a 98.50 entry into $DIS soon after. $MDB is now at $152.76'"," u'https://old.reddit.com/r/wallstreetbets/comments/b4w82s/i_had_planned_to_hold_mongodb_at_3065_for_the/'}
DEBUG:root:Post added to MongoDB
2019-03-24 15:40:03 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/wallstreetbets/new/>
"
https://old.reddit.com/r/wallstreetbets/comments/b4w77i/the_lifecycle_of_losses/'," datetime.datetime(2019, 3, 24, 15, 40, 3, 682698)",'2019-03-24',u'Shitpost',u'232','wallstreetbets',u'The lifecycle of losses'," u'https://old.reddit.com/r/wallstreetbets/comments/b4w77i/the_lifecycle_of_losses/'}
DEBUG:root:Post added to MongoDB
2019-03-24 15:40:03 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/wallstreetbets/new/>
"
https://old.reddit.com/r/wallstreetbets/comments/b4w35d/dont_panic_with_treasury_yield_rates_inverting/'," datetime.datetime(2019, 3, 24, 15, 40, 3, 693084)",'2019-03-24',u'Discussion',u'5','wallstreetbets',"u""Don't Panic With Treasury Yield Rates Inverting"""," u'https://www.reddit.com/user/Fatherthinger/comments/b4ul63/dont_panic_with_treasury_yield_rates_inverting/'}
DEBUG:root:Post added to MongoDB
2019-03-24 15:40:03 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/wallstreetbets/new/>
"
https://old.reddit.com/r/wallstreetbets/comments/b4vrif/consider_this_when_going_in_on_ba/'," datetime.datetime(2019, 3, 24, 15, 40, 3, 702882)",'2019-03-24',u'Discussion',u'2','wallstreetbets',u'Consider this when going in on $BA.'," u'https://old.reddit.com/r/wallstreetbets/comments/b4vrif/consider_this_when_going_in_on_ba/'}
DEBUG:root:Post added to MongoDB
2019-03-24 15:40:03 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/wallstreetbets/new/>
"
https://old.reddit.com/r/wallstreetbets/comments/b4vbry/short_nclh_dd_inside/'," datetime.datetime(2019, 3, 24, 15, 40, 3, 713909)",'2019-03-24',u'Shitpost',u'7','wallstreetbets',u'Short $NCLH. DD Inside.'," u'https://old.reddit.com/r/wallstreetbets/comments/b4vbry/short_nclh_dd_inside/'}
DEBUG:root:Post added to MongoDB
2019-03-24 15:40:03 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/wallstreetbets/new/>
"
https://old.reddit.com/r/wallstreetbets/comments/b4u893/go_long_on_cbd_stocks_jelly_belly_has_joined_the/'," datetime.datetime(2019, 3, 24, 15, 40, 3, 722857)",'2019-03-24',u'Shitpost',u'26','wallstreetbets',"u'Go LONG on CBD stocks, Jelly Belly has joined the game.'"," u'https://www.abc15.com/news/local-news/water-cooler/cbd-jelly-beans-jelly-belly-founder-releases-cannabis-infused-jelly-beans'}
DEBUG:root:Post added to MongoDB
2019-03-24 15:40:03 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/wallstreetbets/new/>
"
https://old.reddit.com/r/wallstreetbets/comments/b4u2s3/so_what_happens_if_i_robinhood_on_robinhood/'," datetime.datetime(2019, 3, 24, 15, 40, 3, 732314)",'2019-03-24',u'Shitpost',u'64','wallstreetbets',u'So what happens if I Robinhood on Robinhood?'," u'https://old.reddit.com/r/wallstreetbets/comments/b4u2s3/so_what_happens_if_i_robinhood_on_robinhood/'}
DEBUG:root:Post added to MongoDB
2019-03-24 15:40:03 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/wallstreetbets/new/>
"
https://old.reddit.com/r/wallstreetbets/comments/b4tpon/china_refuses_to_concede_on_us_demands_to_ease/'," datetime.datetime(2019, 3, 24, 15, 40, 3, 741174)",'2019-03-24',u'Futures',u'37','wallstreetbets',u'China refuses to concede on U.S. demands to ease curbs on tech firms - FT'," u'http://uk.reuters.com/article/uk-usa-trade-china-tech/china-refuses-to-concede-on-u-s-demands-to-ease-curbs-on-tech-firms-ft-idUKKCN1R506Z'}
DEBUG:root:Post added to MongoDB
2019-03-24 15:40:03 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/wallstreetbets/new/>
"
https://old.reddit.com/r/wallstreetbets/comments/b4sz9v/good_morning/'," datetime.datetime(2019, 3, 24, 15, 40, 3, 750353)",'2019-03-24',u'Gain',u'52','wallstreetbets',u'Good morning \U0001f43b'," u'https://old.reddit.com/r/wallstreetbets/comments/b4sz9v/good_morning/'}
DEBUG:root:Post added to MongoDB
2019-03-24 15:40:03 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/wallstreetbets/new/>
"
https://old.reddit.com/r/wallstreetbets/comments/b4snyj/dell_autism_hiring_program_dellcom/'," datetime.datetime(2019, 3, 24, 15, 40, 3, 758739)",'2019-03-24',u'Shitpost',u'39','wallstreetbets',u'Dell Autism Hiring Program (dell.com).'," u'https://jobs.dell.com/neurodiversity'}
DEBUG:root:Post added to MongoDB
2019-03-24 15:40:03 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/stocks/new/>
"
https://old.reddit.com/r/stocks/comments/b4srzc/request_for_reccomendations/'," datetime.datetime(2019, 3, 24, 15, 40, 3, 909318)",'2019-03-24',None,u'5','stocks',u'Request for reccomendations'," u'https://old.reddit.com/r/stocks/comments/b4srzc/request_for_reccomendations/'}
DEBUG:root:Post added to MongoDB
2019-03-24 15:40:03 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/stocks/new/>
"
https://old.reddit.com/r/stocks/comments/b4r5cg/what_companys_stock_are_you_definitely_not_buying/'," datetime.datetime(2019, 3, 24, 15, 40, 3, 919509)",'2019-03-24',None,u'53','stocks',u'What company\u2019s stock are you definitely not buying. Why?'," u'https://old.reddit.com/r/stocks/comments/b4r5cg/what_companys_stock_are_you_definitely_not_buying/'}
DEBUG:root:Post added to MongoDB
2019-03-24 15:40:03 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/stocks/new/>
"
https://old.reddit.com/r/stocks/comments/b4psww/thoughts_on_main_and_apts_thoughts_on_reits_in/'," datetime.datetime(2019, 3, 24, 15, 40, 3, 928506)",'2019-03-24',u'Advice Request',u'5','stocks',u'Thoughts on MAIN and APTS? Thoughts on reits in general?'," u'https://old.reddit.com/r/stocks/comments/b4psww/thoughts_on_main_and_apts_thoughts_on_reits_in/'}
DEBUG:root:Post added to MongoDB
2019-03-24 15:40:03 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/stocks/new/>
"
https://old.reddit.com/r/stocks/comments/b4nekr/vanguard_ibd_leaderboard_stock_locator/'," datetime.datetime(2019, 3, 24, 15, 40, 3, 940808)",'2019-03-24',None,u'1','stocks',u'Vanguard / IBD Leaderboard / Stock Locator'," u'https://old.reddit.com/r/stocks/comments/b4nekr/vanguard_ibd_leaderboard_stock_locator/'}
DEBUG:root:Post added to MongoDB
2019-03-24 15:40:03 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/stocks/new/>
"
https://old.reddit.com/r/stocks/comments/b4mkfu/how_to_invest_for_an_ira/'," datetime.datetime(2019, 3, 24, 15, 40, 3, 949329)",'2019-03-24',None,u'1','stocks',u'How to invest for an IRA.'," u'https://old.reddit.com/r/stocks/comments/b4mkfu/how_to_invest_for_an_ira/'}
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
2019-03-24 15:40:03 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
DEBUG:root:Post added to MongoDB
2019-03-24 15:40:03 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/stocks/new/>
"
https://old.reddit.com/r/stocks/comments/b4mcbx/march_25_2019equities_market_direction_up_or_down/'," datetime.datetime(2019, 3, 24, 15, 40, 3, 965641)",'2019-03-24',None,u'8','stocks',u'March 25 2019...Equities Market direction up or down?'," u'https://old.reddit.com/r/stocks/comments/b4mcbx/march_25_2019equities_market_direction_up_or_down/'}
DEBUG:root:Post added to MongoDB
2019-03-24 15:40:03 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/stocks/new/>
"
https://old.reddit.com/r/stocks/comments/b4m8nk/question_on_roth_ira/'," datetime.datetime(2019, 3, 24, 15, 40, 3, 974185)",'2019-03-24',None,u'1','stocks',u'Question on Roth IRA'," u'https://old.reddit.com/r/stocks/comments/b4m8nk/question_on_roth_ira/'}
DEBUG:root:Post added to MongoDB
2019-03-24 15:40:03 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/stocks/new/>
"
https://old.reddit.com/r/stocks/comments/b4m338/nike_pushes_sports_bras_as_it_hopes_to_boost/'," datetime.datetime(2019, 3, 24, 15, 40, 3, 984619)",'2019-03-24',None,u'80','stocks',u'Nike pushes sports bras as it hopes to boost worldwide sales'," u'https://old.reddit.com/r/stocks/comments/b4m338/nike_pushes_sports_bras_as_it_hopes_to_boost/'}
DEBUG:root:Post added to MongoDB
2019-03-24 15:40:03 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/stocks/new/>
"
https://old.reddit.com/r/stocks/comments/b4lzn4/when_will_uber_and_other_awesome_ipos_be_included/'," datetime.datetime(2019, 3, 24, 15, 40, 3, 993122)",'2019-03-24',None,u'1','stocks',u'When will Uber and other awesome IPOs be included the S&P 500 Index?'," u'https://old.reddit.com/r/stocks/comments/b4lzn4/when_will_uber_and_other_awesome_ipos_be_included/'}
DEBUG:root:Post added to MongoDB
2019-03-24 15:40:04 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/stocks/new/>
"
https://old.reddit.com/r/stocks/comments/b4lgzq/ipos/'," datetime.datetime(2019, 3, 24, 15, 40, 4, 10461)",'2019-03-24',None,u'4','stocks',u'IPOs?'," u'https://old.reddit.com/r/stocks/comments/b4lgzq/ipos/'}
DEBUG:root:Post added to MongoDB
2019-03-24 15:40:04 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/stocks/new/>
"
https://old.reddit.com/r/stocks/comments/b4l9wt/is_it_better_to_put_1000_into_a_single_high_div/'," datetime.datetime(2019, 3, 24, 15, 40, 4, 18698)",'2019-03-24',None,u'224','stocks',"u'Is it better to put $1,000 into a single high div yield stock or $200 into five different high div yield stocks?'"," u'https://old.reddit.com/r/stocks/comments/b4l9wt/is_it_better_to_put_1000_into_a_single_high_div/'}
DEBUG:root:Post added to MongoDB
2019-03-24 15:40:04 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/stocks/new/>
"
https://old.reddit.com/r/stocks/comments/b4jph2/shares_less_than_100x/'," datetime.datetime(2019, 3, 24, 15, 40, 4, 40770)",'2019-03-24',u'Question',u'0','stocks',u'Shares less than 100x?'," u'https://old.reddit.com/r/stocks/comments/b4jph2/shares_less_than_100x/'}
DEBUG:root:Post added to MongoDB
2019-03-24 15:40:04 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/stocks/new/>
"
https://old.reddit.com/r/stocks/comments/b4itff/dividend_yield_data_set/'," datetime.datetime(2019, 3, 24, 15, 40, 4, 50261)",'2019-03-24',None,u'2','stocks',u'Dividend yield data set'," u'https://old.reddit.com/r/stocks/comments/b4itff/dividend_yield_data_set/'}
DEBUG:root:Post added to MongoDB
2019-03-24 15:40:04 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/stocks/new/>
"
https://old.reddit.com/r/stocks/comments/b4ho2e/big_tech_ipos_buy_an_index_instead/'," datetime.datetime(2019, 3, 24, 15, 40, 4, 59139)",'2019-03-24',None,u'1','stocks',u'Big tech IPOs - Buy an Index instead?'," u'https://old.reddit.com/r/stocks/comments/b4ho2e/big_tech_ipos_buy_an_index_instead/'}
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
2019-03-24 15:40:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
DEBUG:root:Post added to MongoDB
2019-03-24 15:40:04 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/stockaday/new/>
"
https://old.reddit.com/r/investing/comments/b4yw1f/whats_a_good_bank_with_high_cd_rates/'," datetime.datetime(2019, 3, 24, 15, 40, 4, 831163)",'2019-03-24',u'Discussion',u'4','investing',u'What\u2019s a good bank with high CD Rates?'," u'https://old.reddit.com/r/investing/comments/b4yw1f/whats_a_good_bank_with_high_cd_rates/'}
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Daytrading/new/> from <GET https://old.reddit.com/r/daytrading/new/>
2019-03-24 15:40:04 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Daytrading/new/> from <GET https://old.reddit.com/r/daytrading/new/>
DEBUG:root:Post added to MongoDB
2019-03-24 15:40:04 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/stockaday/new/>
"
https://old.reddit.com/r/investing/comments/b4yemn/do_you_think_facebooks_messaging_integration/'," datetime.datetime(2019, 3, 24, 15, 40, 4, 866733)",'2019-03-24',u'Discussion',u'0','investing',"u""Do you think facebook's messaging integration along with their coin will be successful. beginning to change my view on it. Might be a sound investment right now"""," u'https://old.reddit.com/r/investing/comments/b4yemn/do_you_think_facebooks_messaging_integration/'}
DEBUG:root:Post added to MongoDB
2019-03-24 15:40:04 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/stockaday/new/>
"
https://old.reddit.com/r/investing/comments/b4y9ik/net_asset_value_of_spy/'," datetime.datetime(2019, 3, 24, 15, 40, 4, 897693)",'2019-03-24',None,u'1','investing',u'Net asset value of SPY'," u'https://old.reddit.com/r/investing/comments/b4y9ik/net_asset_value_of_spy/'}
DEBUG:root:Post added to MongoDB
2019-03-24 15:40:04 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/investing/new/>
"
https://old.reddit.com/r/investing/comments/b4y5so/does_anyone_know_how_to_add_bond_indexes_to_the/'," datetime.datetime(2019, 3, 24, 15, 40, 4, 906121)",'2019-03-24',None,u'0','investing',u'Does anyone know how to add bond indexes to the Apple Stocks App?'," u'https://old.reddit.com/r/investing/comments/b4y5so/does_anyone_know_how_to_add_bond_indexes_to_the/'}
DEBUG:root:Post added to MongoDB
2019-03-24 15:40:04 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/stockaday/new/>
"
https://old.reddit.com/r/investing/comments/b4xu4k/is_apple_still_at_a_good_entry/'," datetime.datetime(2019, 3, 24, 15, 40, 4, 931893)",'2019-03-24',u'Discussion',u'8','investing',u'Is Apple still at a good entry?'," u'https://old.reddit.com/r/investing/comments/b4xu4k/is_apple_still_at_a_good_entry/'}
DEBUG:root:Post added to MongoDB
2019-03-24 15:40:04 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/stockaday/new/>
"
https://old.reddit.com/r/investing/comments/b4wvxt/why_is_jpm_rapidly_declining/'," datetime.datetime(2019, 3, 24, 15, 40, 4, 959110)",'2019-03-24',None,u'170','investing',u'Why is JPM rapidly declining?'," u'https://old.reddit.com/r/investing/comments/b4wvxt/why_is_jpm_rapidly_declining/'}
DEBUG:root:Post added to MongoDB
2019-03-24 15:40:04 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/investing/new/>
"
https://old.reddit.com/r/investing/comments/b4wutt/the_bogleheads_guide_to_the_threefund_portfolio/'," datetime.datetime(2019, 3, 24, 15, 40, 4, 969078)",'2019-03-24',None,u'60','investing',u'The Bogleheads\u2019 Guide to the Three-Fund Portfolio by Four Pillar Freedom'," u'https://old.reddit.com/r/investing/comments/b4wutt/the_bogleheads_guide_to_the_threefund_portfolio/'}
DEBUG:root:Post added to MongoDB
2019-03-24 15:40:04 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/stockaday/new/>
"
https://old.reddit.com/r/investing/comments/b4wu1c/comparison_chart_of_asset_class_tax_efficiency/'," datetime.datetime(2019, 3, 24, 15, 40, 4, 997621)",'2019-03-24',None,u'0','investing',u'Comparison chart of asset class tax efficiency'," u'https://old.reddit.com/r/investing/comments/b4wu1c/comparison_chart_of_asset_class_tax_efficiency/'}
DEBUG:root:Post added to MongoDB
2019-03-24 15:40:05 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/investing/new/>
"
https://old.reddit.com/r/investing/comments/b4wsxz/what_do_you_use_for_stocks_portfolio_tracker/'," datetime.datetime(2019, 3, 24, 15, 40, 5, 6004)",'2019-03-24',u'Help',u'14','investing',u'What do you use for stocks portfolio tracker?'," u'https://old.reddit.com/r/investing/comments/b4wsxz/what_do_you_use_for_stocks_portfolio_tracker/'}
DEBUG:root:Post added to MongoDB
2019-03-24 15:40:05 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/investing/new/>
"
https://old.reddit.com/r/investing/comments/b4ws9u/investing_in_a_house_for_my_and_my_roommates/'," datetime.datetime(2019, 3, 24, 15, 40, 5, 55277)",'2019-03-24',None,u'19','investing',u'Investing in a house for my and my roommates'," u'https://old.reddit.com/r/investing/comments/b4ws9u/investing_in_a_house_for_my_and_my_roommates/'}
DEBUG:root:Post added to MongoDB
2019-03-24 15:40:05 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/investing/new/>
"
https://old.reddit.com/r/investing/comments/b4wjch/should_i_sell_of_my_individual_tax_account_and/'," datetime.datetime(2019, 3, 24, 15, 40, 5, 64809)",'2019-03-24',u'Discussion',u'2','investing',u'Should I sell of my individual tax account and put those funds in my new IRA?'," u'https://old.reddit.com/r/investing/comments/b4wjch/should_i_sell_of_my_individual_tax_account_and/'}
DEBUG:root:Post added to MongoDB
2019-03-24 15:40:05 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/investing/new/>
"
https://old.reddit.com/r/investing/comments/b4vscq/can_you_not_max_out_a_401k_and_contribute_to_both/'," datetime.datetime(2019, 3, 24, 15, 40, 5, 74814)",'2019-03-24',None,u'1','investing',u'Can you not max out a 401k and contribute to both a 401k and a Roth IRA??'," u'https://old.reddit.com/r/investing/comments/b4vscq/can_you_not_max_out_a_401k_and_contribute_to_both/'}
DEBUG:root:Post added to MongoDB
2019-03-24 15:40:05 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/investing/new/>
"
https://old.reddit.com/r/investing/comments/b4vb7l/who_controls_the_pricing_of_the_treasury_yields/'," datetime.datetime(2019, 3, 24, 15, 40, 5, 93991)",'2019-03-24',u'Education',u'33','investing',u'Who controls the pricing of the treasury yields?'," u'https://old.reddit.com/r/investing/comments/b4vb7l/who_controls_the_pricing_of_the_treasury_yields/'}
DEBUG:root:Post added to MongoDB
2019-03-24 15:40:05 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/investing/new/>
"
https://old.reddit.com/r/investing/comments/b4v6b1/help_new_to_investments_here_is_my_portfolio/'," datetime.datetime(2019, 3, 24, 15, 40, 5, 102680)",'2019-03-24',None,u'0','investing',u'HELP! New to Investments... here is my portfolio......'," u'https://old.reddit.com/r/investing/comments/b4v6b1/help_new_to_investments_here_is_my_portfolio/'}
DEBUG:root:Post added to MongoDB
2019-03-24 15:40:05 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/investing/new/>
"
https://old.reddit.com/r/investing/comments/b4unv1/how_do_i_invest_in_an_sp_500_index_fund_using/'," datetime.datetime(2019, 3, 24, 15, 40, 5, 113273)",'2019-03-24',None,u'0','investing',u'How do I invest in an S&P 500 index fund using Interactive Brokers?'," u'https://old.reddit.com/r/investing/comments/b4unv1/how_do_i_invest_in_an_sp_500_index_fund_using/'}
DEBUG:root:Post added to MongoDB
2019-03-24 15:40:05 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/investing/new/>
"
https://old.reddit.com/r/investing/comments/b4tsbv/stadia_and_the_future_of_the_gaming_industry/'," datetime.datetime(2019, 3, 24, 15, 40, 5, 121949)",'2019-03-24',u'Discussion',u'0','investing',u'Stadia and the Future of the Gaming Industry'," u'https://old.reddit.com/r/investing/comments/b4tsbv/stadia_and_the_future_of_the_gaming_industry/'}
DEBUG:root:Post added to MongoDB
2019-03-24 15:40:05 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/investing/new/>
"
https://old.reddit.com/r/investing/comments/b4tmo4/can_a_stock_have_an_implied_volatility_if_there/'," datetime.datetime(2019, 3, 24, 15, 40, 5, 132488)",'2019-03-24',None,u'0','investing',u'Can a stock have an implied volatility if there are no options for that stock?'," u'https://old.reddit.com/r/investing/comments/b4tmo4/can_a_stock_have_an_implied_volatility_if_there/'}
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-03-24 15:40:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
DEBUG:root:Post added to MongoDB
2019-03-24 15:40:05 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/ValueInvesting/new/>
"
https://old.reddit.com/r/Trading/comments/b4yai3/trade_your_favorite_stock_using_ubetcoin_apple/'," datetime.datetime(2019, 3, 24, 15, 40, 6, 113193)",'2019-03-24',None,u'0','Trading',"u'Trade your Favorite Stock using Ubetcoin Apple , NFLX, MSFT with Block chain Technology'"," u'https://old.reddit.com/r/Trading/comments/b4yai3/trade_your_favorite_stock_using_ubetcoin_apple/'}
DEBUG:root:Post added to MongoDB
2019-03-24 15:40:06 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/thewallstreet/new/>
"
https://old.reddit.com/r/thewallstreet/comments/b2s8e1/deviations_poc_and_value_area_for_tuesday_march/'," datetime.datetime(2019, 3, 24, 15, 40, 6, 304300)",'2019-03-24',None,u'14','thewallstreet',"u'Deviations, POC, and Value Area for Tuesday, March 19, 2019'"," u'https://i.imgur.com/8fx69Fv.png'}
DEBUG:root:Post added to MongoDB
2019-03-24 15:40:06 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/Trading/new/>
"
https://old.reddit.com/r/thewallstreet/comments/b2onoy/nightly_trading_discussion_march_1819/'," datetime.datetime(2019, 3, 24, 15, 40, 6, 389929)",'2019-03-24',u'Daily',u'6','thewallstreet',u'Nightly Trading Discussion - (March 18/19)'," u'https://old.reddit.com/r/thewallstreet/comments/b2onoy/nightly_trading_discussion_march_1819/'}
DEBUG:root:Post added to MongoDB
2019-03-24 15:40:06 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/Trading/new/>
"
https://old.reddit.com/r/thewallstreet/comments/b2nb38/post_market_discussion_march_18/'," datetime.datetime(2019, 3, 24, 15, 40, 6, 417054)",'2019-03-24',u'Daily',u'5','thewallstreet',u'Post Market Discussion - (March 18)'," u'https://old.reddit.com/r/thewallstreet/comments/b2nb38/post_market_discussion_march_18/'}
DEBUG:root:Post added to MongoDB
2019-03-24 15:40:06 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/thewallstreet/new/>
"
https://old.reddit.com/r/thewallstreet/comments/b2hmah/daily_discussion_march_18/'," datetime.datetime(2019, 3, 24, 15, 40, 6, 425434)",'2019-03-24',u'Daily',u'12','thewallstreet',u'Daily Discussion - (March 18)'," u'https://old.reddit.com/r/thewallstreet/comments/b2hmah/daily_discussion_march_18/'}
DEBUG:root:Post added to MongoDB
2019-03-24 15:40:06 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/Trading/new/>
"
https://old.reddit.com/r/StockMarket/comments/b4deee/i_sent_money_to_etrade_on_accident/'," datetime.datetime(2019, 3, 24, 15, 40, 7, 206206)",'2019-03-24',None,u'0','StockMarket',u'I sent money to e*trade on accident.'," u'https://old.reddit.com/r/StockMarket/comments/b4deee/i_sent_money_to_etrade_on_accident/'}
DEBUG:root:Post added to MongoDB
2019-03-24 15:40:07 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/StockMarket/new/>
"
https://old.reddit.com/r/StockMarket/comments/b4cpxp/new_to_stock_investment_what_should_i_do_in_the/'," datetime.datetime(2019, 3, 24, 15, 40, 7, 216110)",'2019-03-24',None,u'0','StockMarket',u'New to stock investment. What should I do in the lobg term?'," u'https://old.reddit.com/r/StockMarket/comments/b4cpxp/new_to_stock_investment_what_should_i_do_in_the/'}
DEBUG:root:Post added to MongoDB
2019-03-24 15:40:07 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/Daytrading/new/>
"
https://old.reddit.com/r/StockMarket/comments/b4c0rv/innovative_industrial_properties_inc_iipr_schwab/'," datetime.datetime(2019, 3, 24, 15, 40, 7, 251851)",'2019-03-24',None,u'0','StockMarket',u'Innovative Industrial Properties Inc (IIPR) Schwab Equity Rating: F?!'," u'https://old.reddit.com/r/StockMarket/comments/b4c0rv/innovative_industrial_properties_inc_iipr_schwab/'}
DEBUG:root:Post added to MongoDB
2019-03-24 15:40:07 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/StockMarket/new/>
"
https://old.reddit.com/r/StockMarket/comments/b4b78c/tsx_growth_stocks_recommendations/'," datetime.datetime(2019, 3, 24, 15, 40, 7, 260166)",'2019-03-24',None,u'1','StockMarket',u'TSX growth stocks recommendations'," u'https://old.reddit.com/r/StockMarket/comments/b4b78c/tsx_growth_stocks_recommendations/'}
DEBUG:root:Post added to MongoDB
2019-03-24 15:40:07 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/Daytrading/new/>
"
https://old.reddit.com/r/StockMarket/comments/b4a52r/rate_my_portfolio/'," datetime.datetime(2019, 3, 24, 15, 40, 7, 293364)",'2019-03-24',None,u'3','StockMarket',u'Rate my portfolio'," u'https://old.reddit.com/r/StockMarket/comments/b4a52r/rate_my_portfolio/'}
DEBUG:root:Post added to MongoDB
2019-03-24 15:40:07 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/StockMarket/new/>
"
https://old.reddit.com/r/StockMarket/comments/b497u8/a_question_for_all_the_swing_traders_out_there/'," datetime.datetime(2019, 3, 24, 15, 40, 7, 301899)",'2019-03-24',None,u'1','StockMarket',u'A question for all the swing traders out there!'," u'https://old.reddit.com/r/StockMarket/comments/b497u8/a_question_for_all_the_swing_traders_out_there/'}
DEBUG:root:Post added to MongoDB
2019-03-24 15:40:07 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/Daytrading/new/>
"
https://old.reddit.com/r/StockMarket/comments/b491jm/why_did_the_market_have_a_big_fall_today/'," datetime.datetime(2019, 3, 24, 15, 40, 7, 321217)",'2019-03-24',None,u'156','StockMarket',u'Why did the market have a big fall today?'," u'https://old.reddit.com/r/StockMarket/comments/b491jm/why_did_the_market_have_a_big_fall_today/'}
DEBUG:root:Post added to MongoDB
2019-03-24 15:40:07 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/Daytrading/new/>
"
https://old.reddit.com/r/Daytrading/comments/b3ftb5/looking_for_the_bare_basics_on_where_to_start_so/'," datetime.datetime(2019, 3, 24, 15, 40, 7, 471890)",'2019-03-24',None,u'16','Daytrading',u'Looking for the bare basics on where to start so I can teach myself'," u'https://old.reddit.com/r/Daytrading/comments/b3ftb5/looking_for_the_bare_basics_on_where_to_start_so/'}
DEBUG:root:Post added to MongoDB
2019-03-24 15:40:07 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/Daytrading/new/>
"
https://old.reddit.com/r/Daytrading/comments/b3ef2d/i_was_told_most_traders_lose_money/'," datetime.datetime(2019, 3, 24, 15, 40, 7, 486692)",'2019-03-24',None,u'0','Daytrading',u'I was told most traders lose money'," u'https://old.reddit.com/r/Daytrading/comments/b3ef2d/i_was_told_most_traders_lose_money/'}
INFO:scrapy.core.engine:Closing spider (finished)
2019-03-24 15:40:07 [scrapy.core.engine] INFO: Closing spider (finished)
INFO:scrapy.statscollectors:Dumping Scrapy stats:
"
https://old.reddit.com/r/wallstreetbets/comments/b4w894/ual_short_airlines_are_weak_right_now/'," datetime.datetime(2019, 3, 25, 9, 30, 3, 73823)",'2019-03-25',u'Technicals',u'20','wallstreetbets',u'UAL short. Airlines are weak right now.'," u'https://old.reddit.com/r/wallstreetbets/comments/b4w894/ual_short_airlines_are_weak_right_now/'}
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
2019-03-25 09:30:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
2019-03-25 09:30:03 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
DEBUG:root:Post added to MongoDB
2019-03-25 09:30:03 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/stocks/new/>
"
https://old.reddit.com/r/investing/comments/b564zu/what_happened_to_apple/'," datetime.datetime(2019, 3, 25, 9, 35, 2, 972744)",'2019-03-25',None,u'0','investing',u'What happened to Apple?'," u'https://old.reddit.com/r/investing/comments/b564zu/what_happened_to_apple/'}
DEBUG:root:Post added to MongoDB
2019-03-25 09:35:02 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/investing/new/>
"
https://old.reddit.com/r/StockMarket/comments/b4fdod/with_march_madness_taking_place_college/'," datetime.datetime(2019, 3, 25, 9, 55, 4, 922513)",'2019-03-25',None,u'0','StockMarket',"u'With March madness taking place (college basketball) , any stocks that benefit from this?'"," u'https://old.reddit.com/r/StockMarket/comments/b4fdod/with_march_madness_taking_place_college/'}
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/Daytrading/new/> (referer: None)
2019-03-25 09:55:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/Daytrading/new/> (referer: None)
DEBUG:root:Post added to MongoDB
2019-03-25 09:55:05 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/Daytrading/new/>
"
https://old.reddit.com/r/stocks/comments/b4vq88/help_getting_started/'," datetime.datetime(2019, 3, 25, 10, 0, 2, 391422)",'2019-03-25',None,u'0','stocks',u'Help getting started'," u'https://old.reddit.com/r/stocks/comments/b4vq88/help_getting_started/'}
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
2019-03-25 10:00:02 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
2019-03-25 10:00:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
DEBUG:root:Post added to MongoDB
2019-03-25 10:00:03 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/stockaday/new/>
"
https://old.reddit.com/r/Daytrading/comments/b3h3dp/acb_keg_pyx_nvda_swing_trade_recap_using_my/'," datetime.datetime(2019, 3, 25, 10, 0, 5, 727663)",'2019-03-25',None,u'0','Daytrading',u'$ACB $KEG $PYX $NVDA Swing Trade recap using my favorite swing trade set up!'," u'https://www.youtube.com/watch?v=Jh8eq_q2DBc'}
INFO:scrapy.core.engine:Closing spider (finished)
2019-03-25 10:00:05 [scrapy.core.engine] INFO: Closing spider (finished)
INFO:scrapy.statscollectors:Dumping Scrapy stats:
"
https://old.reddit.com/r/investing/comments/b53cs7/how_does_everyone_feel_about_dave_ramsey/'," datetime.datetime(2019, 3, 25, 10, 20, 3, 192252)",'2019-03-25',None,u'0','investing',u'How does everyone feel about Dave Ramsey?'," u'https://old.reddit.com/r/investing/comments/b53cs7/how_does_everyone_feel_about_dave_ramsey/'}
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-03-25 10:20:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
DEBUG:root:Post added to MongoDB
2019-03-25 10:20:03 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/ValueInvesting/new/>
"
https://old.reddit.com/r/investing/comments/b53dw0/most_polite_way_to_ping_contact_about_an/'," datetime.datetime(2019, 3, 25, 10, 40, 9, 587760)",'2019-03-25',None,u'0','investing',u'Most polite way to ping contact about an introduction they promised?'," u'https://old.reddit.com/r/investing/comments/b53dw0/most_polite_way_to_ping_contact_about_an/'}
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-03-25 10:40:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
DEBUG:root:Post added to MongoDB
2019-03-25 10:40:09 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/ValueInvesting/new/>
"
https://old.reddit.com/r/investing/comments/b53nzt/risky_asset_allocations/'," datetime.datetime(2019, 3, 25, 10, 45, 4, 173097)",'2019-03-25',None,u'0','investing',u'Risky asset allocations'," u'https://old.reddit.com/r/investing/comments/b53nzt/risky_asset_allocations/'}
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-03-25 10:45:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
DEBUG:root:Post added to MongoDB
2019-03-25 10:45:04 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/ValueInvesting/new/>
"
https://old.reddit.com/r/investing/comments/b53so3/do_you_believe_that_fear_is_an_indication_of_buy/'," datetime.datetime(2019, 3, 25, 11, 5, 3, 805245)",'2019-03-25',None,u'0','investing',"u'Do you believe that fear is an indication of ""buy now"" in current economic conditions?'"," u'https://old.reddit.com/r/investing/comments/b53so3/do_you_believe_that_fear_is_an_indication_of_buy/'}
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Daytrading/new/> from <GET https://old.reddit.com/r/daytrading/new/>
2019-03-25 11:05:03 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Daytrading/new/> from <GET https://old.reddit.com/r/daytrading/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-03-25 11:05:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
DEBUG:root:Post added to MongoDB
2019-03-25 11:05:04 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/ValueInvesting/new/>
"
https://old.reddit.com/r/stocks/comments/b4xxre/best_under_100_stocks_to_get_into_right_now/'," datetime.datetime(2019, 3, 25, 11, 10, 1, 819542)",'2019-03-25',u'Question',u'0','stocks',u'Best under 100$ stocks to get into right now.'," u'https://old.reddit.com/r/stocks/comments/b4xxre/best_under_100_stocks_to_get_into_right_now/'}
DEBUG:root:Post added to MongoDB
2019-03-25 11:10:01 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/stocks/new/>
"
https://old.reddit.com/r/stocks/comments/b4wnkc/companies_all_across_america_are_warning_business/'," datetime.datetime(2019, 3, 25, 11, 10, 1, 821939)",'2019-03-25',None,u'184','stocks',u'Companies all across America are warning business is slowing down. Here are 6 you should pay close attention to.'," u'https://old.reddit.com/r/stocks/comments/b4wnkc/companies_all_across_america_are_warning_business/'}
DEBUG:root:Post added to MongoDB
2019-03-25 11:10:01 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/stocks/new/>
"
https://old.reddit.com/r/stocks/comments/b4w1p6/cash_share_importance/'," datetime.datetime(2019, 3, 25, 11, 10, 1, 825567)",'2019-03-25',None,u'32','stocks',"u'Cash / Share, importance ?'"," u'https://old.reddit.com/r/stocks/comments/b4w1p6/cash_share_importance/'}
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
2019-03-25 11:10:02 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
2019-03-25 11:10:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
DEBUG:root:Post added to MongoDB
2019-03-25 11:10:02 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/stockaday/new/>
"
https://old.reddit.com/r/investing/comments/b53yjt/is_there_a_set_schedule_when_the_sp_500_adds_or/'," datetime.datetime(2019, 3, 25, 11, 20, 3, 756768)",'2019-03-25',None,u'1','investing',u'Is there a set schedule when the S&P 500 adds or removes companies?'," u'https://old.reddit.com/r/investing/comments/b53yjt/is_there_a_set_schedule_when_the_sp_500_adds_or/'}
DEBUG:root:Post added to MongoDB
2019-03-25 11:20:03 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/investing/new/>
"
https://old.reddit.com/r/investing/comments/b53y6c/are_target_date_funds_worth_it/'," datetime.datetime(2019, 3, 25, 11, 20, 3, 759955)",'2019-03-25',None,u'0','investing',u'Are Target Date funds worth it?'," u'https://old.reddit.com/r/investing/comments/b53y6c/are_target_date_funds_worth_it/'}
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Daytrading/new/> from <GET https://old.reddit.com/r/daytrading/new/>
2019-03-25 11:20:03 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Daytrading/new/> from <GET https://old.reddit.com/r/daytrading/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-03-25 11:20:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
DEBUG:root:Post added to MongoDB
2019-03-25 11:20:04 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/ValueInvesting/new/>
"
https://old.reddit.com/r/wallstreetbets/comments/b4wlfy/what_an_inverted_yield_curve_means_to_the_market/'," datetime.datetime(2019, 3, 25, 11, 50, 1, 901949)",'2019-03-25',u'Fundamentals',u'94','wallstreetbets',"u""What an inverted yield curve means to the market- Rick Santelli's Best Work yet!"""," u'https://www.youtube.com/attribution_link?a=cEqybFEsnqE&u=%2Fwatch%3Fv%3DzVsgCYGn_cs%26feature%3Dshare'}
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
2019-03-25 11:50:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
2019-03-25 11:50:02 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
DEBUG:root:Post added to MongoDB
2019-03-25 11:50:02 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/stocks/new/>
"
https://old.reddit.com/r/investing/comments/b540ad/dividend_in_a_roth_ira/'," datetime.datetime(2019, 3, 25, 11, 55, 4, 95593)",'2019-03-25',None,u'0','investing',u'Dividend in a Roth IRA'," u'https://old.reddit.com/r/investing/comments/b540ad/dividend_in_a_roth_ira/'}
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-03-25 11:55:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
DEBUG:root:Post added to MongoDB
2019-03-25 11:55:04 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/ValueInvesting/new/>
"
https://old.reddit.com/r/stocks/comments/b4ybfm/invest_200_right_now_for_best_profit_in_1_month/'," datetime.datetime(2019, 3, 25, 12, 0, 2, 967731)",'2019-03-25',None,u'0','stocks',u'Invest $200 right now for best profit in 1 month'," u'https://old.reddit.com/r/stocks/comments/b4ybfm/invest_200_right_now_for_best_profit_in_1_month/'}
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
2019-03-25 12:00:03 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
2019-03-25 12:00:03 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
2019-03-25 12:00:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
DEBUG:root:Post added to MongoDB
2019-03-25 12:00:03 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/stockaday/new/>
"
https://old.reddit.com/r/StockMarket/comments/b4jdc3/what_the_yield_curve_inversion_really_means/'," datetime.datetime(2019, 3, 25, 12, 15, 5, 964469)",'2019-03-25',None,u'272','StockMarket',u'What the Yield Curve Inversion Really Means'," u'https://old.reddit.com/r/StockMarket/comments/b4jdc3/what_the_yield_curve_inversion_really_means/'}
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/Daytrading/new/> (referer: None)
2019-03-25 12:15:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/Daytrading/new/> (referer: None)
DEBUG:root:Post added to MongoDB
2019-03-25 12:15:06 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/Daytrading/new/>
"
https://old.reddit.com/r/stocks/comments/b4yvw6/momentum_traders_watchlist_for_3252019/'," datetime.datetime(2019, 3, 25, 12, 25, 1, 847041)",'2019-03-25',None,u'0','stocks',u'Momentum Traders Watch-List for 3.25.2019'," u'https://old.reddit.com/r/stocks/comments/b4yvw6/momentum_traders_watchlist_for_3252019/'}
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
2019-03-25 12:25:01 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
2019-03-25 12:25:02 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
2019-03-25 12:25:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
DEBUG:root:Post added to MongoDB
2019-03-25 12:25:02 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/stockaday/new/>
"
https://old.reddit.com/r/wallstreetbets/comments/b4wq7i/pinterest_business_model_x_post_from_rinfographics/'," datetime.datetime(2019, 3, 25, 12, 30, 2, 732152)",'2019-03-25',u'Fundamentals',u'52','wallstreetbets',u'Pinterest Business Model x post from r/infographics'," u'https://old.reddit.com/r/wallstreetbets/comments/b4wq7i/pinterest_business_model_x_post_from_rinfographics/'}
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
2019-03-25 12:30:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
2019-03-25 12:30:02 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
DEBUG:root:Post added to MongoDB
2019-03-25 12:30:02 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/stocks/new/>
"
https://old.reddit.com/r/wallstreetbets/comments/b4wxsv/economic_calendar_week_of_march_24th/'," datetime.datetime(2019, 3, 25, 12, 40, 1, 895673)",'2019-03-25',u'Futures',u'21','wallstreetbets',u'Economic Calendar Week of March 24th'," u'https://old.reddit.com/r/wallstreetbets/comments/b4wxsv/economic_calendar_week_of_march_24th/'}
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
2019-03-25 12:40:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
2019-03-25 12:40:02 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
DEBUG:root:Post added to MongoDB
2019-03-25 12:40:02 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/stocks/new/>
"
https://old.reddit.com/r/investing/comments/b544p0/3x_etfs_for_grandma_ny_help_please/'," datetime.datetime(2019, 3, 25, 12, 40, 3, 347937)",'2019-03-25',None,u'137','investing',u'3x etfs for grandma? NY help please'," u'https://old.reddit.com/r/investing/comments/b544p0/3x_etfs_for_grandma_ny_help_please/'}
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Daytrading/new/> from <GET https://old.reddit.com/r/daytrading/new/>
2019-03-25 12:40:03 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Daytrading/new/> from <GET https://old.reddit.com/r/daytrading/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-03-25 12:40:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
DEBUG:root:Post added to MongoDB
2019-03-25 12:40:04 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/ValueInvesting/new/>
"
https://old.reddit.com/r/wallstreetbets/comments/b4y2fh/appl_event_talk/'," datetime.datetime(2019, 3, 25, 12, 45, 2, 555118)",'2019-03-25',u'Discussion',u'28','wallstreetbets',u'$Appl event talk'," u'https://old.reddit.com/r/wallstreetbets/comments/b4y2fh/appl_event_talk/'}
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
2019-03-25 12:45:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
2019-03-25 12:45:02 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
DEBUG:root:Post added to MongoDB
2019-03-25 12:45:02 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/stocks/new/>
"
https://old.reddit.com/r/stocks/comments/b4zjsg/honest_thoughts_on_aapl/'," datetime.datetime(2019, 3, 25, 12, 45, 2, 801649)",'2019-03-25',None,u'2','stocks',u'Honest thoughts on $AAPL?'," u'https://old.reddit.com/r/stocks/comments/b4zjsg/honest_thoughts_on_aapl/'}
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
2019-03-25 12:45:03 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
2019-03-25 12:45:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
DEBUG:root:Post added to MongoDB
2019-03-25 12:45:03 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/stockaday/new/>
"
https://old.reddit.com/r/investing/comments/b54fkd/just_a_quick_question_from_a_student_who_hasnt/'," datetime.datetime(2019, 3, 25, 12, 45, 4, 93615)",'2019-03-25',None,u'1','investing',"u""Just a quick question from a student who hasn't got a job in finance yet..."""," u'https://old.reddit.com/r/investing/comments/b54fkd/just_a_quick_question_from_a_student_who_hasnt/'}
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Daytrading/new/> from <GET https://old.reddit.com/r/daytrading/new/>
2019-03-25 12:45:04 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Daytrading/new/> from <GET https://old.reddit.com/r/daytrading/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-03-25 12:45:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
DEBUG:root:Post added to MongoDB
2019-03-25 12:45:04 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/ValueInvesting/new/>
"
https://old.reddit.com/r/StockMarket/comments/b4jqt8/lyft_risky_valuation_and_no_intellectual_property/'," datetime.datetime(2019, 3, 25, 12, 45, 6, 140664)",'2019-03-25',None,u'31','StockMarket',u'Lyft: Risky Valuation and No Intellectual Property'," u'https://old.reddit.com/r/StockMarket/comments/b4jqt8/lyft_risky_valuation_and_no_intellectual_property/'}
DEBUG:root:Post added to MongoDB
2019-03-25 12:45:06 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/Stock_Picks/new/>
"
https://old.reddit.com/r/investing/comments/b54s4s/does_vanguard_withhold_tax_for_mf_dividend/'," datetime.datetime(2019, 3, 25, 12, 50, 3, 144521)",'2019-03-25',None,u'0','investing',u'Does Vanguard withhold tax for MF dividend?'," u'https://old.reddit.com/r/investing/comments/b54s4s/does_vanguard_withhold_tax_for_mf_dividend/'}
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Daytrading/new/> from <GET https://old.reddit.com/r/daytrading/new/>
2019-03-25 12:50:03 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Daytrading/new/> from <GET https://old.reddit.com/r/daytrading/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-03-25 12:50:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
DEBUG:root:Post added to MongoDB
2019-03-25 12:50:03 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/ValueInvesting/new/>
"
https://old.reddit.com/r/investing/comments/b571hf/is_the_former_enron_ceo_mulling_a_crypto_stint/'," datetime.datetime(2019, 3, 25, 13, 10, 2, 998822)",'2019-03-25',None,u'3','investing',u'Is the Former Enron CEO Mulling a Crypto Stint? Probably Not'," u'https://old.reddit.com/r/investing/comments/b571hf/is_the_former_enron_ceo_mulling_a_crypto_stint/'}
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Daytrading/new/> from <GET https://old.reddit.com/r/daytrading/new/>
2019-03-25 13:10:03 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Daytrading/new/> from <GET https://old.reddit.com/r/daytrading/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-03-25 13:10:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
DEBUG:root:Post added to MongoDB
2019-03-25 13:10:03 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/ValueInvesting/new/>
"
https://old.reddit.com/r/StockMarket/comments/b4kvrw/divergence/'," datetime.datetime(2019, 3, 25, 13, 20, 5, 387680)",'2019-03-25',None,u'1','StockMarket',u'Divergence'," u'https://old.reddit.com/r/StockMarket/comments/b4kvrw/divergence/'}
DEBUG:root:Post added to MongoDB
2019-03-25 13:20:05 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/StockMarket/new/>
"
https://old.reddit.com/r/StockMarket/comments/b4jxge/resources_for_stock_market_forecasts/'," datetime.datetime(2019, 3, 25, 13, 20, 5, 390216)",'2019-03-25',None,u'4','StockMarket',u'Resources for stock market forecasts'," u'https://old.reddit.com/r/StockMarket/comments/b4jxge/resources_for_stock_market_forecasts/'}
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/Daytrading/new/> (referer: None)
2019-03-25 13:20:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/Daytrading/new/> (referer: None)
DEBUG:root:Post added to MongoDB
2019-03-25 13:20:05 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/Daytrading/new/>
"
https://old.reddit.com/r/wallstreetbets/comments/b4y72h/can_we_just_take_a_quick_poll_of_which_way_you/'," datetime.datetime(2019, 3, 25, 13, 25, 2, 336747)",'2019-03-25',u'Discussion',u'18','wallstreetbets',u'Can we just take a quick poll of which way you guys think the market goes Monday? I\u2019d like to hear down from you guys so I know my calls are safe.'," u'https://old.reddit.com/r/wallstreetbets/comments/b4y72h/can_we_just_take_a_quick_poll_of_which_way_you/'}
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
2019-03-25 13:25:02 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
DEBUG:root:Post added to MongoDB
2019-03-25 13:25:02 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/stocks/new/>
"
https://old.reddit.com/r/stocks/comments/b505uq/unless_lululemon_is_able_to_establish_itself_as_a/'," datetime.datetime(2019, 3, 25, 13, 35, 2, 360719)",'2019-03-25',None,u'34','stocks',"u'Unless Lululemon is able to establish itself as a modern day Nike with the modern day equivalent of Michael Jordan, it\u2019s hard to see how its products cannot be replicated and its market position weakened.'"," u'https://old.reddit.com/r/stocks/comments/b505uq/unless_lululemon_is_able_to_establish_itself_as_a/'}
DEBUG:root:Post added to MongoDB
2019-03-25 13:35:02 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/stocks/new/>
"
https://old.reddit.com/r/stocks/comments/b500sk/would_it_not_be_better_for_apple_to_focus_on/'," datetime.datetime(2019, 3, 25, 13, 35, 2, 364439)",'2019-03-25',None,u'2','stocks',"u'Would it not be better for Apple to focus on hardware design and its \u201cWearables, Home, and Accessories\u201d department? Surely this is where Apple has the ability to add real value.'"," u'https://old.reddit.com/r/stocks/comments/b500sk/would_it_not_be_better_for_apple_to_focus_on/'}
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
2019-03-25 13:35:02 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
2019-03-25 13:35:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
DEBUG:root:Post added to MongoDB
2019-03-25 13:35:03 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/stockaday/new/>
"
https://old.reddit.com/r/stocks/comments/b50l39/discord/'," datetime.datetime(2019, 3, 25, 13, 40, 9, 894022)",'2019-03-25',u'Question',u'2','stocks',u'Discord?'," u'https://old.reddit.com/r/stocks/comments/b50l39/discord/'}
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
2019-03-25 13:40:09 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
2019-03-25 13:40:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
DEBUG:root:Post added to MongoDB
2019-03-25 13:40:10 [root] DEBUG: Post added to MongoDB
DEBUG:scrapy.core.scraper:Scraped from <200 https://old.reddit.com/r/stockaday/new/>
"
https://old.reddit.com/r/Daytrading/comments/b5b2fj/das_trader_trial_reset/'," datetime.datetime(2019, 3, 25, 14, 40, 43, 579734)",'2019-03-25',None,u'1','Daytrading',u'Das Trader trial reset'," u'https://old.reddit.com/r/Daytrading/comments/b5b2fj/das_trader_trial_reset/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b52iw5/will_the_end_of_the_mueller_investigation_result/'," datetime.datetime(2019, 3, 25, 15, 11, 36, 708237)",'2019-03-25',None,u'8','stocks',u'Will the end of the Mueller investigation result in a market rally Monday / this week?'," u'https://www.reddit.com/r/investing/comments/b52dri/will_the_end_of_the_mueller_investigation_result/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b51kmd/thoughts_on_bumpedcom/'," datetime.datetime(2019, 3, 25, 15, 11, 43, 697242)",'2019-03-25',None,u'0','stocks',u'Thoughts on Bumped.com?'," u'https://old.reddit.com/r/stocks/comments/b51kmd/thoughts_on_bumpedcom/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b4ynln/stock_buybacks_blackout_period_gaining_traction/'," datetime.datetime(2019, 3, 25, 15, 23, 17, 368925)",'2019-03-25',u'Fundamentals',u'61','wallstreetbets',u'Stock buybacks blackout period gaining traction from the beggining of next week till the end of April. October v2 incoming?'," u'https://old.reddit.com/r/wallstreetbets/comments/b4ynln/stock_buybacks_blackout_period_gaining_traction/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-25 15:23:47 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b53ju0/american_airlines_is_cancelling_90_flights_a_day/'," datetime.datetime(2019, 3, 25, 15, 42, 37, 662991)",'2019-03-25',None,u'454','stocks',u'American Airlines is cancelling 90 flights a day as Boeing 737 Max remains grounded'," u'https://old.reddit.com/r/stocks/comments/b53ju0/american_airlines_is_cancelling_90_flights_a_day/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b52u9m/stock_and_share_app_uk/'," datetime.datetime(2019, 3, 25, 15, 47, 10, 817276)",'2019-03-25',None,u'0','stocks',u'Stock and share app Uk'," u'https://old.reddit.com/r/stocks/comments/b52u9m/stock_and_share_app_uk/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b5djae/my_stock_tracker/'," datetime.datetime(2019, 3, 25, 15, 48, 21, 243552)",'2019-03-25',None,u'5','StockMarket',u'My Stock Tracker'," u'https://old.reddit.com/r/StockMarket/comments/b5djae/my_stock_tracker/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b4l27x/wall_street_week_ahead_for_the_trading_week/'," datetime.datetime(2019, 3, 25, 15, 54, 3, 932524)",'2019-03-25',u'News',u'13','StockMarket',"u'Wall Street Week Ahead for the trading week beginning March 25th, 2019'"," u'https://old.reddit.com/r/StockMarket/comments/b4l27x/wall_street_week_ahead_for_the_trading_week/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b55qsn/international_stocks_will_not_save_you/'," datetime.datetime(2019, 3, 25, 15, 55, 38, 358370)",'2019-03-25',None,u'0','investing',u'International Stocks Will Not Save You.'," u'https://old.reddit.com/r/investing/comments/b55qsn/international_stocks_will_not_save_you/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-25 15:56:08 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b4kz6l/so_a_couple_of_weeks_ago_i_posted_a_survey_here/'," datetime.datetime(2019, 3, 25, 15, 57, 5, 802828)",'2019-03-25',None,u'3','StockMarket',"u'So a couple of weeks ago, I posted a survey here detailing, how I was 14 and if you could help me with my project by filling out this survey. I have now finished a first draft of the project so: https://docs.google.com/document/d/1KJgxuw_ef1n1vOivEJyE92MDNIuugGjD2CuiJnTWphE/edit?usp=sharing'"," u'https://old.reddit.com/r/StockMarket/comments/b4kz6l/so_a_couple_of_weeks_ago_i_posted_a_survey_here/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b4zlkt/gamestop_investment_thesis/'," datetime.datetime(2019, 3, 25, 15, 57, 41, 571283)",'2019-03-25',u'Fundamentals',u'34','wallstreetbets',u'GameStop Investment Thesis'," u'https://old.reddit.com/r/wallstreetbets/comments/b4zlkt/gamestop_investment_thesis/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b5ah40/cocacola_executives_signed_with_cannabis_company/'," datetime.datetime(2019, 3, 25, 16, 8, 34, 231561)",'2019-03-25',None,u'76','StockMarket',"u'Coca-Cola Executives Signed with Cannabis Company - Aurora, Aphria, and Canopy likely Candidates (Report)'"," u'https://old.reddit.com/r/StockMarket/comments/b5ah40/cocacola_executives_signed_with_cannabis_company/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b54gj2/reverse_etfs/'," datetime.datetime(2019, 3, 25, 16, 12, 36, 835399)",'2019-03-25',None,u'1','stocks',u'Reverse ETFS??'," u'https://old.reddit.com/r/stocks/comments/b54gj2/reverse_etfs/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b53t82/what_are_your_top_5_methods_to_invest_your_money/'," datetime.datetime(2019, 3, 25, 16, 17, 39, 950904)",'2019-03-25',None,u'2','stocks',u'What Are Your Top 5 Methods To Invest Your Money?'," u'https://old.reddit.com/r/stocks/comments/b53t82/what_are_your_top_5_methods_to_invest_your_money/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b4znoa/midwestern_flood_plays/'," datetime.datetime(2019, 3, 25, 16, 18, 10, 288836)",'2019-03-25',u'Discussion',u'12','wallstreetbets',u'Midwestern Flood Plays?'," u'https://old.reddit.com/r/wallstreetbets/comments/b4znoa/midwestern_flood_plays/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-25 16:18:40 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b57et2/bond_etf_alternative_for_europe/'," datetime.datetime(2019, 3, 25, 16, 19, 31, 218373)",'2019-03-25',None,u'0','investing',u'Bond ETF alternative for europe'," u'https://old.reddit.com/r/investing/comments/b57et2/bond_etf_alternative_for_europe/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-25 16:20:01 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b4z0ch/my_lyft_dd/'," datetime.datetime(2019, 3, 25, 16, 22, 43, 57304)",'2019-03-25',u'DD',u'117','wallstreetbets',u'My Lyft DD'," u'https://old.reddit.com/r/wallstreetbets/comments/b4z0ch/my_lyft_dd/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-25 16:23:13 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b58t3h/golden_visa_investment_scheme_low_risk_or_a_good/'," datetime.datetime(2019, 3, 25, 16, 25, 59, 606400)",'2019-03-25',None,u'0','investing',u'Golden Visa Investment scheme. Low risk or a good opportunity?'," u'https://old.reddit.com/r/investing/comments/b58t3h/golden_visa_investment_scheme_low_risk_or_a_good/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b57qka/flashing_amber_stocks_tumble_as_bond_markets/'," datetime.datetime(2019, 3, 25, 16, 30, 2, 245556)",'2019-03-25',u'News',u'0','investing',"u""'Flashing amber': stocks tumble as bond markets sound US recession warning"""," u'https://old.reddit.com/r/investing/comments/b57qka/flashing_amber_stocks_tumble_as_bond_markets/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b59inw/weekly_forex_market_preview/'," datetime.datetime(2019, 3, 25, 16, 34, 50, 999821)",'2019-03-25',None,u'0','investing',u'Weekly Forex Market Preview'," u'https://old.reddit.com/r/investing/comments/b59inw/weekly_forex_market_preview/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b58y33/what_caused_stock_prices_to_be_so_low_in_december/'," datetime.datetime(2019, 3, 25, 16, 35, 29, 213362)",'2019-03-25',None,u'22','investing',u'What caused stock prices to be so low in december'," u'https://old.reddit.com/r/investing/comments/b58y33/what_caused_stock_prices_to_be_so_low_in_december/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-25 16:35:59 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b59fb4/dow_jones_industrial_average_futures_rise/'," datetime.datetime(2019, 3, 25, 16, 38, 53, 491029)",'2019-03-25',None,u'6','investing',u'Dow Jones Industrial Average futures rise... positive news?'," u'https://old.reddit.com/r/investing/comments/b59fb4/dow_jones_industrial_average_futures_rise/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b59ba0/what_is_the_motley_fool_one_stock_for_the_coming/'," datetime.datetime(2019, 3, 25, 16, 42, 55, 961971)",'2019-03-25',None,u'0','investing',"u""What is the Motley Fool ''One Stock for the Coming Maruijuana Boom''?"""," u'https://old.reddit.com/r/investing/comments/b59ba0/what_is_the_motley_fool_one_stock_for_the_coming/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b58zau/freetrade_how_safe_is_my_money/'," datetime.datetime(2019, 3, 25, 16, 46, 58, 472258)",'2019-03-25',None,u'0','investing',u'Freetrade - how safe is my money?'," u'https://old.reddit.com/r/investing/comments/b58zau/freetrade_how_safe_is_my_money/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Daytrading/comments/b3i20f/a_beginners_guide_to_robinhood_investing/'," datetime.datetime(2019, 3, 25, 16, 59, 44, 573822)",'2019-03-25',None,u'0','Daytrading',u'A Beginners Guide To Robinhood Investing'," u'https://youtu.be/fq4Qx7WVgbw'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-25 17:00:14 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
INFO:scrapy.core.engine:Closing spider (finished)
2019-03-25 17:00:14 [scrapy.core.engine] INFO: Closing spider (finished)
INFO:scrapy.statscollectors:Dumping Scrapy stats:
"
https://old.reddit.com/r/thewallstreet/comments/b2t7r1/daily_spx_tpos_03182019/'," datetime.datetime(2019, 3, 25, 17, 12, 45, 123022)",'2019-03-25',None,u'7','thewallstreet',"u""Daily SPX TPO's 03-18-2019"""," u'https://i.imgur.com/kyTkYSE.png'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b516ir/robinhood_screen_time_iphone/'," datetime.datetime(2019, 3, 25, 17, 48, 32, 195030)",'2019-03-25',u'Discussion',u'32','wallstreetbets',u'Robinhood screen time (iPhone)'," u'https://old.reddit.com/r/wallstreetbets/comments/b516ir/robinhood_screen_time_iphone/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b51ja7/what_are_your_moves_tomorrow_march_25/'," datetime.datetime(2019, 3, 25, 17, 53, 30, 995709)",'2019-03-25',u'Daily Discussion',u'73','wallstreetbets',"u'What Are Your Moves Tomorrow, March 25'"," u'https://old.reddit.com/r/wallstreetbets/comments/b51ja7/what_are_your_moves_tomorrow_march_25/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b50ozt/cant_look_at_it_after_you_buy_it_cause_its_so/'," datetime.datetime(2019, 3, 25, 17, 53, 35, 480187)",'2019-03-25',u'Shitpost',u'106','wallstreetbets',"u'""Can\'t Look At It After You Buy It Cause It\'s So Horrible"" - Jim Cramer & r/wallstreetbets'"," u'https://old.reddit.com/r/wallstreetbets/comments/b50ozt/cant_look_at_it_after_you_buy_it_cause_its_so/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b4zqwp/just_a_helpful_video_i_found_explaining_the_yield/'," datetime.datetime(2019, 3, 25, 17, 53, 39, 415425)",'2019-03-25',u'Technicals',u'35','wallstreetbets',u'Just a helpful video I found explaining the yield curve'," u'https://old.reddit.com/r/wallstreetbets/comments/b4zqwp/just_a_helpful_video_i_found_explaining_the_yield/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b502u7/is_there_any_relationship_here_we_could_perhaps/'," datetime.datetime(2019, 3, 25, 17, 58, 38, 668728)",'2019-03-25',u'Discussion',u'65','wallstreetbets',u'Is there any relationship here we could perhaps monetize?'," u'https://old.reddit.com/r/wallstreetbets/comments/b502u7/is_there_any_relationship_here_we_could_perhaps/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b54amg/theranos_documentary/'," datetime.datetime(2019, 3, 25, 18, 2, 30, 183975)",'2019-03-25',u'Storytime',u'165','wallstreetbets',u'Theranos documentary.'," u'https://www.youtube.com/watch?v=wtDaP18OGfw'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b5f87o/ubs_chief_investment_office_time_to_lock_in_some/'," datetime.datetime(2019, 3, 25, 18, 2, 37, 277580)",'2019-03-25',None,u'6','investing',u'UBS Chief Investment Office: time to lock in some profits'," u'https://old.reddit.com/r/investing/comments/b5f87o/ubs_chief_investment_office_time_to_lock_in_some/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b5f57f/is_wsj_worth_it/'," datetime.datetime(2019, 3, 25, 18, 4, 38, 597614)",'2019-03-25',None,u'8','investing',u'Is WSJ worth it?'," u'https://old.reddit.com/r/investing/comments/b5f57f/is_wsj_worth_it/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b5faa3/how_would_steve_jobs_react_if_he_found_out_that_8/'," datetime.datetime(2019, 3, 25, 18, 5, 2, 88244)",'2019-03-25',u'Discussion',u'149','wallstreetbets',u'How would Steve Jobs react if he found out that 8 years after his death Apple product annoucement events will be used to launch a credit card?'," u'https://old.reddit.com/r/wallstreetbets/comments/b5faa3/how_would_steve_jobs_react_if_he_found_out_that_8/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
2019-03-25 18:05:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b5erdr/paypal_vs_square/'," datetime.datetime(2019, 3, 25, 18, 5, 32, 501982)",'2019-03-25',u'Discussion',u'8','wallstreetbets',u'Paypal vs Square'," u'https://old.reddit.com/r/wallstreetbets/comments/b5erdr/paypal_vs_square/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 2 pages (at 2 pages/min), scraped 0 items (at 0 items/min)
2019-03-25 18:06:02 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 2 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b5ed6x/i_thought_i_was_buying_wed_spy_calls_but_they/'," datetime.datetime(2019, 3, 25, 18, 6, 2, 828944)",'2019-03-25',u'Loss',u'85','wallstreetbets',u'I thought I was buying WED spy calls but they were for today. I kept doubling down. I belong here'," u'https://old.reddit.com/r/wallstreetbets/comments/b5ed6x/i_thought_i_was_buying_wed_spy_calls_but_they/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
2019-03-25 18:06:33 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b5f2v6/mu_sentiment_is_trending_down/'," datetime.datetime(2019, 3, 25, 18, 7, 10, 373448)",'2019-03-25',None,u'2','investing',u'$MU sentiment is trending down'," u'https://old.reddit.com/r/investing/comments/b5f2v6/mu_sentiment_is_trending_down/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/pytERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b5dlgo/apples_cook_says_apple_pay_will_hit_10b/'," datetime.datetime(2019, 3, 25, 18, 7, 3, 739648)",'2019-03-25',u'Fundamentals',u'25','wallstreetbets',"u""Apple's Cook Says Apple Pay Will Hit 10B Transactions In 2019"""," u'https://old.reddit.com/r/wallstreetbets/comments/b5dlgo/apples_cook_says_apple_pay_will_hit_10b/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
2019-03-25 21:07:25 [scrapy.core.scraper] ERROR: Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b5ddlc/federal_prosecutors_announce_charges_against/'," datetime.datetime(2019, 3, 26, 9, 23, 17, 731450)",'2019-03-26',u'Shitpost',u'579','wallstreetbets',"u'Federal prosecutors announce charges against Michael Avenatti, alleging he tried to extort Nike, threatening to release damaging info about the company, and told Nike attorneys if his demands were not met, ""I\'ll go take ten billion dollars off your client\'s market cap...I\'m not fucking around.""'"," u'https://www.cnbc.com/2019/03/25/michael-avenatti-to-be-charged-with-wire-and-bank-fraud.html'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b5d5ig/robinhood_made_its_first_acquisition_ever_and_its/'," datetime.datetime(2019, 3, 26, 9, 24, 18, 466139)",'2019-03-26',u'Discussion',u'92','wallstreetbets',u'Robinhood Made Its First Acquisition Ever \u2014 And It\u2019s a Financial Newsletter'," u'http://fortune.com/2019/03/25/robinhood-acquires-marketsnacks/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b5ev4g/low_interest_rates_might_be_whats_hurting_growth/'," datetime.datetime(2019, 3, 26, 9, 25, 18, 961174)",'2019-03-26',u'Discussion',u'8','investing',u'Low Interest Rates Might Be What\u2019s Hurting Growth: Cheap credit does a poor job of weeding out zombie companies that compete for scarce resources.'," u'https://old.reddit.com/r/investing/comments/b5ev4g/low_interest_rates_might_be_whats_hurting_growth/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b5d2bv/becky_might_be_attracting_attention/'," datetime.datetime(2019, 3, 26, 9, 25, 19, 108383)",'2019-03-26',u'Shitpost',u'39','wallstreetbets',u'$BECKY Might Be Attracting Attention'," u'https://old.reddit.com/r/wallstreetbets/comments/b5d2bv/becky_might_be_attracting_attention/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 2 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-26 09:25:49 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b5e5l4/atrs/'," datetime.datetime(2019, 3, 26, 9, 26, 50, 44287)",'2019-03-26',None,u'6','stocks',u'ATRS'," u'https://old.reddit.com/r/stocks/comments/b5e5l4/atrs/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/si2019-03-26 09:27:20 [scrapy.core.scraper] ERROR: Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b5aq1b/yield_curve_inversion_links/'," datetime.datetime(2019, 3, 26, 9, 27, 20, 320871)",'2019-03-26',u'Discussion',u'89','wallstreetbets',u'Yield curve inversion links'," u'https://old.reddit.com/r/wallstreetbets/comments/b5aq1b/yield_curve_inversion_links/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
2019-03-26 09:27:50 [scrapy.core.scraper] ERROR: Error processing "
https://old.reddit.com/r/stocks/comments/b5dt80/intel_innovation_requires_originality/'," datetime.datetime(2019, 3, 26, 9, 27, 50, 623443)",'2019-03-26',None,u'8','stocks',u'Intel: Innovation Requires Originality'," u'https://old.reddit.com/r/stocks/comments/b5dt80/intel_innovation_requires_originality/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
2019-03-26 09:28:20 [scrapy.core.scraper] ERROR: Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b5a4ue/reuters_tencent_shareholder_naspers_plots/'," datetime.datetime(2019, 3, 26, 9, 28, 20, 930706)",'2019-03-26',u'Discussion',u'9','wallstreetbets',"u'Reuters: ""Tencent shareholder Naspers plots Euronext e-commerce IPO.""'"," u'https://old.reddit.com/r/wallstreetbets/comments/b5a4ue/reuters_tencent_shareholder_naspers_plots/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
2019-03-26 09:28:51 [scrapy.core.scraper] ERROR: Error processing "
https://old.reddit.com/r/investing/comments/b5euyn/apple_announces_new_credit_card_and_subscription/'," datetime.datetime(2019, 3, 26, 9, 28, 51, 146623)",'2019-03-26',None,u'28','investing',u'Apple Announces New Credit Card and Subscription Services'," u'https://old.reddit.com/r/investing/comments/b5euyn/apple_announces_new_credit_card_and_subscription/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
DEBUG:scrapy.downloadermiddlewares.retry:Retrying <GET https://old.reddit.com/r/StockMarket/new/> (failed 1 times): User timeout caused connection failure: Getting https://old.reddit.com/r/StockMarket/new/ took longer than 180.0 seconds..
2019-03-26 09:29:21 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://old.reddit.com/r/StockMarket/new/> (failed 1 times): User timeout caused connection failure: Getting https://old.reddit.com/r/StockMarket/new/ took longer than 180.0 seconds..
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b5dqbd/apples_spring_event_is_happening_now_announces/'," datetime.datetime(2019, 3, 26, 9, 28, 51, 215480)",'2019-03-26',None,u'8','stocks',u'Apple\u2019s spring event is happening now \u2014 announces Apple News+ subscription service'," u'https://old.reddit.com/r/stocks/comments/b5dqbd/apples_spring_event_is_happening_now_announces/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
  _session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
  check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
DEBUG:scrapy.downloadermiddlewares.retry:Retrying <GET https://old.reddit.com/r/Stock_Picks/new/> (failed 1 times): User timeout caused connection failure: Getting https://old.reddit.com/r/Stock_Picks/new/ took longer than 180.0 seconds..
2019-03-26 09:29:21 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://old.reddit.com/r/Stock_Picks/new/> (failed 1 times): User timeout caused connection failure: Getting https://old.reddit.com/r/Stock_Picks/new/ took longer than 180.0 seconds..
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b59vcm/what_the_yield_curve_means_from_an_economists/'," datetime.datetime(2019, 3, 26, 9, 29, 21, 511489)",'2019-03-26',u'Futures',u'13','wallstreetbets',"u""What the yield curve means from an economist's perspective"""," u'https://old.reddit.com/r/wallstreetbets/comments/b59vcm/what_the_yield_curve_means_from_an_economists/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
2019-03-26 09:29:51 [scrapy.core.scraper] ERROR: Error processing "
https://old.reddit.com/r/stocks/comments/b5dq70/naspers_spinning_off_tencentother_holdings_into/'," datetime.datetime(2019, 3, 26, 9, 29, 51, 770026)",'2019-03-26',None,u'1','stocks',"u""Nasper's spinning off Tencent+other holdings into Euronext listing."""," u'https://old.reddit.com/r/stocks/comments/b5dq70/naspers_spinning_off_tencentother_holdings_into/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b59vb3/baml_positive_on_apple_aapl_iphone_order_trends/'," datetime.datetime(2019, 3, 26, 9, 30, 22, 73878)",'2019-03-26',u'DD',u'14','wallstreetbets',u'BAML positive on Apple (AAPL) iPhone order trends'," u'https://old.reddit.com/r/wallstreetbets/comments/b59vb3/baml_positive_on_apple_aapl_iphone_order_trends/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
DEBUG:scrapy.downloadermiddlewares.retry:Retrying <GET https://old.reddit.com/r/stockmarket/new/> (failed 2 times): User timeout caused connection failure: Getting https://old.reddit.com/r/stockmarket/new/ took longer than 180.0 seconds..
2019-03-26 09:30:52 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://old.reddit.com/r/stockmarket/new/> (failed 2 times): User timeout caused connection failure: Getting https://old.reddit.com/r/stockmarket/new/ took longer than 180.0 seconds..
INFO:scrapy.extensions.logstats:Crawled 2 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-26 09:30:52 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
DEBUG:scrapy.downloadermiddlewares.retry:Retrying <GET https://old.reddit.com/r/Trading/new/> (failed 1 times): User timeout caused connection failure: Getting https://old.reddit.com/r/Trading/new/ took longer than 180.0 seconds..
2019-03-26 09:30:52 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://old.reddit.com/r/Trading/new/> (failed 1 times): User timeout caused connection failure: Getting https://old.reddit.com/r/Trading/new/ took longer than 180.0 seconds..
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b5d1p5/cannabis/'," datetime.datetime(2019, 3, 26, 9, 30, 52, 326196)",'2019-03-26',u'Discussion',u'0','stocks',u'Cannabis'," u'https://old.reddit.com/r/stocks/comments/b5d1p5/cannabis/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b59g3j/uber_to_buy_careem_for_31_billion_this_week/'," datetime.datetime(2019, 3, 26, 9, 31, 22, 595335)",'2019-03-26',u'Discussion',u'46','wallstreetbets',u'Uber to buy Careem for $3.1 billion this week'," u'https://techleak.video.blog/2019/03/25/uber-to-buy-careem-for-3-1-billion-this-week/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 2 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-26 09:31:52 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
DEBUG:scrapy.downloadermiddlewares.retry:Retrying <GET https://old.reddit.com/r/Stock_Picks/new/> (failed 1 times): User timeout caused connection failure: Getting https://old.reddit.com/r/Stock_Picks/new/ took longer than 180.0 seconds..
2019-03-26 09:31:52 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://old.reddit.com/r/Stock_Picks/new/> (failed 1 times): User timeout caused connection failure: Getting https://old.reddit.com/r/Stock_Picks/new/ took longer than 180.0 seconds..
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b57ha8/us30usd/'," datetime.datetime(2019, 3, 26, 9, 31, 52, 315931)",'2019-03-26',None,u'0','stocks',u'US30USD'," u'https://old.reddit.com/r/stocks/comments/b57ha8/us30usd/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b5ctnm/best_stocks_to_short_for_the_long_term/'," datetime.datetime(2019, 3, 26, 9, 31, 52, 896877)",'2019-03-26',u'Discussion',u'0','stocks',u'Best stocks to short for the long term'," u'https://old.reddit.com/r/stocks/comments/b5ctnm/best_stocks_to_short_for_the_long_term/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b5eqqh/snap_android_update_was_the_er_a_lie/'," datetime.datetime(2019, 3, 26, 9, 32, 53, 257445)",'2019-03-26',u'Discussion',u'2','investing',u'$SNAP Android Update - Was the ER a lie?'," u'https://old.reddit.com/r/investing/comments/b5eqqh/snap_android_update_was_the_er_a_lie/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b56pxt/charts_5_bullish_trades_for_anyone_feeling_uneasy/'," datetime.datetime(2019, 3, 26, 9, 33, 23, 779218)",'2019-03-26',u'Technicals',u'13','wallstreetbets',u'(Charts) 5 bullish trades for anyone feeling uneasy about the next quarter.'," u'https://old.reddit.com/r/wallstreetbets/comments/b56pxt/charts_5_bullish_trades_for_anyone_feeling_uneasy/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b59efh/aapl_apple_special_event_announcement_megathread/'," datetime.datetime(2019, 3, 26, 9, 33, 53, 858389)",'2019-03-26',u'Discussion',u'82','wallstreetbets',"u'$AAPL Apple Special Event Announcement Megathread - March 25, 2019'"," u'https://old.reddit.com/r/wallstreetbets/comments/b59efh/aapl_apple_special_event_announcement_megathread/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b5cgdd/question_about_the_stocks_market_are_them/'," datetime.datetime(2019, 3, 26, 9, 33, 54, 27844)",'2019-03-26',u'Question',u'0','stocks',"u'Question about the stocks market, are them worldwide?'"," u'https://old.reddit.com/r/stocks/comments/b5cgdd/question_about_the_stocks_market_are_them/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
DEBUG:scrapy.downloadermiddlewares.retry:Retrying <GET https://old.reddit.com/r/investing/new/> (failed 2 times): User timeout caused connection failure: Getting https://old.reddit.com/r/investing/new/ took longer than 180.0 seconds..
2019-03-26 09:34:24 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://old.reddit.com/r/investing/new/> (failed 2 times): User timeout caused connection failure: Getting https://old.reddit.com/r/investing/new/ took longer than 180.0 seconds..
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b5of5r/my_analyst_predicts_a_green_day_pretty_reliable/'," datetime.datetime(2019, 3, 26, 9, 35, 1, 943538)",'2019-03-26',None,u'6','wallstreetbets',"u'My analyst predicts a Green Day, pretty reliable source'"," u'https://old.reddit.com/r/wallstreetbets/comments/b5of5r/my_analyst_predicts_a_green_day_pretty_reliable/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
2019-03-26 09:35:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b5bvw3/airlines_warn_of_cancellations_as_boeing_readies/'," datetime.datetime(2019, 3, 26, 9, 35, 25, 28649)",'2019-03-26',None,u'149','stocks',u'Airlines warn of cancellations as Boeing readies 737 Max software fix'," u'https://old.reddit.com/r/stocks/comments/b5bvw3/airlines_warn_of_cancellations_as_boeing_readies/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b54mik/us30usd_chart_on_tradingview/'," datetime.datetime(2019, 3, 26, 9, 36, 25, 177628)",'2019-03-26',None,u'2','stocks',u'US30USD chart on Tradingview'," u'https://old.reddit.com/r/stocks/comments/b54mik/us30usd_chart_on_tradingview/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b5elgs/sq_or_pypl/'," datetime.datetime(2019, 3, 26, 9, 36, 55, 687352)",'2019-03-26',u'Discussion',u'2','investing',u'$SQ or $PYPL?'," u'https://old.reddit.com/r/investing/comments/b5elgs/sq_or_pypl/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b5bvrs/buying_stocks_should_i_buy_in_my_local_stock/'," datetime.datetime(2019, 3, 26, 9, 37, 25, 989091)",'2019-03-26',None,u'0','stocks',u'Buying stocks. Should i buy in my local Stock Exchange or NYSE/NASDAQ?'," u'https://old.reddit.com/r/stocks/comments/b5bvrs/buying_stocks_should_i_buy_in_my_local_stock/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b59eal/yield_curve_inversion_and_term_premium/'," datetime.datetime(2019, 3, 26, 9, 37, 56, 305927)",'2019-03-26',u'Discussion',u'48','wallstreetbets',u'Yield curve inversion and term premium'," u'https://old.reddit.com/r/wallstreetbets/comments/b59eal/yield_curve_inversion_and_term_premium/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b5d7c7/pimco_on_their_active_etf_vs_aggbnd_etc_how_much/'," datetime.datetime(2019, 3, 26, 9, 40, 58, 444749)",'2019-03-26',None,u'2','investing',u'Pimco on their active ETF vs AGG/BND etc. How much of it are true?'," u'https://old.reddit.com/r/investing/comments/b5d7c7/pimco_on_their_active_etf_vs_aggbnd_etc_how_much/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b5bubt/cities_and_states_around_the_country_are_banning/'," datetime.datetime(2019, 3, 26, 9, 41, 28, 848008)",'2019-03-26',None,u'17','stocks',"u""Cities and states around the country are banning stores from refusing to accept cash, and it's a troubling trend for Amazon"""," u'https://old.reddit.com/r/stocks/comments/b5bubt/cities_and_states_around_the_country_are_banning/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b599yo/daily_discussion_thread_march_25_2019/'," datetime.datetime(2019, 3, 26, 9, 41, 59, 91575)",'2019-03-26',u'Daily Discussion',u'60','wallstreetbets',"u'Daily Discussion Thread - March 25, 2019'"," u'https://old.reddit.com/r/wallstreetbets/comments/b599yo/daily_discussion_thread_march_25_2019/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b5d3cg/best_market_update_website_for_newbies/'," datetime.datetime(2019, 3, 26, 9, 45, 0, 806167)",'2019-03-26',None,u'1','investing',u'Best Market Update Website for Newbies?'," u'https://old.reddit.com/r/investing/comments/b5d3cg/best_market_update_website_for_newbies/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 8 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-26 09:45:31 [scrapy.core.scraper] ERROR: Error processing "
https://old.reddit.com/r/stocks/comments/b5b1hj/tesla_swing_trade/'," datetime.datetime(2019, 3, 26, 9, 45, 31, 137925)",'2019-03-26',u'Question',u'11','stocks',u'Tesla Swing Trade?'," u'https://old.reddit.com/r/stocks/comments/b5b1hj/tesla_swing_trade/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b5d1n9/is_it_a_good_time_to_buy_gold/'," datetime.datetime(2019, 3, 26, 9, 48, 32, 885123)",'2019-03-26',None,u'2','investing',u'Is it a good time to buy gold?'," u'https://old.reddit.com/r/investing/comments/b5d1n9/is_it_a_good_time_to_buy_gold/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b5aiqc/shorting_uk_stocks_which_platform_do_you_use/'," datetime.datetime(2019, 3, 26, 9, 49, 3, 192663)",'2019-03-26',u'Advice Request',u'6','stocks',u'Shorting UK stocks - which platform do you use?'," u'https://old.reddit.com/r/stocks/comments/b5aiqc/shorting_uk_stocks_which_platform_do_you_use/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_conce2019-03-26 09:49:33 [scrapy.core.scraper] ERROR: Error processing "
https://old.reddit.com/r/investing/comments/b5d0ax/etf_portfolio/'," datetime.datetime(2019, 3, 26, 9, 52, 5, 329121)",'2019-03-26',None,u'3','investing',u'ETF portfolio'," u'https://old.reddit.com/r/investing/comments/b5d0ax/etf_portfolio/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
2019-03-26 09:52:35 [scrapy.core.scraper] ERROR: Error processing "
https://old.reddit.com/r/investing/comments/b59is3/can_i_use_my_personal_performance_for_job/'," datetime.datetime(2019, 3, 26, 9, 52, 5, 709064)",'2019-03-26',None,u'2','investing',u'Can I use my personal performance for job interviews'," u'https://old.reddit.com/r/investing/comments/b59is3/can_i_use_my_personal_performance_for_job/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b5agcs/dariohealth_reports_fourth_quarter_and_year_end/'," datetime.datetime(2019, 3, 26, 9, 52, 35, 650583)",'2019-03-26',u'News',u'4','stocks',u'DarioHealth Reports Fourth Quarter and Year End 2018 Record Results'," u'https://old.reddit.com/r/stocks/comments/b5agcs/dariohealth_reports_fourth_quarter_and_year_end/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b5cjnt/during_a_recession_what_stocks_should_be_shorted/'," datetime.datetime(2019, 3, 26, 9, 55, 37, 502194)",'2019-03-26',None,u'0','investing',"u'During a recession, what stocks should be shorted?'"," u'https://old.reddit.com/r/investing/comments/b5cjnt/during_a_recession_what_stocks_should_be_shorted/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b5cexh/lazy_portfolio_returns_update/'," datetime.datetime(2019, 3, 26, 9, 59, 9, 826637)",'2019-03-26',None,u'1','investing',u'Lazy Portfolio Returns Update'," u'https://old.reddit.com/r/investing/comments/b5cexh/lazy_portfolio_returns_update/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b59w0i/rstocks_daily_discussion_monday_mar_25_2019/'," datetime.datetime(2019, 3, 26, 9, 59, 40, 135896)",'2019-03-26',None,u'5','stocks',"u'r/Stocks Daily Discussion Monday - Mar 25, 2019'"," u'https://old.reddit.com/r/stocks/comments/b59w0i/rstocks_daily_discussion_monday_mar_25_2019/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b5byom/if_a_stock_you_previously_bought_reaches_your/'," datetime.datetime(2019, 3, 26, 10, 2, 42, 177806)",'2019-03-26',None,u'5','investing',"u'If a stock you previously bought reaches your valuation, why would you ever keep it?'"," u'https://old.reddit.com/r/investing/comments/b5byom/if_a_stock_you_previously_bought_reaches_your/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 8 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-26 10:03:12 [scrapy.extensions.logstats] INFO: Crawled 8 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b59oai/global_stocks_open_week_lower_this_morning_as/'," datetime.datetime(2019, 3, 26, 10, 3, 12, 534771)",'2019-03-26',u'News',u'0','stocks',u'Global stocks open week lower this morning as global growth fears grip markets'," u'https://old.reddit.com/r/stocks/comments/b59oai/global_stocks_open_week_lower_this_morning_as/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b5btch/rumblings_of_a_downturn/'," datetime.datetime(2019, 3, 26, 10, 6, 14, 524051)",'2019-03-26',None,u'9','investing',u'Rumblings of a downturn??'," u'https://old.reddit.com/r/investing/comments/b5btch/rumblings_of_a_downturn/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b590j8/can_anyone_explain_me_options_simply/'," datetime.datetime(2019, 3, 26, 10, 6, 44, 875429)",'2019-03-26',u'Question',u'2','stocks',u'Can anyone explain me options simply?'," u'https://old.reddit.com/r/stocks/comments/b590j8/can_anyone_explain_me_options_simply/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 8 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-26 10:07:15 [scrapy.extensions.logstats] INFO: Crawled 8 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b5bt0c/when_to_drip/'," datetime.datetime(2019, 3, 26, 10, 9, 46, 907824)",'2019-03-26',None,u'3','investing',u'When to DRIP?'," u'https://old.reddit.com/r/investing/comments/b5bt0c/when_to_drip/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b589oa/is_now_a_good_time_to_finally_short_wayfairw/'," datetime.datetime(2019, 3, 26, 10, 10, 17, 251865)",'2019-03-26',u'Ticker Discussion',u'0','stocks',u'Is now a good time to finally short Wayfair(W)?'," u'https://old.reddit.com/r/stocks/comments/b589oa/is_now_a_good_time_to_finally_short_wayfairw/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b5bk8m/zillows_house_flipping_averages_an_abysmal_06_per/'," datetime.datetime(2019, 3, 26, 10, 13, 50, 515955)",'2019-03-26',None,u'743','investing',u'Zillow\u2019s House Flipping Averages An Abysmal 0.6% Per Flip'," u'https://old.reddit.com/r/investing/comments/b5bk8m/zillows_house_flipping_averages_an_abysmal_06_per/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b57gkh/monthly_dividend_stocks/'," datetime.datetime(2019, 3, 26, 10, 14, 20, 814364)",'2019-03-26',None,u'3','StockMarket',u'Monthly Dividend Stocks'," u'https://old.reddit.com/r/StockMarket/comments/b57gkh/monthly_dividend_stocks/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b5bhqv/bad_side_of_etfs_and_index_funds/'," datetime.datetime(2019, 3, 26, 10, 17, 52, 891170)",'2019-03-26',u'Discussion',u'0','investing',u'Bad side of ETFs and Index Funds'," u'https://old.reddit.com/r/investing/comments/b5bhqv/bad_side_of_etfs_and_index_funds/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b529mf/how_to_do_good_research/'," datetime.datetime(2019, 3, 26, 10, 18, 23, 139609)",'2019-03-26',None,u'6','StockMarket',u'How To Do Good Research'," u'https://old.reddit.com/r/StockMarket/comments/b529mf/how_to_do_good_research/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b5baig/tax_free_municipals_vs_treasury_bonds_need/'," datetime.datetime(2019, 3, 26, 10, 21, 55, 390024)",'2019-03-26',None,u'0','investing',u'Tax Free Municipals VS Treasury Bonds Need Guidance'," u'https://old.reddit.com/r/investing/comments/b5baig/tax_free_municipals_vs_treasury_bonds_need/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b4zw97/investment_thesis_short_term/'," datetime.datetime(2019, 3, 26, 10, 22, 25, 732297)",'2019-03-26',None,u'2','StockMarket',u'Investment Thesis (short term)'," u'https://old.reddit.com/r/StockMarket/comments/b4zw97/investment_thesis_short_term/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b5ag42/separately_managed_account_vs_vanguard/'," datetime.datetime(2019, 3, 26, 10, 25, 57, 658518)",'2019-03-26',None,u'0','investing',u'Separately managed account vs. Vanguard'," u'https://old.reddit.com/r/investing/comments/b5ag42/separately_managed_account_vs_vanguard/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b4yupj/how_do_you_protect_against_market_downturn/'," datetime.datetime(2019, 3, 26, 10, 26, 27, 947769)",'2019-03-26',None,u'2','StockMarket',u'How do you protect against market downturn?'," u'https://old.reddit.com/r/StockMarket/comments/b4yupj/how_do_you_protect_against_market_downturn/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b4l7eo/how_does_one_start_an_index_fund/'," datetime.datetime(2019, 3, 26, 10, 28, 58, 729533)",'2019-03-26',None,u'0','StockMarket',u'How does one start an index fund ?'," u'https://old.reddit.com/r/StockMarket/comments/b4l7eo/how_does_one_start_an_index_fund/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b5abil/can_someone_explain_a_wash_sale/'," datetime.datetime(2019, 3, 26, 10, 30, 0, 147464)",'2019-03-26',u'Help',u'0','investing',u'Can someone explain a wash sale?'," u'https://old.reddit.com/r/investing/comments/b5abil/can_someone_explain_a_wash_sale/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b4yi58/why_most_traders_fail/'," datetime.datetime(2019, 3, 26, 10, 30, 30, 366266)",'2019-03-26',None,u'164','StockMarket',u'Why Most Traders Fail'," u'https://old.reddit.com/r/StockMarket/comments/b4yi58/why_most_traders_fail/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b5a0zf/pfizer_what_is_it_worth/'," datetime.datetime(2019, 3, 26, 10, 34, 2, 527988)",'2019-03-26',None,u'2','investing',"u'Pfizer, what is it worth?'"," u'https://old.reddit.com/r/investing/comments/b5a0zf/pfizer_what_is_it_worth/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b4ygjv/help_me_get_into_trading/'," datetime.datetime(2019, 3, 26, 10, 34, 32, 836681)",'2019-03-26',None,u'3','StockMarket',u'Help me get into trading .'," u'https://old.reddit.com/r/StockMarket/comments/b4ygjv/help_me_get_into_trading/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b4xzgx/what_is_an_good_education_for_going_into_trading/'," datetime.datetime(2019, 3, 26, 10, 38, 5, 55556)",'2019-03-26',None,u'7','StockMarket',u'What is an good education for going into trading?'," u'https://old.reddit.com/r/StockMarket/comments/b4xzgx/what_is_an_good_education_for_going_into_trading/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/thewallstreet/comments/b36l7y/deviations_poc_and_value_area_for_wednesday_march/'," datetime.datetime(2019, 3, 26, 10, 40, 36, 626785)",'2019-03-26',None,u'7','thewallstreet',"u'Deviations, POC, and Value Area for Wednesday, March 20, 2019'"," u'https://i.imgur.com/wZUZ4YO.png'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
2019-03-26 10:41:06 [scrapy.core.scraper] ERROR: Error processing "
https://old.reddit.com/r/thewallstreet/comments/b33gue/nightly_trading_discussion_march_1920/'," datetime.datetime(2019, 3, 26, 10, 44, 8, 696948)",'2019-03-26',u'Daily',u'14','thewallstreet',u'Nightly Trading Discussion - (March 19/20)'," u'https://old.reddit.com/r/thewallstreet/comments/b33gue/nightly_trading_discussion_march_1920/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b4o1xv/pre_market_move_march_25_2019/'," datetime.datetime(2019, 3, 26, 10, 44, 38, 941897)",'2019-03-26',None,u'7','StockMarket',"u'Pre Market Move March 25, 2019'"," u'https://old.reddit.com/r/StockMarket/comments/b4o1xv/pre_market_move_march_25_2019/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/ValueInvesting/comments/awoim5/hmc_is_an_incredible_buy_for_value_investors_or/'," datetime.datetime(2019, 3, 26, 10, 45, 8, 949415)",'2019-03-26',u'stocks',u'6','ValueInvesting',u'HMC is an incredible buy for value investors or I\u2019m missing something?'," u'https://old.reddit.com/r/ValueInvesting/comments/awoim5/hmc_is_an_incredible_buy_for_value_investors_or/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
2019-03-26 10:45:39 [scrapy.core.scraper] ERROR: Error processing "
https://old.reddit.com/r/thewallstreet/comments/b32422/post_market_discussion_march_19/'," datetime.datetime(2019, 3, 26, 10, 47, 10, 436030)",'2019-03-26',u'Daily',u'9','thewallstreet',u'Post Market Discussion - (March 19)'," u'https://old.reddit.com/r/thewallstreet/comments/b32422/post_market_discussion_march_19/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-26 10:47:40 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b4medm/rite_aid_post_reverse_split_looks_enticing_110/'," datetime.datetime(2019, 3, 26, 10, 47, 40, 720476)",'2019-03-26',None,u'0','StockMarket',u'Rite Aid Post Reverse Split Looks Enticing. 1-10 Split Results In Market Cap Of $70 million'," u'https://old.reddit.com/r/StockMarket/comments/b4medm/rite_aid_post_reverse_split_looks_enticing_110/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/thewallstreet/comments/b2wb33/daily_discussion_march_19/'," datetime.datetime(2019, 3, 26, 10, 49, 41, 919813)",'2019-03-26',u'Daily',u'9','thewallstreet',u'Daily Discussion - (March 19)'," u'https://old.reddit.com/r/thewallstreet/comments/b2wb33/daily_discussion_march_19/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-26 10:50:12 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b4lal1/will_airbus_stock_rise_after_the_troubles_at/'," datetime.datetime(2019, 3, 26, 10, 50, 12, 186467)",'2019-03-26',None,u'1','StockMarket',u'Will Airbus stock rise after the troubles at Boeing?'," u'https://old.reddit.com/r/StockMarket/comments/b4lal1/will_airbus_stock_rise_after_the_troubles_at/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b4l7g2/question_on_sp500_etfs/'," datetime.datetime(2019, 3, 26, 10, 52, 13, 94991)",'2019-03-26',None,u'1','StockMarket',u'Question on S&P500 ETFs'," u'https://old.reddit.com/r/StockMarket/comments/b4l7g2/question_on_sp500_etfs/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-26 10:52:43 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Daytrading/comments/b3ss57/how_does_this_scenario_count_towards_a_day_trade/'," datetime.datetime(2019, 3, 26, 10, 52, 43, 318680)",'2019-03-26',None,u'3','Daytrading',u'How does this scenario count towards a day trade according to the PDT rule?'," u'https://old.reddit.com/r/Daytrading/comments/b3ss57/how_does_this_scenario_count_towards_a_day_trade/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Trading/comments/axiobq/newbie/'," datetime.datetime(2019, 3, 26, 10, 53, 13, 569516)",'2019-03-26',None,u'1','Trading',u'Newbie'," u'https://old.reddit.com/r/Trading/comments/axiobq/newbie/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Daytrading/comments/b3sd3u/tradezero_pro_v3_how_to_maximise_the_main_window/'," datetime.datetime(2019, 3, 26, 10, 54, 14, 121608)",'2019-03-26',None,u'1','Daytrading',u'TradeZero Pro V3 - How to maximise the main window?'," u'https://old.reddit.com/r/Daytrading/comments/b3sd3u/tradezero_pro_v3_how_to_maximise_the_main_window/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Daytrading/comments/b3q0p6/what_is_your_favorite_instrument_to_trade_and_why/'," datetime.datetime(2019, 3, 26, 10, 55, 14, 616923)",'2019-03-26',None,u'6','Daytrading',u'What is your favorite instrument to trade and why?'," u'https://old.reddit.com/r/Daytrading/comments/b3q0p6/what_is_your_favorite_instrument_to_trade_and_why/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-26 10:55:44 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Daytrading/comments/b3p6ct/beginner_looking_at_the_correct_platform/'," datetime.datetime(2019, 3, 26, 10, 55, 44, 861893)",'2019-03-26',None,u'3','Daytrading',u'Beginner looking at the correct platform'," u'https://old.reddit.com/r/Daytrading/comments/b3p6ct/beginner_looking_at_the_correct_platform/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-26 10:56:15 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
INFO:scrapy.core.engine:Closing spider (finished)
2019-03-26 10:56:15 [scrapy.core.engine] INFO: Closing spider (finished)
INFO:scrapy.statscollectors:Dumping Scrapy stats:
"
https://old.reddit.com/r/stocks/comments/b5oyh7/nvidia_ceo_says_no_plans_for_more_acquisitions/'," datetime.datetime(2019, 3, 26, 11, 4, 33, 920723)",'2019-03-26',None,u'2','stocks',u'Nvidia CEO says no plans for more acquisitions after Mellanox deal'," u'https://old.reddit.com/r/stocks/comments/b5oyh7/nvidia_ceo_says_no_plans_for_more_acquisitions/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b5jerw/difficult_question_that_needs_to_be_asked/'," datetime.datetime(2019, 3, 26, 11, 13, 36, 722558)",'2019-03-26',u'Discussion',u'6','wallstreetbets',u'Difficult question that needs to be asked...'," u'https://old.reddit.com/r/wallstreetbets/comments/b5jerw/difficult_question_that_needs_to_be_asked/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Trading/comments/axy4gz/can_you_bet_on_a_shares_future_price_to_be_below/'," datetime.datetime(2019, 3, 26, 11, 16, 10, 535036)",'2019-03-26',None,u'1','Trading',"u'Can you bet on a shares future price to be below a certain amount anywhere, within say a day or a week?'"," u'https://old.reddit.com/r/Trading/comments/axy4gz/can_you_bet_on_a_shares_future_price_to_be_below/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b5e6la/ci/'," datetime.datetime(2019, 3, 26, 11, 27, 15, 463513)",'2019-03-26',None,u'0','stocks',u'CI'," u'https://old.reddit.com/r/stocks/comments/b5e6la/ci/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b5mz1i/apple_news_benzinga_wall_street_journal_basically/'," datetime.datetime(2019, 3, 26, 11, 32, 7, 290472)",'2019-03-26',None,u'2','wallstreetbets',"u'Apple news +, Benzinga, Wall Street Journal basically for $10 totally worth it'"," u'https://old.reddit.com/r/wallstreetbets/comments/b5mz1i/apple_news_benzinga_wall_street_journal_basically/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-03-26 11:32:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b5mubv/is_new_imac_trash/'," datetime.datetime(2019, 3, 26, 11, 38, 41, 808687)",'2019-03-26',u'Discussion',u'1','wallstreetbets',u'Is new iMac trash?'," u'https://www.youtube.com/watch?v=7wICIuHK4-E'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/Stock_Picks/new/> (referer: None)
2019-03-26 11:39:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/Stock_Picks/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b5enlm/general_advice_for_when_to_sell_stocks/'," datetime.datetime(2019, 3, 26, 11, 41, 18, 52760)",'2019-03-26',None,u'2','stocks',u'General advice for when to sell stocks'," u'https://old.reddit.com/r/stocks/comments/b5enlm/general_advice_for_when_to_sell_stocks/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b5iis2/thoughts_on_dividend_darlings_when_they_hit_high/'," datetime.datetime(2019, 3, 26, 11, 53, 54, 385175)",'2019-03-26',None,u'2','investing',u'Thoughts on Dividend Darlings when they hit high payout ratios?'," u'https://old.reddit.com/r/investing/comments/b5iis2/thoughts_on_dividend_darlings_when_they_hit_high/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b5epat/any_hope_for_dropbox_to_escape_its_doldrums/'," datetime.datetime(2019, 3, 26, 11, 56, 18, 592348)",'2019-03-26',None,u'3','stocks',u'Any hope for Dropbox to escape its doldrums?'," u'https://old.reddit.com/r/stocks/comments/b5epat/any_hope_for_dropbox_to_escape_its_doldrums/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b5jefz/why_do_most_people_prefer_etfs_and_a_diversified/'," datetime.datetime(2019, 3, 26, 11, 57, 22, 680800)",'2019-03-26',u'Help',u'1','investing',u'Why do most people prefer ETFs and a diversified portfolio instead of concentrated stocks? I\u2019m a newbie to the investment world.'," u'https://old.reddit.com/r/investing/comments/b5jefz/why_do_most_people_prefer_etfs_and_a_diversified/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b5finn/dont_upvote_just_need_to_know_what_this_indicator/'," datetime.datetime(2019, 3, 26, 12, 2, 15, 708474)",'2019-03-26',None,u'8','stocks',"u""Don't upvote just need to know what this indicator is called"""," u'https://old.reddit.com/r/stocks/comments/b5finn/dont_upvote_just_need_to_know_what_this_indicator/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b5jjr2/apple_credit_card/'," datetime.datetime(2019, 3, 26, 12, 3, 40, 556609)",'2019-03-26',u'Discussion',u'10','wallstreetbets',u'Apple credit card'," u'https://old.reddit.com/r/wallstreetbets/comments/b5jjr2/apple_credit_card/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b5jzqc/do_you_trust_robo_advisors/'," datetime.datetime(2019, 3, 26, 12, 18, 26, 123774)",'2019-03-26',None,u'0','investing',u'Do you trust robo advisors?'," u'https://old.reddit.com/r/investing/comments/b5jzqc/do_you_trust_robo_advisors/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-26 12:18:56 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b5fjc1/i_wanna_invest_in_artificial_intelligence/'," datetime.datetime(2019, 3, 26, 12, 21, 45, 684715)",'2019-03-26',None,u'3','stocks',u'I wanna invest in artificial intelligence'," u'https://old.reddit.com/r/stocks/comments/b5fjc1/i_wanna_invest_in_artificial_intelligence/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-26 12:22:16 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b5gfwu/penny_stocks_that_is_still_ventures/'," datetime.datetime(2019, 3, 26, 12, 23, 9, 634073)",'2019-03-26',None,u'0','stocks',u'Penny stocks that is still ventures?'," u'https://old.reddit.com/r/stocks/comments/b5gfwu/penny_stocks_that_is_still_ventures/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b5km06/bond_recession/'," datetime.datetime(2019, 3, 26, 12, 32, 59, 216652)",'2019-03-26',None,u'1','investing',u'Bond recession'," u'https://old.reddit.com/r/investing/comments/b5km06/bond_recession/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Daytrading/comments/b3sukl/day_1_of_my_papertrading_my_strategy_here_are_my/'," datetime.datetime(2019, 3, 26, 12, 35, 43, 646342)",'2019-03-26',None,u'3','Daytrading',"u'""Day 1"" of my papertrading my strategy. Here are my results so far Hope it continues!'"," u'https://old.reddit.com/r/Daytrading/comments/b3sukl/day_1_of_my_papertrading_my_strategy_here_are_my/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-26 12:36:13 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b5jkah/what_are_technical_indicators/'," datetime.datetime(2019, 3, 26, 12, 53, 13, 462717)",'2019-03-26',u'Shitpost',u'218','wallstreetbets',u'What are technical indicators?'," u'https://old.reddit.com/r/wallstreetbets/comments/b5jkah/what_are_technical_indicators/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b5ksa6/thoughts_on_fidelity_full_view/'," datetime.datetime(2019, 3, 26, 12, 55, 30, 924557)",'2019-03-26',None,u'1','investing',u'Thoughts on Fidelity Full View?'," u'https://old.reddit.com/r/investing/comments/b5ksa6/thoughts_on_fidelity_full_view/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b5hq0d/what_does_everyone_think_of_apples_new/'," datetime.datetime(2019, 3, 26, 12, 58, 7, 476596)",'2019-03-26',None,u'25','stocks',u'What does everyone think of Apple\u2019s new announcements. With the news subscription and Apple card. Also with this new card do you think Visa or MasterCard should be worried?'," u'https://old.reddit.com/r/stocks/comments/b5hq0d/what_does_everyone_think_of_apples_new/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b58ily/out_of_stocks_for_the_foreseeable_future/'," datetime.datetime(2019, 3, 26, 13, 11, 18, 383843)",'2019-03-26',None,u'32','StockMarket',u'Out of stocks for the foreseeable future'," u'https://old.reddit.com/r/StockMarket/comments/b58ily/out_of_stocks_for_the_foreseeable_future/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b5jrg0/anyone_else_been_trading_iron_condors_on_ba/'," datetime.datetime(2019, 3, 26, 13, 27, 46, 77984)",'2019-03-26',u'Options',u'4','wallstreetbets',u'Anyone else been trading Iron Condors on BA?'," u'https://old.reddit.com/r/wallstreetbets/comments/b5jrg0/anyone_else_been_trading_iron_condors_on_ba/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b5hqk1/mcdonalds_announces_acquisition_to_personalize/'," datetime.datetime(2019, 3, 26, 13, 33, 13, 121497)",'2019-03-26',None,u'48','stocks',u'McDonald\u2019s announces acquisition to personalize the drive thru'," u'https://old.reddit.com/r/stocks/comments/b5hqk1/mcdonalds_announces_acquisition_to_personalize/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b5kwsa/general_question_does_barrons_magazine_parallel/'," datetime.datetime(2019, 3, 26, 13, 45, 31, 514556)",'2019-03-26',u'Discussion',u'0','investing',"u'General Question: Does Barron\u2019s magazine parallel the typical political and economic views of Murdoch\u2019s News Corp, or is the reporting more \u201dliberal\u201d ?'"," u'https://old.reddit.com/r/investing/comments/b5kwsa/general_question_does_barrons_magazine_parallel/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-26 13:46:01 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b5l0ef/i_keep_hearing_different_things_about_dividend/'," datetime.datetime(2019, 3, 26, 13, 47, 59, 677681)",'2019-03-26',None,u'0','investing',"u'I keep hearing different things about dividend ETFs. Some people love them, some don\u2019t. What is your opinion?'"," u'https://old.reddit.com/r/investing/comments/b5l0ef/i_keep_hearing_different_things_about_dividend/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b5jwrn/does_adobes_dominance_in_its_field_and_the/'," datetime.datetime(2019, 3, 26, 13, 48, 6, 374117)",'2019-03-26',None,u'1','stocks',"u""Does Adobe's dominance in it's field and the necessity of it's products for many industries + subscription model make it more recession-proof than industries that rely on public consumption?"""," u'https://old.reddit.com/r/stocks/comments/b5jwrn/does_adobes_dominance_in_its_field_and_the/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b5jr2w/looking_for_suggestions_of_apps_debit_cards_and/'," datetime.datetime(2019, 3, 26, 13, 53, 9, 799326)",'2019-03-26',None,u'1','stocks',"u'Looking for suggestions of apps, debit cards, and or credit cards that give ""Stock Back"" as rewards'"," u'https://old.reddit.com/r/stocks/comments/b5jr2w/looking_for_suggestions_of_apps_debit_cards_and/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b5ljsu/under_rated_recession_proof_stock_choices/'," datetime.datetime(2019, 3, 26, 13, 57, 59, 809296)",'2019-03-26',None,u'1','investing',u'Under rated recession proof stock choices'," u'https://old.reddit.com/r/investing/comments/b5ljsu/under_rated_recession_proof_stock_choices/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b5jeg6/im_an_astrophysicist_and_i_perform_my_own_simple/'," datetime.datetime(2019, 3, 26, 13, 58, 13, 178979)",'2019-03-26',None,u'138','stocks',"u""I'm an astrophysicist and I perform my own (simple) stock analyses based on kinematic principles: comments welcome! (March 25, 2019)"""," u'https://old.reddit.com/r/stocks/comments/b5jeg6/im_an_astrophysicist_and_i_perform_my_own_simple/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b5k4pj/did_i_do_it_right/'," datetime.datetime(2019, 3, 26, 13, 58, 43, 430525)",'2019-03-26',u'Loss',u'13','wallstreetbets',u'Did I do it right'," u'https://old.reddit.com/r/wallstreetbets/comments/b5k4pj/did_i_do_it_right/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b5lxyd/help_for_my_401k/'," datetime.datetime(2019, 3, 26, 14, 3, 5, 346410)",'2019-03-26',None,u'1','stocks',u'Help For My 401K'," u'https://old.reddit.com/r/stocks/comments/b5lxyd/help_for_my_401k/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b5if4k/wherehow_do_i_start_buying_and_selling_stocks/'," datetime.datetime(2019, 3, 26, 14, 3, 16, 381458)",'2019-03-26',u'Advice',u'1','stocks',u'Where/How do I start buying and selling stocks? What apps/websites should I be using?'," u'https://old.reddit.com/r/stocks/comments/b5if4k/wherehow_do_i_start_buying_and_selling_stocks/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b5k810/tim_apples_new_card/'," datetime.datetime(2019, 3, 26, 14, 37, 16, 995614)",'2019-03-26',u'Shitpost',u'1079','wallstreetbets',"u""Tim Apple's new card"""," u'https://old.reddit.com/r/wallstreetbets/comments/b5k810/tim_apples_new_card/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b5klm3/for_real_this_time_short_snap/'," datetime.datetime(2019, 3, 26, 14, 38, 10, 301917)",'2019-03-26',u'Shitpost',u'640','wallstreetbets',u'For real this time... short $SNAP'," u'https://old.reddit.com/r/wallstreetbets/comments/b5klm3/for_real_this_time_short_snap/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b5m6fe/papa_johns/'," datetime.datetime(2019, 3, 26, 14, 38, 24, 959069)",'2019-03-26',None,u'0','investing',u'Papa johns'," u'https://old.reddit.com/r/investing/comments/b5m6fe/papa_johns/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b5ksp1/wait_why_do_we_need_the_banks_again/'," datetime.datetime(2019, 3, 26, 14, 38, 38, 23172)",'2019-03-26',u'Discussion',u'20','wallstreetbets',u'Wait why do we need the banks again'," u'https://old.reddit.com/r/wallstreetbets/comments/b5ksp1/wait_why_do_we_need_the_banks_again/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b5lpot/samsung_warns_firstquarter_earnings_will_fall/'," datetime.datetime(2019, 3, 26, 14, 40, 0, 515951)",'2019-03-26',None,u'65','investing',u'Samsung warns first-quarter earnings will fall short of expectations (CNBC)'," u'https://old.reddit.com/r/investing/comments/b5lpot/samsung_warns_firstquarter_earnings_will_fall/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b5lyip/apple_announced_its_apple_tv_product_shares_down/'," datetime.datetime(2019, 3, 26, 14, 40, 28, 914168)",'2019-03-26',None,u'558','investing',"u'Apple announced its Apple TV product. Shares down 1.21%. Meanwhile, Roku up 4.68% and Netflix up 1.45%.'"," u'https://old.reddit.com/r/investing/comments/b5lyip/apple_announced_its_apple_tv_product_shares_down/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-26 14:40:59 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b5m3ys/t_bonds_versus_cds/'," datetime.datetime(2019, 3, 26, 14, 42, 27, 930999)",'2019-03-26',None,u'1','investing',u'T bonds versus cds'," u'https://old.reddit.com/r/investing/comments/b5m3ys/t_bonds_versus_cds/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b5kdpg/kevin_o_leary_giving_investing_advice/'," datetime.datetime(2019, 3, 26, 14, 42, 43, 561677)",'2019-03-26',u'Shitpost',u'89','wallstreetbets',u'Kevin o Leary giving investing advice'," u'https://old.reddit.com/r/wallstreetbets/comments/b5kdpg/kevin_o_leary_giving_investing_advice/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b5l13g/does_anyone_remember_that_dude_that_learned_to/'," datetime.datetime(2019, 3, 26, 15, 8, 37, 255417)",'2019-03-26',u'Discussion',u'28','wallstreetbets',u'Does anyone remember that dude that learned to code and shared a program that showed stock reaction to Trumps tweets?'," u'https://old.reddit.com/r/wallstreetbets/comments/b5l13g/does_anyone_remember_that_dude_that_learned_to/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Daytrading/comments/b3u6k1/need_help_with_forex/'," datetime.datetime(2019, 3, 26, 15, 10, 49, 746238)",'2019-03-26',None,u'1','Daytrading',u'Need help with forex'," u'https://old.reddit.com/r/Daytrading/comments/b3u6k1/need_help_with_forex/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-26 15:11:19 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
INFO:scrapy.core.engine:Closing spider (finished)
2019-03-26 15:11:19 [scrapy.core.engine] INFO: Closing spider (finished)
INFO:scrapy.statscollectors:Dumping Scrapy stats:
"
https://old.reddit.com/r/wallstreetbets/comments/b5myg2/everyone_beeing_in_panic_mode_about_an_incoming/'," datetime.datetime(2019, 3, 26, 15, 13, 25, 64902)",'2019-03-26',u'Discussion',u'0','wallstreetbets',u'Everyone beeing in panic mode about an incoming recession could be a good reason to panic first right now'," u'https://old.reddit.com/r/wallstreetbets/comments/b5myg2/everyone_beeing_in_panic_mode_about_an_incoming/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b5la3p/im_toooooooo_deeeeeeeep/'," datetime.datetime(2019, 3, 26, 15, 13, 34, 322995)",'2019-03-26',u'Options',u'20','wallstreetbets',"u""I'M TOOOOOOOO DEEEEEEEEP!!!!"""," u'https://old.reddit.com/r/wallstreetbets/comments/b5la3p/im_toooooooo_deeeeeeeep/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b5mlvm/value_factor_or_straight_index/'," datetime.datetime(2019, 3, 26, 15, 16, 53, 543989)",'2019-03-26',None,u'2','investing',u'Value factor or straight index?'," u'https://old.reddit.com/r/investing/comments/b5mlvm/value_factor_or_straight_index/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b5lb8k/earnings_season_is_far_away_thought_this_was/'," datetime.datetime(2019, 3, 26, 15, 18, 35, 28887)",'2019-03-26',u'Stocks',u'34','wallstreetbets',u'Earnings season is far away thought this was interesting'," u'https://old.reddit.com/r/wallstreetbets/comments/b5lb8k/earnings_season_is_far_away_thought_this_was/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b5n5np/pinterest_reveals_s1_records_annual_losses_but/'," datetime.datetime(2019, 3, 26, 15, 18, 54, 188929)",'2019-03-26',u'Shitpost',u'30','wallstreetbets',"u'Pinterest reveals S-1, records annual losses but quarterly profit (White Girl Index?)'"," u'https://pitchbook.com/news/articles/pinterest-reveals-s-1-records-annual-losses-but-quarterly-profit'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Stock_Picks/comments/av6grm/what_to_make_of_gdot_at_the_moment/'," datetime.datetime(2019, 3, 26, 15, 19, 18, 746034)",'2019-03-26',u'Discussion',u'8','Stock_Picks',u'What to make of GDOT at the moment?'," u'https://old.reddit.com/r/Stock_Picks/comments/av6grm/what_to_make_of_gdot_at_the_moment/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-26 15:19:49 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b5ma54/its_a_tough_environment_for_retail_but_ascena/'," datetime.datetime(2019, 3, 26, 15, 27, 14, 316370)",'2019-03-26',None,u'0','stocks',"u'It\u2019s a tough environment for retail but Ascena (ASNA) is doing the right thing by cutting costs, paying down debt and focusing on its strong performing premium and kids fashions.'"," u'https://old.reddit.com/r/stocks/comments/b5ma54/its_a_tough_environment_for_retail_but_ascena/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b5o5cm/yield_curve_inversion_will_lead_to_asset_bubbles/'," datetime.datetime(2019, 3, 26, 15, 33, 20, 772497)",'2019-03-26',u'Technicals',u'5','wallstreetbets',u'Yield curve inversion will lead to asset bubbles not recession'," u'https://www.businessinsider.com/yield-curve-inversion-will-lead-to-asset-bubbles-not-recession-2019-3?r=US&IR=T'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b5nont/does_buying_a_3month_tbill_today_ensure_a_2/'," datetime.datetime(2019, 3, 26, 15, 33, 50, 808164)",'2019-03-26',u'Education',u'25','investing',u'Does buying a 3-month T-Bill today ensure a 2%+ guaranteed return in 3 months?'," u'https://old.reddit.com/r/investing/comments/b5nont/does_buying_a_3month_tbill_today_ensure_a_2/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b5pp91/discord_ipo/'," datetime.datetime(2019, 3, 26, 15, 38, 50, 818424)",'2019-03-26',u'Meme',u'71','wallstreetbets',u'Discord IPO'," u'https://old.reddit.com/r/wallstreetbets/comments/b5pp91/discord_ipo/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b5mspz/cmgo/'," datetime.datetime(2019, 3, 26, 15, 40, 43, 931972)",'2019-03-26',None,u'0','stocks',u'CMGO'," u'https://old.reddit.com/r/stocks/comments/b5mspz/cmgo/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b5n7z5/uber_acquires_its_middleeastern_rival_careem_i/'," datetime.datetime(2019, 3, 26, 15, 41, 39, 116940)",'2019-03-26',u'Ticker Question',u'1','stocks',u'Uber acquires its middle-eastern rival Careem + I own stock options in Careem = Good/Bad?'," u'https://old.reddit.com/r/stocks/comments/b5n7z5/uber_acquires_its_middleeastern_rival_careem_i/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b5p5jl/helpful_bookmark_for_dd_delinquency_rate_on/'," datetime.datetime(2019, 3, 26, 15, 42, 53, 347763)",'2019-03-26',u'DD',u'28','wallstreetbets',u'Helpful Bookmark for DD: Delinquency Rate on Credit Cards at US Banks (Updated Monthly)'," u'https://fred.stlouisfed.org/series/DRCCLACBS'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b5nw7w/to_figure_out_overall_stock_market_trend/'," datetime.datetime(2019, 3, 26, 15, 45, 37, 468474)",'2019-03-26',u'Question',u'0','stocks',u'To figure out overall stock market trend'," u'https://old.reddit.com/r/stocks/comments/b5nw7w/to_figure_out_overall_stock_market_trend/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b5oxkl/updated_cron_positionhad_to_edit_a_little_bit_to/'," datetime.datetime(2019, 3, 26, 15, 47, 26, 446841)",'2019-03-26',u'Options',u'34','wallstreetbets',u'Updated $cron position.....had to edit a little bit to leverage losses yesterday. Dreaming of the $Tendies to come at open.'," u'https://old.reddit.com/r/wallstreetbets/comments/b5oxkl/updated_cron_positionhad_to_edit_a_little_bit_to/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b5odj0/excess_return_for_famous_investors_over_time_2014/'," datetime.datetime(2019, 3, 26, 15, 52, 29, 879163)",'2019-03-26',u'Discussion',u'21','wallstreetbets',u'Excess Return for Famous Investors Over Time (2014)'," u'https://www.reddit.com/user/Fatherthinger/comments/b5o96u/excess_return_for_famous_investors_over_time_2014/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-26 15:53:00 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b5obpj/daily_discussion_thread_march_26_2019/'," datetime.datetime(2019, 3, 26, 15, 57, 33, 430493)",'2019-03-26',u'Daily Discussion',u'47','wallstreetbets',"u'Daily Discussion Thread - March 26, 2019'"," u'https://old.reddit.com/r/wallstreetbets/comments/b5obpj/daily_discussion_thread_march_26_2019/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-26 15:58:03 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b5pt8j/goldman_sachs_analysts_are_underwhelmed_by_the/'," datetime.datetime(2019, 3, 26, 16, 21, 13, 860329)",'2019-03-26',u'Discussion',u'127','wallstreetbets',u'Goldman Sachs analysts are underwhelmed by the new Goldman Sachs-Apple credit card'," u'https://www.cnbc.com/2019/03/26/goldman-analysts-are-underwhelmed-by-the-new-goldman-sachs-apple-card.html'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/StockMarket/new/> (referer: None)
2019-03-26 16:21:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/StockMarket/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b5oh2h/uber_announces_31_billion_deal_to_buy_middle_east/'," datetime.datetime(2019, 3, 26, 16, 23, 20, 36794)",'2019-03-26',None,u'157','stocks',u'Uber announces $3.1 billion deal to buy Middle East rival Careem'," u'https://old.reddit.com/r/stocks/comments/b5oh2h/uber_announces_31_billion_deal_to_buy_middle_east/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b58rp4/bet_you_a_bitcoin_beer_thirty_comes_early_for_the/'," datetime.datetime(2019, 3, 26, 16, 33, 16, 945708)",'2019-03-26',None,u'1','StockMarket',"u'Bet you a Bitcoin beer thirty comes early for the NYSE today,'"," u'https://old.reddit.com/r/StockMarket/comments/b58rp4/bet_you_a_bitcoin_beer_thirty_comes_early_for_the/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-26 16:33:47 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b5opfa/beginning_in_stock_trade_and_was_wondering_about/'," datetime.datetime(2019, 3, 26, 16, 38, 41, 63413)",'2019-03-26',u'Advice Request',u'1','stocks',u'Beginning in stock trade and was wondering about different brokerage accounts.'," u'https://old.reddit.com/r/stocks/comments/b5opfa/beginning_in_stock_trade_and_was_wondering_about/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b5nzia/volatility_has_been_volatile_lately/'," datetime.datetime(2019, 3, 26, 16, 39, 30, 650686)",'2019-03-26',None,u'0','investing',u'Volatility has been volatile lately'," u'https://old.reddit.com/r/investing/comments/b5nzia/volatility_has_been_volatile_lately/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b5a7ve/todays_premarket_news_monday_march_25th_2019/'," datetime.datetime(2019, 3, 26, 16, 41, 49, 647582)",'2019-03-26',u'News',u'35','StockMarket',"u""Today's Pre-Market News [Monday, March 25th, 2019]"""," u'https://old.reddit.com/r/StockMarket/comments/b5a7ve/todays_premarket_news_monday_march_25th_2019/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b5oai9/i_dont_understand_treasury_yields/'," datetime.datetime(2019, 3, 26, 16, 51, 59, 25381)",'2019-03-26',None,u'0','investing',u'I don\u2019t understand treasury yields'," u'https://old.reddit.com/r/investing/comments/b5oai9/i_dont_understand_treasury_yields/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b5onlg/using_investor_relations_to_better_understand_a/'," datetime.datetime(2019, 3, 26, 17, 2, 56, 706247)",'2019-03-26',None,u'6','investing',u'Using Investor Relations to better understand a business'," u'https://old.reddit.com/r/investing/comments/b5onlg/using_investor_relations_to_better_understand_a/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b5iykt/how_would_bacs_stock_react_to_the_equity_market/'," datetime.datetime(2019, 3, 26, 17, 12, 17, 363138)",'2019-03-26',None,u'2','StockMarket',"u""How would BAC's stock react to the equity market crashing"""," u'https://old.reddit.com/r/StockMarket/comments/b5iykt/how_would_bacs_stock_react_to_the_equity_market/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b5p4ls/a_rinvesting_sub_meets_his_babysitter/'," datetime.datetime(2019, 3, 26, 17, 18, 24, 866982)",'2019-03-26',None,u'1','investing',u'A r/investing Sub Meets His Babysitter'," u'https://old.reddit.com/r/investing/comments/b5p4ls/a_rinvesting_sub_meets_his_babysitter/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Daytrading/comments/b3u6m8/moving_averages_change_between_graphs_thinkor_swim/'," datetime.datetime(2019, 3, 26, 17, 30, 43, 579611)",'2019-03-26',None,u'2','Daytrading',u'Moving averages change between graphs Thinkor Swim'," u'https://old.reddit.com/r/Daytrading/comments/b3u6m8/moving_averages_change_between_graphs_thinkor_swim/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-26 17:31:13 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
INFO:scrapy.core.engine:Closing spider (finished)
2019-03-26 17:31:13 [scrapy.core.engine] INFO: Closing spider (finished)
INFO:scrapy.statscollectors:Dumping Scrapy stats:
"
https://old.reddit.com/r/wallstreetbets/comments/b5qa45/airbus_secures_35_billion_china_deal_in_new_blow/'," datetime.datetime(2019, 3, 26, 17, 33, 28, 67944)",'2019-03-26',u'Fundamentals',u'81','wallstreetbets',u'Airbus Secures $35 Billion China Deal in New Blow to Boeing'," u'https://finance.yahoo.com/news/airbus-seals-long-awaited-aircraft-163320433.html'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/thewallstreet/comments/b37x5l/daily_spx_tpos_03192019/'," datetime.datetime(2019, 3, 26, 17, 36, 12, 427596)",'2019-03-26',None,u'8','thewallstreet',"u""Daily SPX TPO's 03-19-2019"""," u'https://i.imgur.com/7AhdxTh.png'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b5pug8/thanks_timmy/'," datetime.datetime(2019, 3, 26, 17, 38, 31, 19314)",'2019-03-26',u'Gain',u'369','wallstreetbets',u'Thanks Timmy!'," u'https://old.reddit.com/r/wallstreetbets/comments/b5pug8/thanks_timmy/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b5abkq/press_release_stem_holdings_just_entered_the/'," datetime.datetime(2019, 3, 26, 18, 29, 10, 344117)",'2019-03-26',None,u'10','StockMarket',u'Press Release: Stem Holdings just entered the Cannabis industry with JOINT partnership with $PSIQ.'," u'https://old.reddit.com/r/StockMarket/comments/b5abkq/press_release_stem_holdings_just_entered_the/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b5p609/rate_my_newbie_portfolio/'," datetime.datetime(2019, 3, 26, 18, 29, 55, 416355)",'2019-03-26',None,u'0','investing',u'Rate my Newbie Portfolio'," u'https://old.reddit.com/r/investing/comments/b5p609/rate_my_newbie_portfolio/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Daytrading/comments/b3uyvd/where_to_find_accurate_reliable_information_for_a/'," datetime.datetime(2019, 3, 26, 18, 33, 42, 899151)",'2019-03-26',None,u'3','Daytrading',"u'Where to find accurate, reliable information for a newbie?'"," u'https://old.reddit.com/r/Daytrading/comments/b3uyvd/where_to_find_accurate_reliable_information_for_a/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-26 18:34:13 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
INFO:scrapy.core.engine:Closing spider (finished)
2019-03-26 18:34:13 [scrapy.core.engine] INFO: Closing spider (finished)
INFO:scrapy.statscollectors:Dumping Scrapy stats:
"
https://old.reddit.com/r/stocks/comments/b5tohn/retailers_want_to_go_cashless_but_opponents_say/'," datetime.datetime(2019, 3, 26, 18, 43, 34, 792001)",'2019-03-26',None,u'1','stocks',"u""Retailers want to go cashless. But opponents say that's discriminatory"""," u'https://old.reddit.com/r/stocks/comments/b5tohn/retailers_want_to_go_cashless_but_opponents_say/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b5qssq/tim_apple_unveils_the_latest_apple_tech/'," datetime.datetime(2019, 3, 26, 18, 43, 38, 849998)",'2019-03-26',u'Satire',u'103','wallstreetbets',u'Tim Apple Unveils the Latest Apple Tech'," u'https://youtu.be/rw2nkoGLhrE'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b5rkc9/shares_of_canadian_pot_company_cronos_slide_after/'," datetime.datetime(2019, 3, 26, 19, 12, 52, 579039)",'2019-03-26',None,u'0','stocks',u'Shares of Canadian pot company Cronos slide after revenue disappoints'," u'https://old.reddit.com/r/stocks/comments/b5rkc9/shares_of_canadian_pot_company_cronos_slide_after/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b5tia2/cool_chart_set_ups_going_into_nbev_er/'," datetime.datetime(2019, 3, 26, 19, 19, 38, 419610)",'2019-03-26',u'Technicals',u'0','wallstreetbets',u'Cool chart set ups going into Nbev ER'," u'https://medium.com/@epicmedia/3-hemp-and-marijuana-stocks-to-watch-c8e637ea174f'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 4 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2019-03-26 19:20:08 [scrapy.extensions.logstats] INFO: Crawled 4 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/thewallstreet/comments/b3awq7/daily_discussion_march_20/'," datetime.datetime(2019, 3, 26, 19, 38, 41, 452587)",'2019-03-26',u'Daily',u'12','thewallstreet',u'Daily Discussion - (March 20)'," u'https://old.reddit.com/r/thewallstreet/comments/b3awq7/daily_discussion_march_20/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b5pd2v/how_easy_is_it_to_convert_from_a_403b_to_a_547b/'," datetime.datetime(2019, 3, 26, 19, 45, 29, 989174)",'2019-03-26',None,u'0','investing',u'How easy is it to convert from a 403b to a 547b ?'," u'https://old.reddit.com/r/investing/comments/b5pd2v/how_easy_is_it_to_convert_from_a_403b_to_a_547b/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b5p9et/trade_american_options_from_europe/'," datetime.datetime(2019, 3, 26, 19, 48, 34, 670130)",'2019-03-26',None,u'0','investing',u'Trade american options from Europe'," u'https://old.reddit.com/r/investing/comments/b5p9et/trade_american_options_from_europe/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b5patb/etrade_cash_purchasing_power_new_to_etrade/'," datetime.datetime(2019, 3, 26, 19, 49, 32, 412161)",'2019-03-26',None,u'0','investing',u'Etrade Cash Purchasing Power (New to Etrade)'," u'https://old.reddit.com/r/investing/comments/b5patb/etrade_cash_purchasing_power_new_to_etrade/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b5qpl7/roth_ira_for_child/'," datetime.datetime(2019, 3, 26, 19, 55, 18, 725982)",'2019-03-26',None,u'0','investing',u'Roth IRA for child'," u'https://old.reddit.com/r/investing/comments/b5qpl7/roth_ira_for_child/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b5pkxf/looking_for_a_guide_outlining_the_approach_to/'," datetime.datetime(2019, 3, 26, 19, 57, 28, 42860)",'2019-03-26',None,u'0','investing',u'Looking for a guide outlining the approach to take when finding potential Acquirers for a company.'," u'https://old.reddit.com/r/investing/comments/b5pkxf/looking_for_a_guide_outlining_the_approach_to/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b5qj9z/how_can_i_better_analyze_the_stocks_i_own/'," datetime.datetime(2019, 3, 26, 19, 59, 51, 436830)",'2019-03-26',None,u'0','investing',u'How can I better analyze the stocks I own?'," u'https://old.reddit.com/r/investing/comments/b5qj9z/how_can_i_better_analyze_the_stocks_i_own/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b5qfea/goldman_sachs_or_mastercard_a_good_buy_with_apple/'," datetime.datetime(2019, 3, 26, 20, 4, 24, 120290)",'2019-03-26',None,u'0','investing',u'Goldman Sachs or MasterCard a good buy with Apple Credit?'," u'https://old.reddit.com/r/investing/comments/b5qfea/goldman_sachs_or_mastercard_a_good_buy_with_apple/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b5plrq/china_plans_record_us_pork_imports_to_resolve/'," datetime.datetime(2019, 3, 26, 20, 8, 26, 624954)",'2019-03-26',u'News',u'11','investing',u'China Plans Record U.S. Pork Imports to Resolve Trade War - Bloomberg'," u'https://old.reddit.com/r/investing/comments/b5plrq/china_plans_record_us_pork_imports_to_resolve/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b5ovwa/rstocks_daily_discussion_tuesday_mar_26_2019/'," datetime.datetime(2019, 3, 26, 20, 13, 54, 57912)",'2019-03-26',None,u'2','stocks',"u'r/Stocks Daily Discussion Tuesday - Mar 26, 2019'"," u'https://old.reddit.com/r/stocks/comments/b5ovwa/rstocks_daily_discussion_tuesday_mar_26_2019/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b5quq1/my_first_truly_successful_option/'," datetime.datetime(2019, 3, 26, 20, 18, 41, 754881)",'2019-03-26',u'Gain',u'199','wallstreetbets',u'My first truly successful option'," u'https://old.reddit.com/r/wallstreetbets/comments/b5quq1/my_first_truly_successful_option/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b5ozzl/cronos_group_earnings/'," datetime.datetime(2019, 3, 26, 20, 39, 55, 152009)",'2019-03-26',None,u'0','stocks',u'Cronos Group Earnings'," u'https://old.reddit.com/r/stocks/comments/b5ozzl/cronos_group_earnings/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b5b0j0/uber_to_seal_31_billion_deal_to_buy_careem_this/'," datetime.datetime(2019, 3, 26, 21, 1, 46, 404791)",'2019-03-26',None,u'61','StockMarket',u'Uber to Seal $3.1 Billion Deal to Buy Careem This Week'," u'https://old.reddit.com/r/StockMarket/comments/b5b0j0/uber_to_seal_31_billion_deal_to_buy_careem_this/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-26 21:02:16 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b5wism/short_ea/'," datetime.datetime(2019, 3, 26, 21, 25, 2, 435956)",'2019-03-26',None,u'60','wallstreetbets',u'Short EA'," u'https://old.reddit.com/r/wallstreetbets/comments/b5wism/short_ea/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
2019-03-26 21:25:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b5qtyf/why_does_brkb_get_no_love/'," datetime.datetime(2019, 3, 26, 21, 35, 56, 30292)",'2019-03-26',None,u'0','investing',u'Why does BRKB get no love?'," u'https://old.reddit.com/r/investing/comments/b5qtyf/why_does_brkb_get_no_love/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b5r4dv/tech_companies_havent_been_this_negative_about_a/'," datetime.datetime(2019, 3, 26, 21, 54, 59, 853922)",'2019-03-26',None,u'107','investing',u'Tech companies haven\u2019t been this negative about a quarter in six years'," u'https://old.reddit.com/r/investing/comments/b5r4dv/tech_companies_havent_been_this_negative_about_a/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b5rtp2/the_stock_market_awakens_a_dangerous_emotion_fear/'," datetime.datetime(2019, 3, 26, 22, 47, 2, 334176)",'2019-03-26',None,u'0','investing',u'The stock market awakens a dangerous emotion: fear.'," u'https://old.reddit.com/r/investing/comments/b5rtp2/the_stock_market_awakens_a_dangerous_emotion_fear/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Daytrading/comments/b3xla6/can_someone_explain_how_a_person_can_get_access/'," datetime.datetime(2019, 3, 26, 22, 47, 51, 766029)",'2019-03-26',None,u'3','Daytrading',u'Can someone explain how a person can get access to trade after hours?'," u'https://old.reddit.com/r/Daytrading/comments/b3xla6/can_someone_explain_how_a_person_can_get_access/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-26 22:48:22 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Daytrading/comments/b3v2v4/funding_cmeg_with_bank_of_america/'," datetime.datetime(2019, 3, 26, 22, 48, 22, 87607)",'2019-03-26',None,u'1','Daytrading',u'Funding CMEG with Bank of America'," u'https://old.reddit.com/r/Daytrading/comments/b3v2v4/funding_cmeg_with_bank_of_america/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.core.engine:Closing spider (finished)
2019-03-26 22:48:52 [scrapy.core.engine] INFO: Closing spider (finished)
INFO:scrapy.statscollectors:Dumping Scrapy stats:
"
https://old.reddit.com/r/wallstreetbets/comments/b5remv/currently_filing_2018_tax_return_because_of_the/'," datetime.datetime(2019, 3, 26, 22, 58, 45, 930737)",'2019-03-26',u'Loss',u'3068','wallstreetbets',"u""Currently filing 2018 tax return. Because of the capital net loss of -$13345 in 2018, I won't have to pay any additional federal taxes."""," u'https://old.reddit.com/r/wallstreetbets/comments/b5remv/currently_filing_2018_tax_return_because_of_the/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b5pcf9/eqxff_vs_eqxwf_is_there_a_difference/'," datetime.datetime(2019, 3, 26, 23, 16, 30, 79122)",'2019-03-26',None,u'0','stocks',u'EQXFF vs EQXWF .. is there a difference?'," u'https://old.reddit.com/r/stocks/comments/b5pcf9/eqxff_vs_eqxwf_is_there_a_difference/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b5rw03/european_investors_etf_setup/'," datetime.datetime(2019, 3, 26, 23, 42, 37, 869521)",'2019-03-26',u'Discussion',u'18','investing',"u""European investor's ETF setup"""," u'https://old.reddit.com/r/investing/comments/b5rw03/european_investors_etf_setup/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b5cy28/how_do_i_short_ukbritish_stocks/'," datetime.datetime(2019, 3, 26, 23, 51, 22, 356474)",'2019-03-26',None,u'3','StockMarket',u'How do I short UK/British stocks?'," u'https://old.reddit.com/r/StockMarket/comments/b5cy28/how_do_i_short_ukbritish_stocks/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.core.engine:Closing spider (finished)
2019-03-26 23:51:52 [scrapy.core.engine] INFO: Closing spider (finished)
INFO:scrapy.statscollectors:Dumping Scrapy stats:
"
https://old.reddit.com/r/stocks/comments/b5saqb/amda_fda_approval_my_dd_whats_your_input_and_am_i/'," datetime.datetime(2019, 3, 26, 23, 52, 51, 643396)",'2019-03-26',u'Discussion',u'1','stocks',"u""AMDA Fda approval. My DD what's your input and am I missing something?"""," u'https://old.reddit.com/r/stocks/comments/b5saqb/amda_fda_approval_my_dd_whats_your_input_and_am_i/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b5s23o/wsb_is_down_prepare_for_the_hordes/'," datetime.datetime(2019, 3, 27, 0, 2, 5, 38447)",'2019-03-27',None,u'0','investing',u'WSB is down. Prepare for the hordes'," u'https://old.reddit.com/r/investing/comments/b5s23o/wsb_is_down_prepare_for_the_hordes/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/thewallstreet/comments/b3gnoz/post_market_discussion_march_20/'," datetime.datetime(2019, 3, 27, 0, 24, 18, 655074)",'2019-03-27',u'Daily',u'7','thewallstreet',u'Post Market Discussion - (March 20)'," u'https://old.reddit.com/r/thewallstreet/comments/b3gnoz/post_market_discussion_march_20/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Daytrading/comments/b41qlm/whole_slew_of_esminitradingview_questions/'," datetime.datetime(2019, 3, 27, 0, 27, 20, 555200)",'2019-03-27',None,u'1','Daytrading',u'Whole slew of ES-mini/tradingview questions'," u'https://old.reddit.com/r/Daytrading/comments/b41qlm/whole_slew_of_esminitradingview_questions/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b5qa06/how_do_you_keep_up_with_news/'," datetime.datetime(2019, 3, 27, 1, 2, 26, 563158)",'2019-03-27',u'Question',u'9','stocks',u'How do you keep up with news?'," u'https://old.reddit.com/r/stocks/comments/b5qa06/how_do_you_keep_up_with_news/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b5d0xl/tesladespite_the_downgrades_is_it_a_good_buy/'," datetime.datetime(2019, 3, 27, 1, 39, 51, 348655)",'2019-03-27',None,u'78','StockMarket',"u'Tesla--despite the downgrades, is it a good buy?'"," u'https://old.reddit.com/r/StockMarket/comments/b5d0xl/tesladespite_the_downgrades_is_it_a_good_buy/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b5sfa2/anyone_know_why_abmd_is_dipping/'," datetime.datetime(2019, 3, 27, 1, 42, 42, 453548)",'2019-03-27',None,u'1','investing',u'Anyone know why ABMD is dipping?'," u'https://old.reddit.com/r/investing/comments/b5sfa2/anyone_know_why_abmd_is_dipping/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b5qcwa/what_inverted_yield_curve_global_stocks_rising/'," datetime.datetime(2019, 3, 27, 1, 57, 1, 445682)",'2019-03-27',u'News',u'33','stocks',u'What inverted yield curve? Global stocks rising today as Wall Street plays down recession fears'," u'https://old.reddit.com/r/stocks/comments/b5qcwa/what_inverted_yield_curve_global_stocks_rising/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-27 01:57:31 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b5qii7/thoughts_on_bayer_ag_bayrypnk_stock/'," datetime.datetime(2019, 3, 27, 2, 7, 29, 110084)",'2019-03-27',None,u'2','stocks',u'Thoughts on BAYER AG (BAYRY:PNK ) stock?'," u'https://old.reddit.com/r/stocks/comments/b5qii7/thoughts_on_bayer_ag_bayrypnk_stock/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b5dje1/a_starting_point/'," datetime.datetime(2019, 3, 27, 2, 14, 53, 477483)",'2019-03-27',None,u'0','StockMarket',u'A starting point.'," u'https://old.reddit.com/r/StockMarket/comments/b5dje1/a_starting_point/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b5sntr/verizon_refuses_to_admit_that_its_first_to_5g/'," datetime.datetime(2019, 3, 27, 2, 48, 50, 41490)",'2019-03-27',u'Discussion',u'19','wallstreetbets',u'Verizon refuses to admit that its \u201cfirst to 5G\u201d commercials are misleading - Any insights on 5G?'," u'https://old.reddit.com/r/wallstreetbets/comments/b5sntr/verizon_refuses_to_admit_that_its_first_to_5g/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b5qp1k/crmd_buy_1_get_4_free/'," datetime.datetime(2019, 3, 27, 3, 2, 25, 466168)",'2019-03-27',None,u'1','stocks',u'CRMD: Buy 1 get 4 free'," u'https://old.reddit.com/r/stocks/comments/b5qp1k/crmd_buy_1_get_4_free/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b5sunm/thanks_spy_you_sexy_son_of_a_bitch/'," datetime.datetime(2019, 3, 27, 3, 8, 42, 566245)",'2019-03-27',u'YOLO',u'167','wallstreetbets',u'Thanks SPY you sexy son of a bitch'," u'https://imgur.com/cBq1SLt'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 9 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-27 03:09:12 [scrapy.extensions.logstats] INFO: Crawled 9 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b5qr0s/news_on_nike_stock/'," datetime.datetime(2019, 3, 27, 3, 34, 37, 711715)",'2019-03-27',None,u'1','stocks',u'News on Nike stock'," u'https://old.reddit.com/r/stocks/comments/b5qr0s/news_on_nike_stock/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 8 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-27 03:35:08 [scrapy.extensions.logstats] INFO: Crawled 8 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b5svw8/touched_4800_today_with_spy_284_calls_and_now_im/'," datetime.datetime(2019, 3, 27, 4, 52, 46, 832471)",'2019-03-27',u'YOLO',u'12','wallstreetbets',"u""Touched 4800 today with SPY 284 calls and now I'm here somehow"""," u'https://old.reddit.com/r/wallstreetbets/comments/b5svw8/touched_4800_today_with_spy_284_calls_and_now_im/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/thewallstreet/comments/b3i1jk/nightly_trading_discussion_march_2021/'," datetime.datetime(2019, 3, 27, 5, 13, 49, 647795)",'2019-03-27',u'Daily',u'11','thewallstreet',u'Nightly Trading Discussion - (March 20/21)'," u'https://old.reddit.com/r/thewallstreet/comments/b3i1jk/nightly_trading_discussion_march_2021/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b5sg9r/timing_market_based_on_interest_rates_and/'," datetime.datetime(2019, 3, 27, 5, 30, 15, 242101)",'2019-03-27',None,u'0','investing',u'Timing market based on interest rates and inflation?'," u'https://old.reddit.com/r/investing/comments/b5sg9r/timing_market_based_on_interest_rates_and/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b5qxsu/thinking_of_selling_brkb_and_buying_into_nvidia/'," datetime.datetime(2019, 3, 27, 5, 38, 26, 858827)",'2019-03-27',None,u'6','stocks',u'Thinking of selling BrkB and buying into NVIDIA...'," u'https://old.reddit.com/r/stocks/comments/b5qxsu/thinking_of_selling_brkb_and_buying_into_nvidia/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-27 05:38:57 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b5si4w/nvda_is_it_time_to_buy/'," datetime.datetime(2019, 3, 27, 5, 52, 12, 140876)",'2019-03-27',None,u'0','investing',u'$NVDA Is it time to buy'," u'https://old.reddit.com/r/investing/comments/b5si4w/nvda_is_it_time_to_buy/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b5skjl/hd_stocks_should_rally/'," datetime.datetime(2019, 3, 27, 6, 23, 43, 68829)",'2019-03-27',None,u'0','investing',u'$HD Stocks should rally'," u'https://old.reddit.com/r/investing/comments/b5skjl/hd_stocks_should_rally/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b5r1pg/settlement_risk_in_otc_stocks_can_someone_explain/'," datetime.datetime(2019, 3, 27, 6, 36, 41, 828259)",'2019-03-27',None,u'1','stocks',u'Settlement Risk in OTC stocks - can someone explain?'," u'https://old.reddit.com/r/stocks/comments/b5r1pg/settlement_risk_in_otc_stocks_can_someone_explain/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b5t071/have_more_sp_500_companies_issued_negative_eps/'," datetime.datetime(2019, 3, 27, 6, 59, 8, 756542)",'2019-03-27',None,u'50','investing',u'Have More S&P 500 Companies Issued Negative EPS Guidance For Q1 Than Average?'," u'https://old.reddit.com/r/investing/comments/b5t071/have_more_sp_500_companies_issued_negative_eps/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b61tlf/those_looking_into_investing_in_new_zealand_share/'," datetime.datetime(2019, 3, 27, 7, 4, 38, 990153)",'2019-03-27',None,u'8','investing',u'Those looking into investing in New Zealand Share market(NZX) - I have analysed the companies with wide economic moats to help you on your journey.'," u'https://old.reddit.com/r/investing/comments/b61tlf/those_looking_into_investing_in_new_zealand_share/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b5t50i/effects_of_article_13_on_future_earnings_of/'," datetime.datetime(2019, 3, 27, 7, 9, 54, 314707)",'2019-03-27',u'Discussion',u'8','wallstreetbets',"u'Effects of Article 13 on future earnings of Facebook, Google etc..'"," u'https://old.reddit.com/r/wallstreetbets/comments/b5t50i/effects_of_article_13_on_future_earnings_of/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-27 07:10:24 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b5t34o/its_gabes_fault/'," datetime.datetime(2019, 3, 27, 7, 14, 57, 429703)",'2019-03-27',u'Shitpost',u'44','wallstreetbets',"u""Its Gabe's fault"""," u'https://old.reddit.com/r/wallstreetbets/comments/b5t34o/its_gabes_fault/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-27 07:15:27 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b5t5ih/condor_gang_caw_caw_caw/'," datetime.datetime(2019, 3, 27, 8, 3, 50, 638576)",'2019-03-27',u'Shitpost',u'81','wallstreetbets',u'CONDOR GANG CAW CAW CAW'," u'https://old.reddit.com/r/wallstreetbets/comments/b5t5ih/condor_gang_caw_caw_caw/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Daytrading/comments/b422xe/is_this_worth_getting_a_subscription/'," datetime.datetime(2019, 3, 27, 8, 7, 0, 18880)",'2019-03-27',None,u'2','Daytrading',u'Is this worth getting a subscription?'," u'https://old.reddit.com/r/Daytrading/comments/b422xe/is_this_worth_getting_a_subscription/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-27 08:07:30 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
INFO:scrapy.core.engine:Closing spider (finished)
2019-03-27 08:07:30 [scrapy.core.engine] INFO: Closing spider (finished)
INFO:scrapy.statscollectors:Dumping Scrapy stats:
"
https://old.reddit.com/r/wallstreetbets/comments/b5yc7q/bigly_bad_news_education_secretary_betsy_devos/'," datetime.datetime(2019, 3, 27, 8, 15, 5, 379872)",'2019-03-27',u'Shitpost',u'133','wallstreetbets',u'Bigly bad news! Education Secretary Betsy DeVos proposes steep cuts to Special Olympics and autism programs'," u'https://twitter.com/i/moments/1110654109416226816'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Daytrading/new/> from <GET https://old.reddit.com/r/daytrading/new/>
2019-03-27 08:15:35 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Daytrading/new/> from <GET https://old.reddit.com/r/daytrading/new/>
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b61p13/research_survey_wsb_tell_us_your_return/'," datetime.datetime(2019, 3, 27, 9, 5, 32, 423850)",'2019-03-27',None,u'0','wallstreetbets',"u'(Research Survey) WSB, tell us your return expectations!'"," u'https://old.reddit.com/r/wallstreetbets/comments/b61p13/research_survey_wsb_tell_us_your_return/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 2 pages (at 2 pages/min), scraped 0 items (at 0 items/min)
2019-03-27 09:06:02 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 2 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b5rg02/ladybaybee_picks_for_20190326/'," datetime.datetime(2019, 3, 27, 9, 5, 32, 934484)",'2019-03-27',None,u'3','stocks',u'Ladybaybee picks for 2019-03-26'," u'https://old.reddit.com/r/stocks/comments/b5rg02/ladybaybee_picks_for_20190326/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b5s384/banks_getting_hit/'," datetime.datetime(2019, 3, 27, 9, 20, 59, 666547)",'2019-03-27',None,u'0','stocks',u'Banks getting hit'," u'https://old.reddit.com/r/stocks/comments/b5s384/banks_getting_hit/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/thewallstreet/comments/b3kgih/deviations_poc_and_value_area_for_thursday_march/'," datetime.datetime(2019, 3, 27, 9, 23, 26, 410548)",'2019-03-27',None,u'20','thewallstreet',"u'Deviations, POC, and Value Area for Thursday, March 21, 2019'"," u'https://i.imgur.com/u2OwN4E.png'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b5s801/cnbc_jim_cramer/'," datetime.datetime(2019, 3, 27, 9, 26, 24, 198279)",'2019-03-27',None,u'0','stocks',u'CNBC Jim Cramer'," u'https://old.reddit.com/r/stocks/comments/b5s801/cnbc_jim_cramer/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b5exor/3_month_treasury_244_vs_5_year_treasury_22/'," datetime.datetime(2019, 3, 27, 9, 35, 22, 762599)",'2019-03-27',None,u'4','StockMarket',u'3 Month treasury (2.44%) vs 5 year treasury (2.2%)'," u'https://old.reddit.com/r/StockMarket/comments/b5exor/3_month_treasury_244_vs_5_year_treasury_22/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b5saix/rstocks_technicals_tuesday_mar_26_2019/'," datetime.datetime(2019, 3, 27, 9, 43, 8, 507858)",'2019-03-27',None,u'1','stocks',"u'r/Stocks Technicals Tuesday - Mar 26, 2019'"," u'https://old.reddit.com/r/stocks/comments/b5saix/rstocks_technicals_tuesday_mar_26_2019/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b5s8mw/apples_move_into_banking_raises_the_bar_for/'," datetime.datetime(2019, 3, 27, 9, 47, 41, 224191)",'2019-03-27',None,u'13','stocks',"u""Apple's move into banking raises the bar for fintech, traditional credit cards"""," u'https://old.reddit.com/r/stocks/comments/b5s8mw/apples_move_into_banking_raises_the_bar_for/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b5u0uc/even_daddy_trump_couldnt_save_me/'," datetime.datetime(2019, 3, 27, 9, 58, 27, 356727)",'2019-03-27',u'Loss',u'76','wallstreetbets',"u""Even daddy Trump couldn't save me"""," u'https://old.reddit.com/r/wallstreetbets/comments/b5u0uc/even_daddy_trump_couldnt_save_me/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b5truv/i_hope_youre_right_fortune_cookie/'," datetime.datetime(2019, 3, 27, 9, 58, 34, 313599)",'2019-03-27',u'Shitpost',u'283','wallstreetbets',u'I hope you\u2019re right fortune cookie'," u'https://old.reddit.com/r/wallstreetbets/comments/b5truv/i_hope_youre_right_fortune_cookie/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b5tazo/survey_which_company_would_you_invest_in/'," datetime.datetime(2019, 3, 27, 9, 58, 40, 242314)",'2019-03-27',None,u'0','investing',u'[SURVEY] Which company would you invest in?'," u'https://old.reddit.com/r/investing/comments/b5tazo/survey_which_company_would_you_invest_in/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b63k82/health_insurer_centene_to_buy_smaller_rival/'," datetime.datetime(2019, 3, 27, 10, 1, 33, 616577)",'2019-03-27',None,u'1','stocks',u'Health insurer Centene to buy smaller rival WellCare for $15.27 billion'," u'https://old.reddit.com/r/stocks/comments/b63k82/health_insurer_centene_to_buy_smaller_rival/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b5shsn/r_housing_stocks_over_bought/'," datetime.datetime(2019, 3, 27, 10, 2, 41, 710008)",'2019-03-27',None,u'0','stocks',u'R Housing stocks over bought'," u'https://old.reddit.com/r/stocks/comments/b5shsn/r_housing_stocks_over_bought/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b5g3dd/are_total_sa_stocks_worth_buying_right_now/'," datetime.datetime(2019, 3, 27, 10, 2, 46, 755868)",'2019-03-27',None,u'1','StockMarket',u'Are total SA stocks worth buying right now ?'," u'https://old.reddit.com/r/StockMarket/comments/b5g3dd/are_total_sa_stocks_worth_buying_right_now/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b5u0ya/the_parable_of_the_capital_loss/'," datetime.datetime(2019, 3, 27, 10, 3, 24, 45099)",'2019-03-27',u'Meme',u'18','wallstreetbets',u'The Parable of the Capital Loss'," u'https://old.reddit.com/r/wallstreetbets/comments/b5u0ya/the_parable_of_the_capital_loss/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b5u0bf/apple_violated_qualcomm_patent_rules_us_trade/'," datetime.datetime(2019, 3, 27, 10, 3, 30, 444230)",'2019-03-27',u'Discussion',u'41','wallstreetbets',"u'Apple violated Qualcomm patent, rules U.S. trade judge, who recommends a limited ban on imports of iPhones'"," u'https://www.wsj.com/articles/apple-violated-qualcomm-patent-u-s-trade-judge-rules-11553624866'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b5tiot/thanks_tim_apple_for_allowing_me_to_pay_off_my/'," datetime.datetime(2019, 3, 27, 10, 3, 37, 501604)",'2019-03-27',u'Gain',u'43','wallstreetbets',u'Thanks Tim Apple for allowing me to pay off my WSB Options Tuition today'," u'https://imgur.com/a/b4PgTt0'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b5tfku/elon_musk_but_with_elizabeth_holmes_eyes/'," datetime.datetime(2019, 3, 27, 10, 3, 40, 161747)",'2019-03-27',u'Shitpost',u'1530','wallstreetbets',"u""Elon Musk but with Elizabeth Holmes' eyes"""," u'https://i0.wp.com/media.boingboing.net/wp-content/uploads/2019/03/therelon2.jpg?resize=682%2C650&ssl=1'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b5t9pf/vix/'," datetime.datetime(2019, 3, 27, 10, 13, 14, 407425)",'2019-03-27',None,u'1','stocks',u'VIX'," u'https://old.reddit.com/r/stocks/comments/b5t9pf/vix/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b5u6y0/might_as_well_hold/'," datetime.datetime(2019, 3, 27, 10, 13, 24, 23086)",'2019-03-27',u'Loss',u'65','wallstreetbets',u'Might as well hold....'," u'https://old.reddit.com/r/wallstreetbets/comments/b5u6y0/might_as_well_hold/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b5tbar/time_to_go_all_in_on_spy_puts/'," datetime.datetime(2019, 3, 27, 10, 13, 44, 794397)",'2019-03-27',u'Technicals',u'22','wallstreetbets',u'Time to go all in on Spy Puts'," u'http://imgur.com/v9bgeY3'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b5grvw/tier_reit_and_cousins_properties_sign_merger/'," datetime.datetime(2019, 3, 27, 10, 17, 21, 77159)",'2019-03-27',None,u'1','StockMarket',u'TIER REIT and Cousins Properties sign merger agreement'," u'https://old.reddit.com/r/StockMarket/comments/b5grvw/tier_reit_and_cousins_properties_sign_merger/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b5ulmx/long_time_lurker_first_time_poster_feeling/'," datetime.datetime(2019, 3, 27, 10, 18, 20, 827760)",'2019-03-27',u'Storytime',u'16','wallstreetbets',"u'Long time lurker, first time poster. Feeling discouraged after a few weeks of ups and downs and breaking even'"," u'https://old.reddit.com/r/wallstreetbets/comments/b5ulmx/long_time_lurker_first_time_poster_feeling/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b5i0w8/just_a_question_should_we_be_expecting_a/'," datetime.datetime(2019, 3, 27, 10, 25, 35, 800872)",'2019-03-27',None,u'0','StockMarket',u'Just a question: should we be expecting a recession soon? Is it time to sell?'," u'https://old.reddit.com/r/StockMarket/comments/b5i0w8/just_a_question_should_we_be_expecting_a/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-27 10:26:06 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Trading/comments/aya1py/best_trading_apps/'," datetime.datetime(2019, 3, 27, 10, 45, 21, 962524)",'2019-03-27',None,u'0','Trading',u'Best trading apps?'," u'https://old.reddit.com/r/Trading/comments/aya1py/best_trading_apps/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b5i78p/well_nuts_i_lost_a_bitcoin_today/'," datetime.datetime(2019, 3, 27, 11, 0, 47, 455613)",'2019-03-27',None,u'1','StockMarket',u'Well.... nuts I lost a bitcoin today.'," u'https://old.reddit.com/r/StockMarket/comments/b5i78p/well_nuts_i_lost_a_bitcoin_today/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b5tkpm/hows_the_economy_doing/'," datetime.datetime(2019, 3, 27, 11, 3, 41, 348353)",'2019-03-27',None,u'0','stocks',"u""How's the economy doing?"""," u'https://old.reddit.com/r/stocks/comments/b5tkpm/hows_the_economy_doing/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Trading/comments/ayey9g/british_traders_what_is_the_cheapest_way_to_trade/'," datetime.datetime(2019, 3, 27, 11, 11, 16, 42218)",'2019-03-27',None,u'1','Trading',"u'British traders, what is the cheapest way to trade?'"," u'https://old.reddit.com/r/Trading/comments/ayey9g/british_traders_what_is_the_cheapest_way_to_trade/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/ValueInvesting/comments/ax2zfn/40c_to_a_dollar/'," datetime.datetime(2019, 3, 27, 11, 18, 46, 186519)",'2019-03-27',None,u'0','ValueInvesting',u'40c to a dollar'," u'https://old.reddit.com/r/ValueInvesting/comments/ax2zfn/40c_to_a_dollar/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b5j3of/im_completely_new_to_the_stock_market_and_ive_got/'," datetime.datetime(2019, 3, 27, 11, 45, 46, 820730)",'2019-03-27',None,u'1','StockMarket',"u""I'm completely new to the stock market and I've got 2k to spend. Where do I start and what do I do?"""," u'https://old.reddit.com/r/StockMarket/comments/b5j3of/im_completely_new_to_the_stock_market_and_ive_got/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Stock_Picks/comments/avhrnd/tiktok_fined_57_million_by_ftc_over_alleged/'," datetime.datetime(2019, 3, 27, 11, 46, 50, 30370)",'2019-03-27',None,u'59','Stock_Picks',u'TikTok fined $5.7 million by FTC over alleged children\u2019s privacy law violations'," u'https://www.theverge.com/2019/2/27/18243312/tiktok-ftc-fine-musically-children-coppa-age-gate'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b5jkcp/a_recession_occurs_on_average_16_months_after_the/'," datetime.datetime(2019, 3, 27, 11, 47, 12, 55312)",'2019-03-27',None,u'336','StockMarket',u'A recession occurs on average 16 months after the 10 year yield falls below the 3 month yield.'," u'https://old.reddit.com/r/StockMarket/comments/b5jkcp/a_recession_occurs_on_average_16_months_after_the/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b5ua16/thoughts_on_amazon_stock/'," datetime.datetime(2019, 3, 27, 11, 55, 11, 963385)",'2019-03-27',None,u'2','investing',u'thoughts on amazon stock?'," u'https://old.reddit.com/r/investing/comments/b5ua16/thoughts_on_amazon_stock/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b5u3u2/arcelormittal_pe_ratio_of_4_rapidly_reducing_debt/'," datetime.datetime(2019, 3, 27, 11, 59, 14, 870050)",'2019-03-27',None,u'9','investing',u'ArcelorMittal PE ratio of 4 rapidly reducing debt and rising demand for steel. Looks super cheap or am i missing something?'," u'https://old.reddit.com/r/investing/comments/b5u3u2/arcelormittal_pe_ratio_of_4_rapidly_reducing_debt/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b5uifv/socially_conscious_companies_wealthfront_fidelity/'," datetime.datetime(2019, 3, 27, 12, 16, 37, 290255)",'2019-03-27',None,u'0','investing',"u'Socially conscious companies: Wealthfront, Fidelity, who should I support?'"," u'https://old.reddit.com/r/investing/comments/b5uifv/socially_conscious_companies_wealthfront_fidelity/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b64qpg/will_the_pound_crash_after_brexit/'," datetime.datetime(2019, 3, 27, 12, 20, 32, 155008)",'2019-03-27',None,u'0','wallstreetbets',u'Will the Pound crash after Brexit?'," u'https://youtu.be/LW1tZ9mxP4w'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 2 pages (at 2 pages/min), scraped 0 items (at 0 items/min)
2019-03-27 12:21:02 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 2 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b64qgo/2500_mac_plus_or_aapl_stock_911989/'," datetime.datetime(2019, 3, 27, 12, 21, 2, 436774)",'2019-03-27',None,u'3','wallstreetbets',u'$2500 Mac Plus or AAPL stock 9/1/1989?'," u'https://old.reddit.com/r/wallstreetbets/comments/b64qgo/2500_mac_plus_or_aapl_stock_911989/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
2019-03-27 12:21:32 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b64j5o/mfw/'," datetime.datetime(2019, 3, 27, 12, 22, 3, 98992)",'2019-03-27',u'Satire',u'66','wallstreetbets',u'MFW...'," u'https://old.reddit.com/r/wallstreetbets/comments/b64j5o/mfw/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 2 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-27 12:22:33 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b5tmao/nio_stock_less_than_5/'," datetime.datetime(2019, 3, 27, 12, 22, 53, 270551)",'2019-03-27',None,u'8','stocks',u'NIO stock less than $5!!'," u'https://old.reddit.com/r/stocks/comments/b5tmao/nio_stock_less_than_5/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b64vdt/robinhood_holders_of_tsla_hitting_another_record/'," datetime.datetime(2019, 3, 27, 12, 25, 32, 975818)",'2019-03-27',u'Stocks',u'7','wallstreetbets',"u'Robinhood holders of $TSLA hitting another record, 128,832, as of 11:53am PT yesterday.'"," u'https://twitter.com/ValueExpected/status/1110615711452323840'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 2 pages (at 2 pages/min), scraped 0 items (at 0 items/min)
2019-03-27 12:26:03 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 2 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b64bjz/looks_like_they_will_solve_the_spiking_aircraft/'," datetime.datetime(2019, 3, 27, 12, 25, 39, 427048)",'2019-03-27',u'Discussion',u'1','wallstreetbets',u'Looks like they will solve the spiking aircraft problem soon. Calls.'," u'https://www.youtube.com/watch?v=x-Bj1sMLmlE'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/Trading/new/> (referer: None)
2019-03-27 12:26:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/Trading/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b641fb/psiq_huge_news_psiq_to_receive_assignment_of/'," datetime.datetime(2019, 3, 27, 12, 28, 52, 359100)",'2019-03-27',None,u'8','StockMarket',"u'$PSIQ Huge News: ""PSIQ to Receive Assignment of License to Prepare Land in Israel With the Intent to Grow, Cultivate, Distribute and Export Medical Cannabis""'"," u'https://old.reddit.com/r/StockMarket/comments/b641fb/psiq_huge_news_psiq_to_receive_assignment_of/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Daytrading/comments/b645c4/27_mar_open_watchlist/'," datetime.datetime(2019, 3, 27, 12, 29, 22, 820930)",'2019-03-27',None,u'1','Daytrading',u'27 Mar Open Watchlist'," u'https://old.reddit.com/r/Daytrading/comments/b645c4/27_mar_open_watchlist/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b5twrp/who_here_has_crazy_roi_percentages_that_would/'," datetime.datetime(2019, 3, 27, 12, 37, 25, 895145)",'2019-03-27',None,u'1','stocks',u'Who here has crazy ROI percentages that would like to share their strategy?'," u'https://old.reddit.com/r/stocks/comments/b5twrp/who_here_has_crazy_roi_percentages_that_would/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b5uj5z/4_red_flags_tesla_investors_cannot_ignore_yahoo/'," datetime.datetime(2019, 3, 27, 12, 39, 4, 138798)",'2019-03-27',None,u'0','investing',u'4 Red Flags Tesla Investors Cannot Ignore - Yahoo Finance (summarizing a presentation given at Wharton a few days ago)'," u'https://old.reddit.com/r/investing/comments/b5uj5z/4_red_flags_tesla_investors_cannot_ignore_yahoo/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b5vdhj/asset_allocation_question/'," datetime.datetime(2019, 3, 27, 12, 45, 31, 344266)",'2019-03-27',None,u'1','investing',u'asset allocation question.'," u'https://old.reddit.com/r/investing/comments/b5vdhj/asset_allocation_question/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b5uz9s/southwest_boeing_737_max_reportedly_made_an/'," datetime.datetime(2019, 3, 27, 12, 55, 17, 901211)",'2019-03-27',None,u'354','stocks',u'Southwest Boeing 737 Max reportedly made an emergency landing at Orlando International Airport'," u'https://old.reddit.com/r/stocks/comments/b5uz9s/southwest_boeing_737_max_reportedly_made_an/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b5u8en/what_are_your_favorite_youtube_channels_centered/'," datetime.datetime(2019, 3, 27, 12, 59, 51, 117065)",'2019-03-27',None,u'27','stocks',u'What are your favorite Youtube channels centered around investing and stocks?'," u'https://old.reddit.com/r/stocks/comments/b5u8en/what_are_your_favorite_youtube_channels_centered/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b5w1oh/are_forex_groups_legit/'," datetime.datetime(2019, 3, 27, 13, 2, 0, 442801)",'2019-03-27',None,u'0','investing',u'Are forex groups legit?'," u'https://old.reddit.com/r/investing/comments/b5w1oh/are_forex_groups_legit/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b5w986/reuters_china_business_activity_recovering_thanks/'," datetime.datetime(2019, 3, 27, 13, 7, 56, 581419)",'2019-03-27',None,u'2','investing',"u""Reuters: China business activity recovering thanks to 'credit-soaked' quarter: Beige Book"""," u'https://old.reddit.com/r/investing/comments/b5w986/reuters_china_business_activity_recovering_thanks/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Daytrading/comments/b5pp9i/26_march_open_watchlist/'," datetime.datetime(2019, 3, 27, 13, 10, 21, 211169)",'2019-03-27',None,u'1','Daytrading',u'26 March Open Watchlist'," u'https://old.reddit.com/r/Daytrading/comments/b5pp9i/26_march_open_watchlist/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b5jv1m/looking_to_invest_in_a_mutual_fund/'," datetime.datetime(2019, 3, 27, 13, 11, 49, 550752)",'2019-03-27',None,u'2','StockMarket',u'Looking to invest in a mutual fund.'," u'https://old.reddit.com/r/StockMarket/comments/b5jv1m/looking_to_invest_in_a_mutual_fund/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b5vbch/mattel_releases_bts_dolls_and_a_strong_buy_rating/'," datetime.datetime(2019, 3, 27, 13, 15, 18, 242501)",'2019-03-27',None,u'3','stocks',u'Mattel Releases BTS Dolls and a Strong Buy Rating from a Stock Market Guru'," u'https://old.reddit.com/r/stocks/comments/b5vbch/mattel_releases_bts_dolls_and_a_strong_buy_rating/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b5wjpe/thoughts_on_vivendi_sa_stock_a_good_investment/'," datetime.datetime(2019, 3, 27, 13, 38, 20, 557169)",'2019-03-27',u'Discussion',u'1','stocks',"u'Thoughts on Vivendi SA stock, a good investment?'"," u'https://old.reddit.com/r/stocks/comments/b5wjpe/thoughts_on_vivendi_sa_stock_a_good_investment/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b5v9yi/what_are_your_moves_tomorrow_march_27/'," datetime.datetime(2019, 3, 27, 13, 38, 44, 500414)",'2019-03-27',u'Daily Discussion',u'51','wallstreetbets',"u'What Are Your Moves Tomorrow, March 27'"," u'https://old.reddit.com/r/wallstreetbets/comments/b5v9yi/what_are_your_moves_tomorrow_march_27/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b5yr1h/should_i_continue_building_a_diversified/'," datetime.datetime(2019, 3, 27, 13, 43, 18, 480418)",'2019-03-27',None,u'3','stocks',"u'Should I continue building a diversified portfolio or just by various Large, Mid, and Small Cap ETFs...'"," u'https://old.reddit.com/r/stocks/comments/b5yr1h/should_i_continue_building_a_diversified/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b5v37c/southwest_boeing_737_max_makes_emergency_landing/'," datetime.datetime(2019, 3, 27, 13, 43, 48, 8803)",'2019-03-27',u'Stocks',u'149','wallstreetbets',u'Southwest Boeing 737 Max makes emergency landing at Orlando International Airport'," u'http://www.fox35orlando.com/news/local-news/southwest-boeing-737-max-makes-emergency-landing-at-orlando-international-airport'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b5vq1c/if_you_could_invest_in_individual_people_who/'," datetime.datetime(2019, 3, 27, 13, 43, 48, 801721)",'2019-03-27',u'Discussion',u'41','wallstreetbets',"u'If you could invest in individual people, who would give you the biggest return?'"," u'https://old.reddit.com/r/wallstreetbets/comments/b5vq1c/if_you_could_invest_in_individual_people_who/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b5xt6l/thoughts_on_investing_in_pershing_square/'," datetime.datetime(2019, 3, 27, 13, 48, 21, 620410)",'2019-03-27',u'Advice',u'6','stocks',u'Thoughts on investing in Pershing Square'," u'https://old.reddit.com/r/stocks/comments/b5xt6l/thoughts_on_investing_in_pershing_square/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b5ww09/tvix_question/'," datetime.datetime(2019, 3, 27, 13, 52, 54, 841690)",'2019-03-27',None,u'6','stocks',u'$TVIX question'," u'https://old.reddit.com/r/stocks/comments/b5ww09/tvix_question/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b5xh2d/searched_google_for_best_stocks_2019_reddit_this/'," datetime.datetime(2019, 3, 27, 13, 53, 33, 161664)",'2019-03-27',u'Shitpost',u'152','wallstreetbets',"u'Searched Google For ""Best Stocks 2019 Reddit"" This Was The Top Google Link/3rd Highest Upvoted Comment. This Didn\'t Didn\'t Age Well.'"," u'https://old.reddit.com/r/wallstreetbets/comments/b5xh2d/searched_google_for_best_stocks_2019_reddit_this/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b5vv2s/after_sec_requests_ruling_judge_allison_nathan/'," datetime.datetime(2019, 3, 27, 13, 58, 36, 753705)",'2019-03-27',u'Fundamentals',u'43','wallstreetbets',"u'After SEC requests ruling, Judge Allison Nathan instead schedules oral arguments for April 4th (This is good for $TSLA)'"," u'https://pbs.twimg.com/media/D2nV-NbXcAAcVN3.jpg'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b5wb0o/the_purpose_of_bonds_in_a_portfolio/'," datetime.datetime(2019, 3, 27, 14, 8, 5, 182872)",'2019-03-27',None,u'0','investing',u'The purpose of Bonds in a portfolio?'," u'https://old.reddit.com/r/investing/comments/b5wb0o/the_purpose_of_bonds_in_a_portfolio/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-27 14:08:35 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b5y0jg/southwest_boeing_737_max_plane_makes_emergency/'," datetime.datetime(2019, 3, 27, 14, 8, 32, 695267)",'2019-03-27',u'Fundamentals',u'57','wallstreetbets',u'Southwest Boeing 737 Max plane makes emergency landing in Orlando after suffering engine failure just two weeks after second fatal crash involving similar aircraft'," u'https://www.dailymail.co.uk/news/article-6853553/Southwest-737-Max-makes-emergency-landing-Orlando.html'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b5zsuw/graph_of_my_pulse_during_trading_day/'," datetime.datetime(2019, 3, 27, 14, 23, 27, 597756)",'2019-03-27',u'Shitpost',u'2126','wallstreetbets',u'Graph of my pulse during trading day.'," u'https://old.reddit.com/r/wallstreetbets/comments/b5zsuw/graph_of_my_pulse_during_trading_day/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b60a5q/shorting_cost_consumer_data_comes_out_tmrw_take_a/'," datetime.datetime(2019, 3, 27, 14, 28, 23, 754761)",'2019-03-27',u'Technicals',u'14','wallstreetbets',u'SHORTING $COST .... CONSUMER DATA COMES OUT TMRW.. TAKE A LOOK AT BEARISH CHART!'," u'https://old.reddit.com/r/wallstreetbets/comments/b60a5q/shorting_cost_consumer_data_comes_out_tmrw_take_a/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b5ykpm/trade_setup_on_mcdonalds_come_get_your_tendies/'," datetime.datetime(2019, 3, 27, 14, 28, 31, 394298)",'2019-03-27',u'Technicals',u'3','wallstreetbets',u'Trade Setup on McDonalds. Come get your tendies. [Bullish Pennant]'," u'https://www.tradingview.com/x/rz4n5Rmi/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b5wkfi/weworks_revenue_doubled_last_year_so_did_its/'," datetime.datetime(2019, 3, 27, 14, 59, 35, 868803)",'2019-03-27',None,u'1','investing',u'WeWork\u2019s Revenue Doubled Last Year. So Did Its Losses (Bloomberg)'," u'https://old.reddit.com/r/investing/comments/b5wkfi/weworks_revenue_doubled_last_year_so_did_its/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b60xhq/737max_news_time_to_short_ba_again_these_planes/'," datetime.datetime(2019, 3, 27, 15, 2, 57, 485681)",'2019-03-27',u'Shitpost',u'99','wallstreetbets',"u""737Max news: Time to short $BA again, these planes can't even make it when being transported on a train see pic for DD"""," u'https://old.reddit.com/r/wallstreetbets/comments/b60xhq/737max_news_time_to_short_ba_again_these_planes/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b5wj5u/find_past_information_about_any_company_stock/'," datetime.datetime(2019, 3, 27, 15, 3, 39, 537823)",'2019-03-27',None,u'0','investing',u'Find past information about any company stock prices'," u'https://old.reddit.com/r/investing/comments/b5wj5u/find_past_information_about_any_company_stock/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b60lyb/china_suspends_airworthiness_certificate_for/'," datetime.datetime(2019, 3, 27, 15, 8, 1, 266518)",'2019-03-27',u'Stocks',u'67','wallstreetbets',u'China Suspends Airworthiness Certificate for Boeing 737 Max'," u'https://www.bloomberg.com/news/articles/2019-03-26/china-suspends-airworthiness-certificate-for-boeing-737-max-jet'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b5yucb/do_stocks_jump_alot_once_they_get_fda_approval/'," datetime.datetime(2019, 3, 27, 15, 14, 49, 200310)",'2019-03-27',None,u'0','stocks',u'Do stocks jump alot once they get fda approval?'," u'https://old.reddit.com/r/stocks/comments/b5yucb/do_stocks_jump_alot_once_they_get_fda_approval/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b5wqzl/wm_stock/'," datetime.datetime(2019, 3, 27, 15, 15, 3, 997556)",'2019-03-27',None,u'8','investing',u'WM Stock'," u'https://old.reddit.com/r/investing/comments/b5wqzl/wm_stock/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b6342u/daily_discussion_thread_march_27_2019/'," datetime.datetime(2019, 3, 27, 15, 23, 24, 659560)",'2019-03-27',u'Daily Discussion',u'36','wallstreetbets',"u'Daily Discussion Thread - March 27, 2019'"," u'https://old.reddit.com/r/wallstreetbets/comments/b6342u/daily_discussion_thread_march_27_2019/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b62pc1/anyone_tried_saxo_banks_discretionary_trading/'," datetime.datetime(2019, 3, 27, 15, 28, 28, 210661)",'2019-03-27',u'Discussion',u'2','wallstreetbets',"u""Anyone tried Saxo Bank's discretionary trading?"""," u'https://old.reddit.com/r/wallstreetbets/comments/b62pc1/anyone_tried_saxo_banks_discretionary_trading/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b5wx5u/what_are_some_good_podcasts_for_getting_daily/'," datetime.datetime(2019, 3, 27, 15, 39, 6, 977449)",'2019-03-27',None,u'3','investing',u'What are some good podcasts for getting daily economic news?'," u'https://old.reddit.com/r/investing/comments/b5wx5u/what_are_some_good_podcasts_for_getting_daily/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b5lzvs/can_anyone_help_with_a_good_app_or_website_theyd/'," datetime.datetime(2019, 3, 27, 15, 43, 24, 114487)",'2019-03-27',None,u'0','StockMarket',"u'Can anyone help with a good app or website they\u2019d recommend for accurate dividend history, upcoming earnings, or just stock data in general? (I have an iPhone). If anyone could recommend something that\u2019d be awesome!'"," u'https://old.reddit.com/r/StockMarket/comments/b5lzvs/can_anyone_help_with_a_good_app_or_website_theyd/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-27 15:43:54 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b5zns0/best_custodial_account/'," datetime.datetime(2019, 3, 27, 15, 43, 51, 458232)",'2019-03-27',u'Advice Request',u'1','stocks',u'Best custodial account'," u'https://old.reddit.com/r/stocks/comments/b5zns0/best_custodial_account/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b67e1j/wsbs_collective_knowledge_of_the_stock_market/'," datetime.datetime(2019, 3, 27, 16, 0, 2, 36251)",'2019-03-27',u'Meme',u'116','wallstreetbets',"u""WSB's collective knowledge of the stock market"""," u'https://old.reddit.com/r/wallstreetbets/comments/b67e1j/wsbs_collective_knowledge_of_the_stock_market/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
2019-03-27 16:00:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b5x043/high_dividend_etf/'," datetime.datetime(2019, 3, 27, 16, 4, 3, 670237)",'2019-03-27',u'Discussion',u'58','investing',u'High Dividend ETF?'," u'https://old.reddit.com/r/investing/comments/b5x043/high_dividend_etf/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b5xor4/cbl_is_not_going_to_be_putting_out_q3q4_dividends/'," datetime.datetime(2019, 3, 27, 16, 8, 32, 594913)",'2019-03-27',None,u'1','investing',"u""$CBL is not going to be putting out Q3Q4 dividends... a stock traded entirely for its dividends is cutting it's divs in half."""," u'https://old.reddit.com/r/investing/comments/b5xor4/cbl_is_not_going_to_be_putting_out_q3q4_dividends/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b65yvi/technology_and_consumer_staples_sentiment/'," datetime.datetime(2019, 3, 27, 16, 22, 48, 601398)",'2019-03-27',None,u'0','investing',u'Technology and Consumer Staples sentiment trending up'," u'https://old.reddit.com/r/investing/comments/b65yvi/technology_and_consumer_staples_sentiment/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b63bnb/bond_autists_what_are_the_factors_that_lead/'," datetime.datetime(2019, 3, 27, 17, 2, 43, 471163)",'2019-03-27',u'Discussion',u'9','wallstreetbets',u'Bond Autists: What are the factors that lead people to bid for bonds and lower the yield?'," u'https://old.reddit.com/r/wallstreetbets/comments/b63bnb/bond_autists_what_are_the_factors_that_lead/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b63qt4/google_backed_mobvoi_seeks_1_billion_valuation/'," datetime.datetime(2019, 3, 27, 17, 8, 36, 370189)",'2019-03-27',u'Discussion',u'6','wallstreetbets',u'Google backed Mobvoi seeks $1 billion valuation'," u'https://techleak.video.blog/2019/03/27/google-backed-mobvoi-seeks-1-billion-valuation/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b5mm4j/everyone_is_in_panic_mode_about_an_incoming/'," datetime.datetime(2019, 3, 27, 17, 24, 53, 635203)",'2019-03-27',None,u'28','StockMarket',u'Everyone is in panic mode about an incoming recession could be a good reason to panic first right now'," u'https://old.reddit.com/r/StockMarket/comments/b5mm4j/everyone_is_in_panic_mode_about_an_incoming/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Daytrading/comments/b45gu3/day_2_of_my_papertrading_so_far/'," datetime.datetime(2019, 3, 27, 17, 26, 55, 572944)",'2019-03-27',None,u'5','Daytrading',"u'""Day 2"" of my papertrading so far.'"," u'https://old.reddit.com/r/Daytrading/comments/b45gu3/day_2_of_my_papertrading_so_far/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-27 17:27:25 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
INFO:scrapy.core.engine:Closing spider (finished)
2019-03-27 17:27:26 [scrapy.core.engine] INFO: Closing spider (finished)
INFO:scrapy.statscollectors:Dumping Scrapy stats:
"
https://old.reddit.com/r/thewallstreet/comments/b3mkaq/daily_spx_tpos_03202019/'," datetime.datetime(2019, 3, 27, 17, 33, 25, 137224)",'2019-03-27',None,u'8','thewallstreet',"u""Daily SPX TPO's 03-20-2019"""," u'https://i.imgur.com/WFijIBR.png'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b5y3i8/thoughts_on_pershing_square/'," datetime.datetime(2019, 3, 27, 17, 35, 7, 831253)",'2019-03-27',u'Discussion',u'0','investing',u'Thoughts on Pershing Square.'," u'https://old.reddit.com/r/investing/comments/b5y3i8/thoughts_on_pershing_square/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b64de2/gamestop_makes_a_play_with_multiple_esports/'," datetime.datetime(2019, 3, 27, 17, 38, 26, 633983)",'2019-03-27',u'Stocks',u'22','wallstreetbets',u'GameStop Makes a Play with Multiple Esports Partners'," u'https://www.nasdaq.com/press-release/gamestop-makes-a-play-with-multiple-esports-partnerssupporting-amateur-players-nationwide-20190327-00473'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b63w6v/after_two_of_the_greatest_bull_markets_in_us/'," datetime.datetime(2019, 3, 27, 17, 38, 36, 419357)",'2019-03-27',u'Discussion',u'100','wallstreetbets',"u'After Two Of The Greatest Bull Markets In U.S. History, Why Are Boomers So Broke?'"," u'https://www.reddit.com/user/Fatherthinger/comments/b62nq8/after_two_of_the_greatest_bull_markets_in_us/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b64hbp/rev_insider_buying_patterns_etc/'," datetime.datetime(2019, 3, 27, 17, 38, 48, 956409)",'2019-03-27',u'DD',u'9','wallstreetbets',"u'$REV - Insider buying patterns, etc.'"," u'https://old.reddit.com/r/wallstreetbets/comments/b64hbp/rev_insider_buying_patterns_etc/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b64gia/sigh_and_lament_robinhood_is_no_longer_supporting/'," datetime.datetime(2019, 3, 27, 17, 43, 22, 333923)",'2019-03-27',u'Fundamentals',u'31','wallstreetbets',u'Sigh and lament Robinhood is no longer supporting its public API'," u'https://old.reddit.com/r/wallstreetbets/comments/b64gia/sigh_and_lament_robinhood_is_no_longer_supporting/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b64cny/hunt_halted_by_nasdaq/'," datetime.datetime(2019, 3, 27, 17, 43, 29, 977173)",'2019-03-27',u'Discussion',u'33','wallstreetbets',u'$HUNT halted by NASDAQ'," u'https://old.reddit.com/r/wallstreetbets/comments/b64cny/hunt_halted_by_nasdaq/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b67qp1/v_ma_axp_dfs_obsolete/'," datetime.datetime(2019, 3, 27, 17, 47, 33, 656818)",'2019-03-27',u'Discussion',u'0','stocks',"u'V, MA, AXP, DFS obsolete?'"," u'https://old.reddit.com/r/stocks/comments/b67qp1/v_ma_axp_dfs_obsolete/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b641sc/faa_boeings_737_max_to_face_heat_in_congress/'," datetime.datetime(2019, 3, 27, 17, 52, 11, 674927)",'2019-03-27',None,u'7','stocks',"u'FAA, Boeing\u2019s 737 Max to face heat in Congress Wednesday as plane-maker outlines software fix'"," u'https://old.reddit.com/r/stocks/comments/b641sc/faa_boeings_737_max_to_face_heat_in_congress/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b5yypr/thinking_about_buying_a_town_house_in_va_that_is/'," datetime.datetime(2019, 3, 27, 17, 55, 1, 357145)",'2019-03-27',None,u'0','investing',"u'Thinking about buying a town house in VA that is 1 hour driving far from DC and it\u2019s in a good good shape but it was built in 1975, should I go for it ? How old is too old ?'"," u'https://old.reddit.com/r/investing/comments/b5yypr/thinking_about_buying_a_town_house_in_va_that_is/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b5ynsm/a_class_action_lawsuit_is_being_filed_against/'," datetime.datetime(2019, 3, 27, 17, 58, 25, 961881)",'2019-03-27',u'Discussion',u'2','investing',u'A class action lawsuit is being filed against CenturyLink for anyone that purchased shares between about May 2018-March 4 2019. What does this mean?'," u'https://old.reddit.com/r/investing/comments/b5ynsm/a_class_action_lawsuit_is_being_filed_against/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b603u8/i_invested_a_large_of_my_net_worth_in_one_single/'," datetime.datetime(2019, 3, 27, 17, 59, 34, 288920)",'2019-03-27',None,u'380','stocks',"u""I invested a large % of my net worth in one single stock. In any other situation, I would think it's absolutely crazy, but it's a pharmaceutical dominating the market for a disease I have. Depending on the outcome of a current trial, I\u2019m either going to be poor and sick or rich and healthy."""," u'https://old.reddit.com/r/stocks/comments/b603u8/i_invested_a_large_of_my_net_worth_in_one_single/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b5y61o/4667_just_randomly_showed_up_in_my_work/'," datetime.datetime(2019, 3, 27, 18, 1, 58, 551755)",'2019-03-27',None,u'0','investing',u'$4667 just randomly showed up in my work retirement plan....what should I do?'," u'https://old.reddit.com/r/investing/comments/b5y61o/4667_just_randomly_showed_up_in_my_work/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b68z7d/lulu_earnings_thread/'," datetime.datetime(2019, 3, 27, 18, 5, 2, 99148)",'2019-03-27',u'Earnings Thread',u'23','wallstreetbets',u'$LULU earnings thread'," u'https://old.reddit.com/r/wallstreetbets/comments/b68z7d/lulu_earnings_thread/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
2019-03-27 21:05:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
INFO:scrapy.extensions.logstats:Crawled 2 pages (at 2 pages/min), scraped 0 items (at 0 items/min)
2019-03-27 21:05:19 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 2 pages/min), scraped 0 items (at 0 items/min)
INFO:scrapy.extensions.logstats:Crawled 6 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2019-03-27 21:05:19 [scrapy.extensions.logstats] INFO: Crawled 6 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b68peh/snap_pumpplunge_protection_continues_seems_the/'," datetime.datetime(2019, 3, 27, 21, 5, 19, 436336)",'2019-03-27',u'Discussion',u'3','wallstreetbets',"u'$SNAP - Pump/plunge protection continues. Seems the ""squeeze chasers"" won\'t let this die. Gap down to $7 or continue trapping retail and squeezing shorts? I was expecting an offering by now.'"," u'https://old.reddit.com/r/wallstreetbets/comments/b68peh/snap_pumpplunge_protection_continues_seems_the/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _INFO:scrapy.statscollectors:Dumping Scrapy stats:
"
https://old.reddit.com/r/wallstreetbets/comments/b68igc/so_ig_group_opens_retail_forex_in_murica_for/'," datetime.datetime(2019, 3, 28, 9, 29, 30, 54495)",'2019-03-28',u'Storytime',u'2','wallstreetbets',"u""So IG Group Opens Retail Forex in 'Murica - For people like me This is How You Can Pass the Approval Process"""," u'https://old.reddit.com/r/wallstreetbets/comments/b68igc/so_ig_group_opens_retail_forex_in_murica_for/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
app.py:12: ScrapyDeprecationWarning: Importing from scrapy.xlib.pydispatch is deprecated and will no longer be supported in future Scrapy versions. If you just want to connect signals use the from_crawler class method, otherwise import pydispatch directly if needed. See: https://github.com/scrapy/scrapy/issues/1762
  from scrapy.xlib.pydispatch import dispatcher
INFO:scrapy.utils.log:Scrapy 1.5.1 started (bot: redditcrawler)
2019-03-28 09:30:02 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: redditcrawler)
INFO:scrapy.utils.log:Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 2.7.14 |Anaconda, Inc.| (default, Dec  7 2017, 11:07:58) - [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)], pyOpenSSL 16.2.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Darwin-15.3.0-x86_64-64bit
2019-03-28 09:30:02 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 2.7.14 |Anaconda, Inc.| (default, Dec  7 2017, 11:07:58) - [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)], pyOpenSSL 16.2.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Darwin-15.3.0-x86_64-64bit
INFO:scrapy.crawler:Overridden settings: "
https://old.reddit.com/r/investing/comments/b68uor/can_you_help_me_understand_my_new_job/'," datetime.datetime(2019, 3, 28, 9, 30, 0, 708)",'2019-03-28',None,u'1','investing',u'Can you help me understand my new job offer/benefits?'," u'https://old.reddit.com/r/investing/comments/b68uor/can_you_help_me_understand_my_new_job/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b67twh/personal_bankruptcy_imminent_boys/'," datetime.datetime(2019, 3, 28, 9, 30, 30, 852279)",'2019-03-28',u'Storytime',u'49','wallstreetbets',u'Personal bankruptcy imminent boys'," u'https://old.reddit.com/r/wallstreetbets/comments/b67twh/personal_bankruptcy_imminent_boys/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 2 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-28 09:31:01 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b67kkx/boeing_issues_software_fix/'," datetime.datetime(2019, 3, 28, 9, 31, 31, 473973)",'2019-03-28',u'Stocks',u'4','wallstreetbets',u'Boeing issues software fix'," u'https://imgur.com/a/21vHgQb'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 2 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-28 09:32:01 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
INFO:__main__:Scrape complete.
2019-03-28 09:32:01 [__main__] INFO: Scrape complete.
Traceback (most recent call last):
  File ""app.py"", line 22, in <module>
    process = CrawlerProcess(get_project_settings())
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/scrapy/crawler.py"", line 249, in __init__
    super(CrawlerProcess, self).__init__(settings)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/scrapy/crawler.py"", line 137, in __init__
    self.spider_loader = _get_spider_loader(settings)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/scrapy/crawler.py"", line 336, in _get_spider_loader
    return loader_cls.from_settings(settings.frozencopy())
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/scrapy/spiderloader.py"", line 61, in from_settings
    return cls(settings)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/scrapy/spiderloader.py"", line 25, in __init__
    self._load_all_spiders()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/scrapy/spiderloader.py"", line 47, in _load_all_spiders
    for module in walk_modules(name):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/scrapy/utils/misc.py"", line 69, in walk_modules
    mods += walk_modules(fullpath)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/scrapy/utils/misc.py"", line 63, in walk_modules
    mod = import_module(path)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/importlib/__init__.py"", line 37, in import_module
    __import__(name)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/spiders/commentspider/__init__.py"", line 6, in <module>
    from redditcrawler.items import CommentItem
ImportError: cannot import name CommentItem
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b68bqp/does_it_ever_concern_you_that_assumptions_of/'," datetime.datetime(2019, 3, 28, 9, 32, 1, 403587)",'2019-03-28',u'Discussion',u'9','investing',u'Does it ever concern you that assumptions of perpetual growth with minimal interruption are only based on the recent trend in one country?'," u'https://old.reddit.com/r/investing/comments/b68bqp/does_it_ever_concern_you_that_assumptions_of/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b67dod/fellow_autists_this_is_my_lulu_trade_what_is_yours/'," datetime.datetime(2019, 3, 28, 9, 32, 32, 54381)",'2019-03-28',u'Options',u'5','wallstreetbets',"u'Fellow autists, this is my $LULU trade, what is yours?'"," u'https://old.reddit.com/r/wallstreetbets/comments/b67dod/fellow_autists_this_is_my_lulu_trade_what_is_yours/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
DEBUG:scrapy.downloadermiddlewares.retry:Retrying <GET https://old.reddit.com/r/stockmarket/new/> (failed 1 times): User timeout caused connection failure: Getting https://old.reddit.com/r/stockmarket/new/ took longer than 180.0 seconds..
2019-03-28 09:33:02 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://old.reddit.com/r/stockmarket/new/> (failed 1 times): User timeout caused connection failure: Getting https://old.reddit.com/r/stockmarket/new/ took longer than 180.0 seconds..
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b5zu78/im_a_little_confused_about_shorts_a_few_questions/'," datetime.datetime(2019, 3, 28, 9, 33, 32, 406617)",'2019-03-28',None,u'8','investing',"u""I'm a little confused about shorts, a few questions."""," u'https://old.reddit.com/r/investing/comments/b5zu78/im_a_little_confused_about_shorts_a_few_questions/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b673pv/lets_talk_about_canada/'," datetime.datetime(2019, 3, 28, 9, 33, 32, 619910)",'2019-03-28',u'DD',u'40','wallstreetbets',"u""Let's talk about Canada"""," u'https://old.reddit.com/r/wallstreetbets/comments/b673pv/lets_talk_about_canada/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b685zn/how_to_learn_more_about_private_equity/'," datetime.datetime(2019, 3, 28, 9, 34, 33, 37492)",'2019-03-28',None,u'6','investing',u'How to learn more about Private Equity?'," u'https://old.reddit.com/r/investing/comments/b685zn/how_to_learn_more_about_private_equity/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b66vvf/indepth_guide_to_the_lyft_ipo_transportation/'," datetime.datetime(2019, 3, 28, 9, 34, 33, 212979)",'2019-03-28',u'Stocks',u'4','wallstreetbets',u'In-Depth Guide to the Lyft IPO - Transportation Disruptor or Money Pit?'," u'https://grizzle.com/lyft-ipo-transportation-disruptor-money-pit/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b66to5/ladybaybee_recommended_stock_picks_20190327/'," datetime.datetime(2019, 3, 28, 9, 35, 33, 948084)",'2019-03-28',u'Stocks',u'7','wallstreetbets',u'Ladybaybee recommended stock picks 2019-03-27'," u'https://imgur.com/a/wGWWYrZ'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
2019-03-28 09:36:04 [scrapy.core.scraper] ERROR: Error processing "
https://old.reddit.com/r/stocks/comments/b65xad/australias_cromwell_confirms_takeover_talks_with/'," datetime.datetime(2019, 3, 28, 9, 36, 4, 213182)",'2019-03-28',None,u'1','stocks',u'Australia\u2019s Cromwell confirms takeover talks with UK REIT RDI'," u'https://old.reddit.com/r/stocks/comments/b65xad/australias_cromwell_confirms_takeover_talks_with/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b66osm/well_lets_hope_these_dont_expire_worthless_on/'," datetime.datetime(2019, 3, 28, 9, 36, 34, 576255)",'2019-03-28',"u""oh no.. it's retarded""",u'72','wallstreetbets',u'Well... Let\u2019s hope these don\u2019t expire worthless on Friday. \U0001f923'," u'https://old.reddit.com/r/wallstreetbets/comments/b66osm/well_lets_hope_these_dont_expire_worthless_on/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
DEBUG:scrapy.downloadermiddlewares.retry:Retrying <GET https://old.reddit.com/r/ValueInvesting/new/> (failed 1 times): User timeout caused connection failure: Getting https://old.reddit.com/r/ValueInvesting/new/ took longer than 180.0 seconds..
2019-03-28 09:37:04 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://old.reddit.com/r/ValueInvesting/new/> (failed 1 times): User timeout caused connection failure: Getting https://old.reddit.com/r/ValueInvesting/new/ took longer than 180.0 seconds..
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b682om/simple_treasury_bill_questions/'," datetime.datetime(2019, 3, 28, 9, 37, 4, 696574)",'2019-03-28',None,u'1','investing',u'simple Treasury Bill questions'," u'https://old.reddit.com/r/investing/comments/b682om/simple_treasury_bill_questions/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b65wh4/how_to_get_started/'," datetime.datetime(2019, 3, 28, 9, 37, 34, 982078)",'2019-03-28',None,u'0','stocks',u'How to get started?'," u'https://old.reddit.com/r/stocks/comments/b65wh4/how_to_get_started/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
2019-03-28 09:38:05 [scrapy.core.scraper] ERROR: Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b66imm/got_tired_of_waiting_to_get_some_gains_said_fuck/'," datetime.datetime(2019, 3, 28, 9, 37, 35, 176185)",'2019-03-28',u'Options',u'29','wallstreetbets',u'Got tired of waiting to get some gains said fuck it and we ended up here'," u'https://old.reddit.com/r/wallstreetbets/comments/b66imm/got_tired_of_waiting_to_get_some_gains_said_fuck/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
DEBUG:scrapy.downloadermiddlewares.retry:Retrying <GET https://old.reddit.com/r/thewallstreet/new/> (failed 1 times): User timeout caused connection failure: Getting https://old.reddit.com/r/thewallstreet/new/ took longer than 180.0 seconds..
2019-03-28 09:38:05 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://old.reddit.com/r/thewallstreet/new/> (failed 1 times): User timeout caused connection failure: Getting https://old.reddit.com/r/thewallstreet/new/ took longer than 180.0 seconds..
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b66grc/elizabeth_holmes_but_with_elon_musks_eyes/'," datetime.datetime(2019, 3, 28, 9, 38, 35, 726978)",'2019-03-28',u'Shitpost',u'2549','wallstreetbets',"u""Elizabeth Holmes but with Elon Musk's eyes"""," u'https://old.reddit.com/r/wallstreetbets/comments/b66grc/elizabeth_holmes_but_with_elon_musks_eyes/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b67gj7/what_happened_to_dogsofthedowcom/'," datetime.datetime(2019, 3, 28, 9, 39, 36, 377752)",'2019-03-28',None,u'5','investing',u'What happened to DogsoftheDow.com?'," u'https://old.reddit.com/r/investing/comments/b67gj7/what_happened_to_dogsofthedowcom/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b65nrb/cnc_acquisition_powermove_or_not/'," datetime.datetime(2019, 3, 28, 9, 40, 6, 747879)",'2019-03-28',None,u'0','stocks',u'CNC Acquisition. Powermove or not?'," u'https://old.reddit.com/r/stocks/comments/b65nrb/cnc_acquisition_powermove_or_not/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b665uc/knock_on_wood_i_dont_get_called_gay_for_this/'," datetime.datetime(2019, 3, 28, 9, 40, 37, 82258)",'2019-03-28',u'YOLO',u'1','wallstreetbets',u'Knock on wood I don\u2019t get called gay for this'," u'https://old.reddit.com/r/wallstreetbets/comments/b665uc/knock_on_wood_i_dont_get_called_gay_for_this/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b67afj/as_xpo_logistics_when_down_over_50_since_its_peak/'," datetime.datetime(2019, 3, 28, 9, 42, 7, 982269)",'2019-03-28',u'Education',u'3','investing',"u'As XPO Logistics when down over 50% since its peak in October 2018, this could be the moment to buy. What do you think, is the panic mode slowing down now?'"," u'https://old.reddit.com/r/investing/comments/b67afj/as_xpo_logistics_when_down_over_50_since_its_peak/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/rdona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b65iuh/google_apple_amazon_is_there_any_real_difference/'," datetime.datetime(2019, 3, 28, 9, 42, 38, 309845)",'2019-03-28',None,u'0','stocks',"u'Google, Apple, Amazon - is there any real difference?'"," u'https://old.reddit.com/r/stocks/comments/b65iuh/google_apple_amazon_is_there_any_real_difference/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
2019-03-28 09:43:08 [scrapy.core.scraper] ERROR: Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b65xmc/finally_dug_out_of_a_75_loss_today/'," datetime.datetime(2019, 3, 28, 9, 43, 8, 628211)",'2019-03-28',u'Options',u'64','wallstreetbets',u'Finally dug out of a 75% loss today'," u'https://imgur.com/8Dl6Yvy'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
DEBUG:scrapy.downloadermiddlewares.retry:Retrying <GET https://old.reddit.com/r/Daytrading/new/> (failed 2 times): User timeout caused connection failure: Getting https://old.reddit.com/r/Daytrading/new/ took longer than 180.0 seconds..
2019-03-28 09:43:38 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://old.reddit.com/r/Daytrading/new/> (failed 2 times): User timeout caused connection failure: Getting https://old.reddit.com/r/Daytrading/new/ took longer than 180.0 seconds..
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b677bx/china_on_the_move/'," datetime.datetime(2019, 3, 28, 9, 44, 39, 504731)",'2019-03-28',None,u'4','investing',u'China on the move'," u'https://old.reddit.com/r/investing/comments/b677bx/china_on_the_move/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 5 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-28 09:45:09 [scrapy.extensions.logstats] INFO: Crawled 5 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b64yy0/how_do_you_buy_and_sell_stocks/'," datetime.datetime(2019, 3, 28, 9, 45, 9, 805417)",'2019-03-28',None,u'0','stocks',u'How do you buy and sell stocks?'," u'https://old.reddit.com/r/stocks/comments/b64yy0/how_do_you_buy_and_sell_stocks/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
DEBUG:scrapy.downloadermiddlewares.retry:Retrying <GET https://old.reddit.com/r/thewallstreet/new/> (failed 2 times): User timeout caused connection failure: Getting https://old.reddit.com/r/thewallstreet/new/ took longer than 180.0 seconds..
2019-03-28 09:45:40 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://old.reddit.com/r/thewallstreet/new/> (failed 2 times): User timeout caused connection failure: Getting https://old.reddit.com/r/thewallstreet/new/ took longer than 180.0 seconds..
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b65221/whats_50k_to_a_mf_like_me_would_you_please_remind/'," datetime.datetime(2019, 3, 28, 9, 45, 40, 130664)",'2019-03-28',u'YOLO',u'286','wallstreetbets',u'Whats 50k to a mf like me would you please remind me'," u'https://old.reddit.com/r/wallstreetbets/comments/b65221/whats_50k_to_a_mf_like_me_would_you_please_remind/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
2019-03-28 09:46:10 [scrapy.core.scraper] ERROR: Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b65jfn/turkish_lira_swap_rate_explosion/'," datetime.datetime(2019, 3, 28, 9, 45, 40, 125637)",'2019-03-28',u'Discussion',u'2','wallstreetbets',u'Turkish lira swap rate explosion?'," u'https://old.reddit.com/r/wallstreetbets/comments/b65jfn/turkish_lira_swap_rate_explosion/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b66ikg/why_arent_all_stocks_transactions_automated_yet/'," datetime.datetime(2019, 3, 28, 9, 47, 11, 630827)",'2019-03-28',None,u'4','investing',"u""Why aren't all stocks transactions automated yet?"""," u'https://old.reddit.com/r/investing/comments/b66ikg/why_arent_all_stocks_transactions_automated_yet/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 8 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-28 09:47:41 [scrapy.extensions.logstats] INFO: Crawled 8 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b64vym/what_are_some_companies_that_might_benefit_from/'," datetime.datetime(2019, 3, 28, 9, 47, 41, 651710)",'2019-03-28',None,u'0','stocks',"u""What are some companies that might benefit from the approval of Trump's wall?"""," u'https://old.reddit.com/r/stocks/comments/b64vym/what_are_some_companies_that_might_benefit_from/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
2019-03-28 09:48:12 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b64dkq/luv_southwest_airlines/'," datetime.datetime(2019, 3, 28, 9, 49, 42, 977806)",'2019-03-28',u'Ticker Discussion',u'2','stocks',u'LUV - Southwest Airlines'," u'https://old.reddit.com/r/stocks/comments/b64dkq/luv_southwest_airlines/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b6611s/post_the_best_and_worst_investments_youve_made/'," datetime.datetime(2019, 3, 28, 9, 50, 13, 541571)",'2019-03-28',None,u'3','investing',"u""Post the best and worst investments you've made, but don't name which one is which."""," u'https://old.reddit.com/r/investing/comments/b6611s/post_the_best_and_worst_investments_youve_made/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
DEBUG:scrapy.downloadermiddlewares.retry:Gave up retrying <GET https://old.reddit.com/r/Daytrading/new/> (failed 3 times): User timeout caused connection failure.
2019-03-28 09:50:43 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET https://old.reddit.com/r/Daytrading/new/> (failed 3 times): User timeout caused connection failure.
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b605lh/will_i_get_striked_as_a_pattern_day_trader_if/'," datetime.datetime(2019, 3, 28, 9, 52, 14, 892323)",'2019-03-28',u'Question',u'0','stocks',u'Will I Get Striked As A Pattern Day Trader If...'," u'https://old.reddit.com/r/stocks/comments/b605lh/will_i_get_striked_as_a_pattern_day_trader_if/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b65rg8/why_do_a_lot_of_stocks_drop_after_positive/'," datetime.datetime(2019, 3, 28, 9, 53, 15, 679880)",'2019-03-28',None,u'0','investing',u'Why do a lot of stocks drop after positive news/events about the company?'," u'https://old.reddit.com/r/investing/comments/b65rg8/why_do_a_lot_of_stocks_drop_after_positive/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error downloading <GET https://old.reddit.com/r/Daytrading/new/>
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/scrapy/core/downloader/middleware.py"", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
TimeoutError: User timeout caused connection failure.
2019-03-28 09:53:46 [scrapy.core.scraper] ERROR: Error downloading <GET https://old.reddit.com/r/Daytrading/new/>
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/scrapy/core/downloader/middleware.py"", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
TimeoutError: User timeout caused connection failure.
DEBUG:scrapy.downloadermiddlewares.retry:Gave up retrying <GET https://old.reddit.com/r/StockMarket/new/> (failed 3 times): User timeout caused connection failure.
2019-03-28 09:53:46 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET https://old.reddit.com/r/StockMarket/new/> (failed 3 times): User timeout caused connection failure.
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b6457r/trnx_taronis_tech_is_a_criminal_fraud_formerly/'," datetime.datetime(2019, 3, 28, 9, 53, 45, 649041)",'2019-03-28',None,u'2','stocks',u'TRNX Taronis Tech is a criminal fraud formerly magnegas'," u'https://old.reddit.com/r/stocks/comments/b6457r/trnx_taronis_tech_is_a_criminal_fraud_formerly/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b65lvm/asset_allocation_and_balancing_based_in_number_of/'," datetime.datetime(2019, 3, 28, 9, 56, 17, 845787)",'2019-03-28',None,u'0','investing',u'Asset allocation and balancing - Based in number of units or equity?'," u'https://old.reddit.com/r/investing/comments/b65lvm/asset_allocation_and_balancing_based_in_number_of/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error downloading <GET https://old.reddit.com/r/StockMarket/new/>
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/scrapy/core/downloader/middleware.py"", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
TimeoutError: User timeout caused connection failure.
2019-03-28 09:56:48 [scrapy.core.scraper] ERROR: Error downloading <GET https://old.reddit.com/r/StockMarket/new/>
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/scrapy/core/downloader/middleware.py"", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
TimeoutError: User timeout caused connection failure.
DEBUG:scrapy.downloadermiddlewares.retry:Gave up retrying <GET https://old.reddit.com/r/Stock_Picks/new/> (failed 3 times): User timeout caused connection failure.
2019-03-28 09:56:48 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET https://old.reddit.com/r/Stock_Picks/new/> (failed 3 times): User timeout caused connection failure.
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b642qc/cannabis_eft/'," datetime.datetime(2019, 3, 28, 9, 57, 48, 873091)",'2019-03-28',None,u'3','stocks',u'Cannabis EFT'," u'https://old.reddit.com/r/stocks/comments/b642qc/cannabis_eft/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
2019-03-28 09:58:19 [scrapy.core.scraper] ERROR: Error processing "
https://old.reddit.com/r/investing/comments/b65c56/mcdonalds_buys_dynamic_yield_for_300_million/'," datetime.datetime(2019, 3, 28, 9, 59, 20, 217364)",'2019-03-28',u'Discussion',u'429','investing',"u""Mcdonald's buys Dynamic Yield for $300 million"""," u'https://old.reddit.com/r/investing/comments/b65c56/mcdonalds_buys_dynamic_yield_for_300_million/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error downloading <GET https://old.reddit.com/r/Stock_Picks/new/>
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/scrapy/core/downloader/middleware.py"", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
TimeoutError: User timeout caused connection failure.
2019-03-28 09:59:50 [scrapy.core.scraper] ERROR: Error downloading <GET https://old.reddit.com/r/Stock_Picks/new/>
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/scrapy/core/downloader/middleware.py"", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
TimeoutError: User timeout caused connection failure.
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b6c1sv/cumulative_central_bank_balance_sheets/'," datetime.datetime(2019, 3, 28, 10, 0, 15, 184750)",'2019-03-28',u'Discussion',u'28','wallstreetbets',u'Cumulative Central Bank Balance Sheets'," u'https://old.reddit.com/r/wallstreetbets/comments/b6c1sv/cumulative_central_bank_balance_sheets/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-03-28 10:00:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b60pkq/should_blockchain_technology_be_a_growing/'," datetime.datetime(2019, 3, 28, 10, 0, 50, 289982)",'2019-03-28',u'Question',u'0','stocks',u'Should Blockchain technology be a growing percentage of total investments?'," u'https://old.reddit.com/r/stocks/comments/b60pkq/should_blockchain_technology_be_a_growing/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b63o8l/rstocks_daily_discussion_wednesday_mar_27_2019/'," datetime.datetime(2019, 3, 28, 10, 1, 51, 849510)",'2019-03-28',None,u'3','stocks',"u'r/Stocks Daily Discussion Wednesday - Mar 27, 2019'"," u'https://old.reddit.com/r/stocks/comments/b63o8l/rstocks_daily_discussion_wednesday_mar_27_2019/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
 client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
 File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b658og/taxes_and_roth_ira/'," datetime.datetime(2019, 3, 28, 10, 2, 22, 117435)",'2019-03-28',None,u'0','investing',u'Taxes and Roth IRA'," u'https://old.reddit.com/r/investing/comments/b658og/taxes_and_roth_ira/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b6bssb/are_we_testing_the_spy_resistance_line_from_last/'," datetime.datetime(2019, 3, 28, 10, 3, 22, 538524)",'2019-03-28',u'Technicals',u'11','wallstreetbets',u'Are we testing the SPY resistance line from last fall?'," u'https://imgur.com/6uV3VxU'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b658mb/is_now_a_good_time_to_pay_down_loans/'," datetime.datetime(2019, 3, 28, 10, 5, 24, 38577)",'2019-03-28',None,u'10','investing',u'Is now a good time to pay down loans?'," u'https://old.reddit.com/r/investing/comments/b658mb/is_now_a_good_time_to_pay_down_loans/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 9 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-28 10:05:54 [scrapy.extensions.logstats] INFO: Crawled 9 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b63ejf/is_goldman_sachs_severely_undervalued_or_a_value/'," datetime.datetime(2019, 3, 28, 10, 5, 54, 363989)",'2019-03-28',None,u'11','stocks',u'Is Goldman Sachs severely undervalued or a value trap?'," u'https://old.reddit.com/r/stocks/comments/b63ejf/is_goldman_sachs_severely_undervalued_or_a_value/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b64rp0/protection_against_recession/'," datetime.datetime(2019, 3, 28, 10, 9, 26, 583993)",'2019-03-28',None,u'0','investing',u'protection against recession?'," u'https://old.reddit.com/r/investing/comments/b64rp0/protection_against_recession/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 9 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-28 10:09:56 [scrapy.extensions.logstats] INFO: Crawled 9 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b625mt/i_think_its_just_a_matter_of_time_until_netflix/'," datetime.datetime(2019, 3, 28, 10, 9, 56, 880505)",'2019-03-28',None,u'1','stocks',"u""I think it's just a matter of time until Netflix starts offering music."""," u'https://old.reddit.com/r/stocks/comments/b625mt/i_think_its_just_a_matter_of_time_until_netflix/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b61z5s/is_boeing_a_buy/'," datetime.datetime(2019, 3, 28, 10, 13, 59, 627338)",'2019-03-28',u'Ticker Question',u'0','stocks',u'Is BOEING a buy?'," u'https://old.reddit.com/r/stocks/comments/b61z5s/is_boeing_a_buy/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b61nv4/gone_for_a_big_sell_on_eurusd/'," datetime.datetime(2019, 3, 28, 10, 17, 31, 160968)",'2019-03-28',None,u'0','investing',u'Gone For A Big Sell On EurUsd'," u'https://old.reddit.com/r/investing/comments/b61nv4/gone_for_a_big_sell_on_eurusd/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b62mbo/want_to_buy_gaming_stockwhere_should_i_go_to/'," datetime.datetime(2019, 3, 28, 10, 17, 31, 868001)",'2019-03-28',None,u'0','investing',u'Want to buy Gaming stock...where should i go to compare them?'," u'https://old.reddit.com/r/investing/comments/b62mbo/want_to_buy_gaming_stockwhere_should_i_go_to/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 9 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-28 10:18:02 [scrapy.extensions.logstats] INFO: Crawled 9 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b617af/softbank_increase_roi_with_robotics_and_iot/'," datetime.datetime(2019, 3, 28, 10, 18, 2, 136318)",'2019-03-28',None,u'2','stocks',u'SoftBank: Increase ROI with Robotics and IoT Automation'," u'https://old.reddit.com/r/stocks/comments/b617af/softbank_increase_roi_with_robotics_and_iot/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b6cl9n/what_are_the_industry_reputations_of_the/'," datetime.datetime(2019, 3, 28, 10, 18, 49, 52632)",'2019-03-28',u'Discussion',u'5','wallstreetbets',"u""What are the industry reputations of the different investment banks' analysts?"""," u'https://old.reddit.com/r/wallstreetbets/comments/b6cl9n/what_are_the_industry_reputations_of_the/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b5mx6i/this_is_fresh_and_new_this_page_will_give_you_a/'," datetime.datetime(2019, 3, 28, 10, 22, 41, 28228)",'2019-03-28',None,u'0','StockMarket',"u'This is fresh and new, this page will give you a tips and news about trading. https://www.facebook.com/TIOMarkets/?ti=as'"," u'https://old.reddit.com/r/StockMarket/comments/b5mx6i/this_is_fresh_and_new_this_page_will_give_you_a/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
2019-03-28 10:23:11 [scrapy.core.scraper] ERROR: Error processing "
https://old.reddit.com/r/investing/comments/b6205n/how_can_i_get_access_to_professional_equity/'," datetime.datetime(2019, 3, 28, 10, 25, 49, 579772)",'2019-03-28',None,u'3','investing',u'How can i get access to professional equity research cheapest?'," u'https://old.reddit.com/r/investing/comments/b6205n/how_can_i_get_access_to_professional_equity/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
2019-03-28 10:26:19 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b5zru6/i_am_an_18_year_old_male_who_just_opened_an/'," datetime.datetime(2019, 3, 28, 10, 29, 54, 678238)",'2019-03-28',None,u'8','StockMarket',u'I am an 18 year old male who just opened an account through ameritrade. Any advice?'," u'https://old.reddit.com/r/StockMarket/comments/b5zru6/i_am_an_18_year_old_male_who_just_opened_an/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b5yn5g/best_stocks_to_short_right_now/'," datetime.datetime(2019, 3, 28, 10, 33, 28, 89076)",'2019-03-28',None,u'12','StockMarket',u'Best stocks to short right now?'," u'https://old.reddit.com/r/StockMarket/comments/b5yn5g/best_stocks_to_short_right_now/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b5wv9x/the_big_short/'," datetime.datetime(2019, 3, 28, 10, 37, 0, 182562)",'2019-03-28',None,u'0','StockMarket',u'The Big Short'," u'https://old.reddit.com/r/StockMarket/comments/b5wv9x/the_big_short/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b5v5o2/sectors_attempt_to_breakout/'," datetime.datetime(2019, 3, 28, 10, 40, 32, 278137)",'2019-03-28',None,u'90','StockMarket',u'Sectors Attempt to Breakout'," u'https://old.reddit.com/r/StockMarket/comments/b5v5o2/sectors_attempt_to_breakout/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b5twnu/whats_your_rule_of_thumb_gain_for_selling_ie_up/'," datetime.datetime(2019, 3, 28, 10, 44, 4, 821661)",'2019-03-28',None,u'0','StockMarket',"u""What's your rule of thumb % gain for selling? ie, up 30% week sell, 20% day etc"""," u'https://old.reddit.com/r/StockMarket/comments/b5twnu/whats_your_rule_of_thumb_gain_for_selling_ie_up/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/thewallstreet/comments/b3wkwe/nightly_trading_discussion_march_2122/'," datetime.datetime(2019, 3, 28, 10, 46, 36, 919534)",'2019-03-28',u'Daily',u'11','thewallstreet',u'Nightly Trading Discussion - (March 21/22)'," u'https://old.reddit.com/r/thewallstreet/comments/b3wkwe/nightly_trading_discussion_march_2122/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b5trsf/looking_for_a_portfolio_financial_calendar/'," datetime.datetime(2019, 3, 28, 10, 47, 37, 502225)",'2019-03-28',None,u'0','StockMarket',u'Looking for a portfolio financial calendar'," u'https://old.reddit.com/r/StockMarket/comments/b5trsf/looking_for_a_portfolio_financial_calendar/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-28 10:48:07 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/thewallstreet/comments/b3v8w5/post_market_discussion_march_21/'," datetime.datetime(2019, 3, 28, 10, 50, 9, 389242)",'2019-03-28',u'Daily',u'6','thewallstreet',u'Post Market Discussion - (March 21)'," u'https://old.reddit.com/r/thewallstreet/comments/b3v8w5/post_market_discussion_march_21/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b5tnj5/getting_started/'," datetime.datetime(2019, 3, 28, 10, 50, 41, 926484)",'2019-03-28',None,u'1','StockMarket',u'Getting started'," u'https://old.reddit.com/r/StockMarket/comments/b5tnj5/getting_started/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b5nrla/advice_on_investment_in_sp500_etf/'," datetime.datetime(2019, 3, 28, 10, 52, 45, 966783)",'2019-03-28',None,u'2','StockMarket',u'Advice on investment in S&P500 ETF'," u'https://old.reddit.com/r/StockMarket/comments/b5nrla/advice_on_investment_in_sp500_etf/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
2019-03-28 10:53:16 [scrapy.core.scraper] ERROR: Error processing "
https://old.reddit.com/r/thewallstreet/comments/b3uy20/equity_markets_2019_eoy_targets_march_2019_edition/'," datetime.datetime(2019, 3, 28, 10, 53, 13, 726678)",'2019-03-28',None,u'17','thewallstreet',u'Equity Markets 2019 EOY Targets - March 2019 Edition'," u'https://old.reddit.com/r/thewallstreet/comments/b3uy20/equity_markets_2019_eoy_targets_march_2019_edition/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b5r8um/why_you_should_invest/'," datetime.datetime(2019, 3, 28, 10, 53, 44, 293515)",'2019-03-28',None,u'0','StockMarket',u'Why you should invest!'," u'https://old.reddit.com/r/StockMarket/comments/b5r8um/why_you_should_invest/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Daytrading/comments/b4p9qi/new_fella_in_this_world/'," datetime.datetime(2019, 3, 28, 10, 54, 14, 854674)",'2019-03-28',None,u'7','Daytrading',u'New fella in this world'," u'https://old.reddit.com/r/Daytrading/comments/b4p9qi/new_fella_in_this_world/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-28 10:54:45 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/thewallstreet/comments/b3pfzg/daily_discussion_march_21/'," datetime.datetime(2019, 3, 28, 10, 56, 16, 507816)",'2019-03-28',u'Daily',u'12','thewallstreet',u'Daily Discussion - (March 21)'," u'https://old.reddit.com/r/thewallstreet/comments/b3pfzg/daily_discussion_march_21/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
2019-03-28 10:56:47 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b5phv1/analysts_release_buy_status_upgrade_for_tilray/'," datetime.datetime(2019, 3, 28, 10, 56, 47, 89661)",'2019-03-28',None,u'1','StockMarket',"u'Analysts Release Buy Status Upgrade For Tilray, Aurora, & Canopy Growth; With New Candidates Close Behind'"," u'https://old.reddit.com/r/StockMarket/comments/b5phv1/analysts_release_buy_status_upgrade_for_tilray/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Daytrading/comments/b4l0eq/trailing_stop_loss/'," datetime.datetime(2019, 3, 28, 10, 57, 17, 690225)",'2019-03-28',None,u'3','Daytrading',u'Trailing stop loss'," u'https://old.reddit.com/r/Daytrading/comments/b4l0eq/trailing_stop_loss/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-28 10:57:48 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b5p59g/todays_premarket_news_tuesday_march_26th_2019/'," datetime.datetime(2019, 3, 28, 10, 58, 49, 813699)",'2019-03-28',u'News',u'11','StockMarket',"u""Today's Pre-Market News [Tuesday, March 26th, 2019]"""," u'https://old.reddit.com/r/StockMarket/comments/b5p59g/todays_premarket_news_tuesday_march_26th_2019/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Daytrading/comments/b4kfri/best_laptop_to_day_trade_under_1000/'," datetime.datetime(2019, 3, 28, 10, 59, 19, 928514)",'2019-03-28',None,u'3','Daytrading',u'Best Laptop to Day Trade Under $1000'," u'https://old.reddit.com/r/Daytrading/comments/b4kfri/best_laptop_to_day_trade_under_1000/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-28 10:59:50 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Trading/comments/ayfe0b/i_want_to_learn_trading_from_basics/'," datetime.datetime(2019, 3, 28, 10, 59, 50, 265112)",'2019-03-28',None,u'12','Trading',u'I want to learn trading from basics.'," u'https://old.reddit.com/r/Trading/comments/ayfe0b/i_want_to_learn_trading_from_basics/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Daytrading/comments/b49w51/best_broker/'," datetime.datetime(2019, 3, 28, 11, 0, 51, 888237)",'2019-03-28',None,u'7','Daytrading',u'Best broker?'," u'https://old.reddit.com/r/Daytrading/comments/b49w51/best_broker/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-28 11:01:22 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Daytrading/comments/b476qz/best_option_for_day_trading_on_a_computer_that_is/'," datetime.datetime(2019, 3, 28, 11, 1, 52, 900004)",'2019-03-28',None,u'2','Daytrading',u'Best option for day trading on a computer that is locked down at work?'," u'https://old.reddit.com/r/Daytrading/comments/b476qz/best_option_for_day_trading_on_a_computer_that_is/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Daytrading/comments/b46icq/how_do_i_trade_vix/'," datetime.datetime(2019, 3, 28, 11, 2, 23, 760948)",'2019-03-28',None,u'2','Daytrading',u'How do I trade VIX?'," u'https://old.reddit.com/r/Daytrading/comments/b46icq/how_do_i_trade_vix/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b6e054/rh_calls_are_literally_free_money/'," datetime.datetime(2019, 3, 28, 11, 4, 21, 642987)",'2019-03-28',u'DD',u'23','wallstreetbets',u'RH Calls are literally Free Money'," u'https://old.reddit.com/r/wallstreetbets/comments/b6e054/rh_calls_are_literally_free_money/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b6couq/long_ba_insider_info/'," datetime.datetime(2019, 3, 28, 11, 5, 10, 993620)",'2019-03-28',u'DD',u'146','wallstreetbets',u'Long $BA insider info'," u'https://old.reddit.com/r/wallstreetbets/comments/b6couq/long_ba_insider_info/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 9 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-28 11:05:41 [scrapy.extensions.logstats] INFO: Crawled 9 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b6da1b/china_bends_the_knee/'," datetime.datetime(2019, 3, 28, 11, 9, 26, 603037)",'2019-03-28',u'Discussion',u'13','wallstreetbets',u'China bends the knee'," u'https://www.investing.com/news/economy-news/exclusive-china-makes-unprecedented-proposals-on-tech-transfer-trade-challenges-remain--us-officials-1820370'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b6clbm/i_dont_know_what_is_wrong/'," datetime.datetime(2019, 3, 28, 11, 10, 52, 646799)",'2019-03-28',None,u'1','investing',u'I dont know what is wrong'," u'https://old.reddit.com/r/investing/comments/b6clbm/i_dont_know_what_is_wrong/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b6d0zg/so_far_weve_survived_four_recessions_in_2019/'," datetime.datetime(2019, 3, 28, 11, 14, 31, 44465)",'2019-03-28',u'Meme',u'1188','wallstreetbets',u'So far we\u2019ve survived four recessions in 2019'," u'https://old.reddit.com/r/wallstreetbets/comments/b6d0zg/so_far_weve_survived_four_recessions_in_2019/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b6cotb/lyft_ipo/'," datetime.datetime(2019, 3, 28, 11, 15, 23, 18196)",'2019-03-28',None,u'12','investing',u'Lyft IPO'," u'https://old.reddit.com/r/investing/comments/b6cotb/lyft_ipo/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b6cp7c/does_buying_options_on_leveraged_etf_tqqq_uvxy/'," datetime.datetime(2019, 3, 28, 11, 19, 38, 198026)",'2019-03-28',u'Options',u'2','wallstreetbets',"u'Does buying options on leveraged etf (TQQQ, UVXY,..) make sense?'"," u'https://old.reddit.com/r/wallstreetbets/comments/b6cp7c/does_buying_options_on_leveraged_etf_tqqq_uvxy/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.utils.log:Scrapy 1.5.1 started (bot: redditcrawler)
2019-03-28 11:20:09 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: redditcrawler)
INFO:scrapy.utils.log:Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 2.7.14 |Anaconda, Inc.| (default, Dec  7 2017, 11:07:58) - [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)], pyOpenSSL 16.2.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Darwin-15.3.0-x86_64-64bit
2019-03-28 11:20:09 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 2.7.14 |Anaconda, Inc.| (default, Dec  7 2017, 11:07:58) - [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)], pyOpenSSL 16.2.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Darwin-15.3.0-x86_64-64bit
INFO:scrapy.crawler:Overridden settings: "
https://old.reddit.com/r/wallstreetbets/comments/b6e4hm/4th_time_the_charm_right_no_more_holding_yolos/'," datetime.datetime(2019, 3, 28, 11, 23, 47, 656346)",'2019-03-28',u'Storytime',u'117','wallstreetbets',u'4th time the charm right? No more holding yolos overnight. Time to go aggressively full autistic mode.'," u'https://old.reddit.com/r/wallstreetbets/comments/b6e4hm/4th_time_the_charm_right_no_more_holding_yolos/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b6en8u/trading_like_a_pro/'," datetime.datetime(2019, 3, 28, 11, 24, 20, 932225)",'2019-03-28',u'Meme',u'181','wallstreetbets',u'Trading like a pro'," u'https://old.reddit.com/r/wallstreetbets/comments/b6en8u/trading_like_a_pro/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b6d29q/historical_daily_data_for_sp500/'," datetime.datetime(2019, 3, 28, 11, 26, 36, 56709)",'2019-03-28',None,u'1','investing',u'Historical Daily Data for S&P500?'," u'https://old.reddit.com/r/investing/comments/b6d29q/historical_daily_data_for_sp500/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b6ek9l/pimco_other_pge_creditors_said_to_pitch_35/'," datetime.datetime(2019, 3, 28, 11, 28, 53, 833243)",'2019-03-28',u'Discussion',u'5','wallstreetbets',"u'Pimco, Other PG&E Creditors Said to Pitch $35 Billion Exit Plan'"," u'https://www.bloomberg.com/news/articles/2019-03-28/pimco-other-pg-e-creditors-said-to-pitch-35-billion-exit-plan'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b6e2hc/difference_between_schwab_one_and_schwab_checking/'," datetime.datetime(2019, 3, 28, 11, 40, 51, 752822)",'2019-03-28',None,u'3','investing',u'Difference between Schwab One and Schwab Checking?'," u'https://old.reddit.com/r/investing/comments/b6e2hc/difference_between_schwab_one_and_schwab_checking/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b6dd17/is_reinvesting_dividends_into_one_stock_that/'," datetime.datetime(2019, 3, 28, 11, 44, 54, 193333)",'2019-03-28',None,u'0','investing',u'Is reinvesting dividends into one stock that doesn\u2019t pay a dividend a bad idea?'," u'https://old.reddit.com/r/investing/comments/b6dd17/is_reinvesting_dividends_into_one_stock_that/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b6e39x/now_that_lyft_is_planning_an_ipo_of_20_billion/'," datetime.datetime(2019, 3, 28, 11, 55, 54, 645758)",'2019-03-28',None,u'206','investing',"u'Now that Lyft is planning an IPO of $20+ billion, Uber @$100-$120 billion. Car rental stocks are in the dumps.'"," u'https://old.reddit.com/r/investing/comments/b6e39x/now_that_lyft_is_planning_an_ipo_of_20_billion/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b66byh/an_assignment_in_stocks/'," datetime.datetime(2019, 3, 28, 12, 1, 7, 854289)",'2019-03-28',None,u'3','stocks',u'An assignment in stocks'," u'https://old.reddit.com/r/stocks/comments/b66byh/an_assignment_in_stocks/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b662u4/iagl/'," datetime.datetime(2019, 3, 28, 12, 4, 40, 173721)",'2019-03-28',None,u'1','stocks',u'IAG.L'," u'https://old.reddit.com/r/stocks/comments/b662u4/iagl/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b66ott/is_noone_afraid_we_are_really_heading_for_a/'," datetime.datetime(2019, 3, 28, 12, 4, 43, 185782)",'2019-03-28',None,u'0','stocks',u'Is noone afraid we are really heading for a prolonged BEAR market right now ?'," u'https://old.reddit.com/r/stocks/comments/b66ott/is_noone_afraid_we_are_really_heading_for_a/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b625h6/yield_curve/'," datetime.datetime(2019, 3, 28, 12, 6, 32, 154390)",'2019-03-28',None,u'50','StockMarket',u'Yield curve'," u'https://old.reddit.com/r/StockMarket/comments/b625h6/yield_curve/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b601pa/what_are_some_examples_of_defensive_stocks_such/'," datetime.datetime(2019, 3, 28, 12, 8, 3, 178526)",'2019-03-28',None,u'0','StockMarket',u'WHAT ARE SOME EXAMPLES OF DEFENSIVE STOCKS? SUCH AS TICKER NAMES IN CONSUMER STAPLES AND UTILITIES?'," u'https://old.reddit.com/r/StockMarket/comments/b601pa/what_are_some_examples_of_defensive_stocks_such/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-28 12:08:33 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b6eoko/seeking_alpha_yield_curve_inversion_suggests_new/'," datetime.datetime(2019, 3, 28, 12, 25, 28, 643873)",'2019-03-28',None,u'0','investing',"u'Seeking Alpha: ""Yield Curve Inversion Suggests New All-Time Highs For Stocks"".'"," u'https://old.reddit.com/r/investing/comments/b6eoko/seeking_alpha_yield_curve_inversion_suggests_new/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b66p6x/lyft_ipo_deepdive/'," datetime.datetime(2019, 3, 28, 12, 40, 48, 119833)",'2019-03-28',u'Resources',u'19','stocks',u'Lyft IPO Deep-Dive'," u'https://old.reddit.com/r/stocks/comments/b66p6x/lyft_ipo_deepdive/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b6icyf/lulu_call_holders_be_like/'," datetime.datetime(2019, 3, 28, 12, 41, 36, 691891)",'2019-03-28',u'Satire',u'3129','wallstreetbets',u'$LULU call holders be like'," u'https://old.reddit.com/r/wallstreetbets/comments/b6icyf/lulu_call_holders_be_like/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-03-28 12:42:07 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b6f0ok/best_diversifier_vs_large_cap_equity_etf/'," datetime.datetime(2019, 3, 28, 12, 55, 50, 639348)",'2019-03-28',None,u'3','investing',u'Best Diversifier vs Large Cap Equity ETF'," u'https://old.reddit.com/r/investing/comments/b6f0ok/best_diversifier_vs_large_cap_equity_etf/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b6gwex/no_8_has_long_been_regarded_as_the_luckiest/'," datetime.datetime(2019, 3, 28, 13, 16, 5, 823650)",'2019-03-28',u'Shitpost',u'28','wallstreetbets',"u""\u201cNo. 8 has long been regarded as the luckiest number in Chinese culture. With pronunciation of 'Ba' in Chinese, no. 8 sounds similar to the word 'Fa', which means to make a fortune. It contains meanings of prosperity, success and high social status too, so all business men favor it very much\u201d"""," u'https://old.reddit.com/r/wallstreetbets/comments/b6gwex/no_8_has_long_been_regarded_as_the_luckiest/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b66p8x/whos_your_favorite_cnbc_person_to_listen_to_and/'," datetime.datetime(2019, 3, 28, 13, 16, 50, 390347)",'2019-03-28',None,u'0','stocks',"u""Who's your favorite CNBC person to listen to and has the best opinions?"""," u'https://old.reddit.com/r/stocks/comments/b66p8x/whos_your_favorite_cnbc_person_to_listen_to_and/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Daytrading/comments/b4yakq/margin_trading_is_it_really_risky/'," datetime.datetime(2019, 3, 28, 13, 17, 14, 137559)",'2019-03-28',None,u'6','Daytrading',u'Margin trading - is it really risky?'," u'https://old.reddit.com/r/Daytrading/comments/b4yakq/margin_trading_is_it_really_risky/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.core.engine:Closing spider (finished)
2019-03-28 13:17:44 [scrapy.core.engine] INFO: Closing spider (finished)
INFO:scrapy.statscollectors:Dumping Scrapy stats:
"
https://old.reddit.com/r/wallstreetbets/comments/b6hfg1/daily_discussion_thread_march_28_2019/'," datetime.datetime(2019, 3, 28, 13, 28, 30, 465682)",'2019-03-28',u'Daily Discussion',u'32','wallstreetbets',"u'Daily Discussion Thread - March 28, 2019'"," u'https://old.reddit.com/r/wallstreetbets/comments/b6hfg1/daily_discussion_thread_march_28_2019/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b6hf4k/mcdonalds_is_going_forward_with_the_300_million/'," datetime.datetime(2019, 3, 28, 13, 33, 33, 705531)",'2019-03-28',u'Discussion',u'35','wallstreetbets',u'McDonald\u2019s is going forward with the $300 million acquisition of a tech company'," u'https://techleak.video.blog/2019/03/28/mcdonalds-is-going-forward-with-the-300-million-acquisition-of-a-tech-company/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b6f2v4/any_interesting_case_studies_on_companiesinvesting/'," datetime.datetime(2019, 3, 28, 13, 39, 27, 973417)",'2019-03-28',None,u'1','investing',u'Any interesting case studies on companies/investing?'," u'https://old.reddit.com/r/investing/comments/b6f2v4/any_interesting_case_studies_on_companiesinvesting/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b67vt7/smta_smallcap_reit_analysis_and_discussion/'," datetime.datetime(2019, 3, 28, 13, 46, 14, 500713)",'2019-03-28',u'Discussion',u'4','stocks',u'SMTA: Small-cap REIT analysis and discussion'," u'https://old.reddit.com/r/stocks/comments/b67vt7/smta_smallcap_reit_analysis_and_discussion/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/StockMarket/new/> (referer: None)
2019-03-28 13:46:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/StockMarket/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b69k6o/post_your_holdings_not_share_amounts_or_dollar/'," datetime.datetime(2019, 3, 28, 13, 56, 12, 883800)",'2019-03-28',None,u'12','stocks',"u'Post your holdings. Not share amounts or dollar amounts, just tickers'"," u'https://old.reddit.com/r/stocks/comments/b69k6o/post_your_holdings_not_share_amounts_or_dollar/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b6hp8r/bluelion_has_a_plan/'," datetime.datetime(2019, 3, 28, 14, 23, 39, 91217)",'2019-03-28',u'Shitpost',u'8','wallstreetbets',u'BlueLion has a plan!'," u'https://old.reddit.com/r/wallstreetbets/comments/b6hp8r/bluelion_has_a_plan/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b6hvdf/i_indexed_beckys_portfolio_so_you_dont_have_to/'," datetime.datetime(2019, 3, 28, 14, 28, 33, 409235)",'2019-03-28',u'DD',u'242','wallstreetbets',"u""I indexed Becky's portfolio so you don't have to"""," u'https://old.reddit.com/r/wallstreetbets/comments/b6hvdf/i_indexed_beckys_portfolio_so_you_dont_have_to/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b6hjdh/short_icelands_economy/'," datetime.datetime(2019, 3, 28, 14, 28, 41, 583329)",'2019-03-28',None,u'2','wallstreetbets',u'Short Iceland\u2019s economy'," u'https://www.theglobeandmail.com/amp/business/article-icelands-wow-air-ceases-operations-leaving-passengers-stranded/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b69jl2/lyft_ups_expected_ipo_price_to_between_70_and_72/'," datetime.datetime(2019, 3, 28, 14, 31, 49, 592135)",'2019-03-28',None,u'100','stocks',u'Lyft ups expected IPO price to between $70 and $72 a share'," u'https://old.reddit.com/r/stocks/comments/b69jl2/lyft_ups_expected_ipo_price_to_between_70_and_72/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-28 14:32:19 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
INFO:scrapy.extensions.logstats:Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-28 14:32:19 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b6a1v3/i_sold_all_my_stocks_now_what/'," datetime.datetime(2019, 3, 28, 14, 33, 13, 636977)",'2019-03-28',None,u'0','stocks',"u'I sold all my stocks, now what?'"," u'https://old.reddit.com/r/stocks/comments/b6a1v3/i_sold_all_my_stocks_now_what/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b6g0ct/how_do_you_guys_deal_with_loss/'," datetime.datetime(2019, 3, 28, 14, 37, 28, 52672)",'2019-03-28',None,u'8','investing',u'How do you guys deal with loss?'," u'https://old.reddit.com/r/investing/comments/b6g0ct/how_do_you_guys_deal_with_loss/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b69tvg/best_products_for_traders_interviews_with_the/'," datetime.datetime(2019, 3, 28, 14, 37, 45, 756840)",'2019-03-28',None,u'0','stocks',"u""Best products for traders - Interviews with the world's best traders and investors"""," u'https://old.reddit.com/r/stocks/comments/b69tvg/best_products_for_traders_interviews_with_the/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b6hzpx/share_buybacks_hit_their_highest_level_in_history/'," datetime.datetime(2019, 3, 28, 14, 38, 26, 490482)",'2019-03-28',u'Discussion',u'4','wallstreetbets',u'Share buybacks hit their highest level in history last year. Does this mean the current market value is inflated?'," u'https://www.cnbc.com/2019/03/25/share-buybacks-soar-to-a-record-topping-800-billion-bigger-than-a-facebook-or-exxon-mobil.html'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b6flbg/message_board_for_tesla_stock/'," datetime.datetime(2019, 3, 28, 14, 41, 30, 148876)",'2019-03-28',None,u'0','investing',u'Message board for tesla stock?'," u'https://old.reddit.com/r/investing/comments/b6flbg/message_board_for_tesla_stock/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b6hwwr/trump_is_tired_of_zucks_shit_asks_his/'," datetime.datetime(2019, 3, 28, 14, 43, 29, 162839)",'2019-03-28',u'Fundamentals',u'66','wallstreetbets',"u""Trump is tired of Zuck's shit, asks his administration team to sue them for discriminatory advertising practices"""," u'https://www.cnbc.com/2019/03/28/trump-administration-sues-facebook-over-discriminatory-advertising-practices.html'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b6grho/investing_in_shopify_stores/'," datetime.datetime(2019, 3, 28, 14, 43, 55, 887394)",'2019-03-28',None,u'1','investing',u'Investing in shopify stores'," u'https://old.reddit.com/r/investing/comments/b6grho/investing_in_shopify_stores/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b6f39l/made_money_in_futures_lost_it_all_in_stocks_tax/'," datetime.datetime(2019, 3, 28, 14, 45, 32, 182333)",'2019-03-28',None,u'2','investing',"u'Made money in futures, lost it all in stocks... TAX question?'"," u'https://old.reddit.com/r/investing/comments/b6f39l/made_money_in_futures_lost_it_all_in_stocks_tax/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b6gfgy/is_it_a_good_timing_for_fed_to_shrink_10_yr/'," datetime.datetime(2019, 3, 28, 14, 47, 57, 871066)",'2019-03-28',None,u'4','investing',u'is it a good timing for fed to shrink 10 yr treasury holding in portfolio?'," u'https://old.reddit.com/r/investing/comments/b6gfgy/is_it_a_good_timing_for_fed_to_shrink_10_yr/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b6h4v2/portfolio_analysis_tracking/'," datetime.datetime(2019, 3, 28, 14, 49, 18, 729736)",'2019-03-28',u'Education',u'4','investing',u'Portfolio Analysis Tracking'," u'https://old.reddit.com/r/investing/comments/b6h4v2/portfolio_analysis_tracking/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-28 14:49:49 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b6gv19/why_are_etfs_here_so_popular/'," datetime.datetime(2019, 3, 28, 14, 49, 52, 264777)",'2019-03-28',None,u'2','investing',u'Why are ETFs here so popular?'," u'https://old.reddit.com/r/investing/comments/b6gv19/why_are_etfs_here_so_popular/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b6hii3/what_stocks_to_watch_for_resurgence_of_nuclear/'," datetime.datetime(2019, 3, 28, 15, 3, 53, 408585)",'2019-03-28',None,u'3','investing',u'What stocks to watch for resurgence of nuclear power?'," u'https://old.reddit.com/r/investing/comments/b6hii3/what_stocks_to_watch_for_resurgence_of_nuclear/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b6a3xc/how_do_holdings_factor_into_a_market_valuation/'," datetime.datetime(2019, 3, 28, 15, 4, 43, 449423)",'2019-03-28',None,u'0','stocks',u'How do holdings factor into a market valuation?'," u'https://old.reddit.com/r/stocks/comments/b6a3xc/how_do_holdings_factor_into_a_market_valuation/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b6hl5a/thoughts_on_abiomed_abmd/'," datetime.datetime(2019, 3, 28, 15, 4, 48, 865092)",'2019-03-28',None,u'0','investing',u'Thoughts on ABIOMED (ABMD)'," u'https://old.reddit.com/r/investing/comments/b6hl5a/thoughts_on_abiomed_abmd/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b6hx6b/is_there_a_site_that_compiles_annual_shareholder/'," datetime.datetime(2019, 3, 28, 15, 5, 40, 586667)",'2019-03-28',None,u'0','investing',u'Is there a site that compiles annual shareholder letters for companies?'," u'https://old.reddit.com/r/investing/comments/b6hx6b/is_there_a_site_that_compiles_annual_shareholder/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-28 15:06:10 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b6hycp/robinhood_investing_app_takes_on_wall_street/'," datetime.datetime(2019, 3, 28, 15, 12, 14, 296947)",'2019-03-28',u'News',u'0','investing',u'Robinhood investing app takes on Wall Street'," u'https://old.reddit.com/r/investing/comments/b6hycp/robinhood_investing_app_takes_on_wall_street/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b6i2vc/chats_rooms_about_options_and_trading/'," datetime.datetime(2019, 3, 28, 15, 12, 25, 68522)",'2019-03-28',u'Discussion',u'5','wallstreetbets',u'Chats rooms about options and trading'," u'https://old.reddit.com/r/wallstreetbets/comments/b6i2vc/chats_rooms_about_options_and_trading/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b6i674/wow_airlines_downfall/'," datetime.datetime(2019, 3, 28, 15, 51, 3, 687869)",'2019-03-28',None,u'6','investing',u'WOW Airlines Downfall'," u'https://old.reddit.com/r/investing/comments/b6i674/wow_airlines_downfall/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-28 15:51:34 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b6ibwg/ways_of_measuring_volatility/'," datetime.datetime(2019, 3, 28, 15, 51, 33, 534253)",'2019-03-28',None,u'1','investing',u'Ways of measuring volatility?'," u'https://old.reddit.com/r/investing/comments/b6ibwg/ways_of_measuring_volatility/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b6i8bl/very_important_that_opec_increase_the_flow_of_oil/'," datetime.datetime(2019, 3, 28, 15, 53, 30, 221618)",'2019-03-28',u'DD',u'61','wallstreetbets',"u'\u201cVery important that OPEC increase the flow of Oil. World Markets are fragile, price of Oil getting too high.\u201d'"," u'https://old.reddit.com/r/wallstreetbets/comments/b6i8bl/very_important_that_opec_increase_the_flow_of_oil/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b6if9j/real_state_crowdfunding/'," datetime.datetime(2019, 3, 28, 15, 56, 29, 924613)",'2019-03-28',u'Help',u'0','investing',u'Real State crowdfunding'," u'https://old.reddit.com/r/investing/comments/b6if9j/real_state_crowdfunding/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b6iblh/4q_gdp_revised_down_22/'," datetime.datetime(2019, 3, 28, 16, 8, 33, 162230)",'2019-03-28',None,u'24','wallstreetbets',u'4Q GDP revised down 2.2%'," u'https://www.marketscreener.com/S-P-500-4985/news/U-S-fourth-quarter-GDP-revised-down-profits-weak-28253194/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b62qao/survey_investment_performance_expectations/'," datetime.datetime(2019, 3, 28, 16, 25, 8, 163077)",'2019-03-28',None,u'1','StockMarket',u'Survey: Investment Performance Expectations'," u'https://old.reddit.com/r/StockMarket/comments/b62qao/survey_investment_performance_expectations/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b6bdpz/averaging_down_nio_and_qrtea/'," datetime.datetime(2019, 3, 28, 16, 38, 25, 814920)",'2019-03-28',None,u'0','stocks',u'Averaging Down: NIO and QRTEA'," u'https://old.reddit.com/r/stocks/comments/b6bdpz/averaging_down_nio_and_qrtea/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-28 16:38:56 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b6ih6q/low_effort_meme_lulu_today/'," datetime.datetime(2019, 3, 28, 16, 48, 36, 592431)",'2019-03-28',u'Meme',u'25','wallstreetbets',u'(Low Effort Meme) $Lulu today'," u'https://old.reddit.com/r/wallstreetbets/comments/b6ih6q/low_effort_meme_lulu_today/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b6ip0l/billiondollar_startup_zoom_filed_to_go_public_and/'," datetime.datetime(2019, 3, 28, 16, 59, 47, 412218)",'2019-03-28',None,u'806','investing',"u'Billion-dollar startup Zoom filed to go public \u2014 and shares of a totally unrelated company also called called Zoom shot up 1,100% (Business Insider)'"," u'https://old.reddit.com/r/investing/comments/b6ip0l/billiondollar_startup_zoom_filed_to_go_public_and/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b6j0gi/10_bagger_or_na/'," datetime.datetime(2019, 3, 28, 17, 13, 39, 21296)",'2019-03-28',u'Gain',u'42','wallstreetbets',u'10 bagger or na'," u'http://imgur.com/a/Uj4uOtN'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
2019-03-28 17:14:09 [scrapy.core.scraper] ERROR: Error processing "
https://old.reddit.com/r/investing/comments/b6j588/cplg_a_value_bet_on_midscale_hotels/'," datetime.datetime(2019, 3, 28, 17, 30, 29, 819640)",'2019-03-28',u'Discussion',u'2','investing',u'$CPLG - A value bet on midscale hotels'," u'https://old.reddit.com/r/investing/comments/b6j588/cplg_a_value_bet_on_midscale_hotels/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b63ly2/i_got_an_invite_to_become_a_broker_trainee_for_an/'," datetime.datetime(2019, 3, 28, 17, 31, 18, 775306)",'2019-03-28',None,u'21','StockMarket',"u""I got an invite to become a Broker Trainee for an Interdealer Broker Firm, but I'm not from the industry."""," u'https://old.reddit.com/r/StockMarket/comments/b63ly2/i_got_an_invite_to_become_a_broker_trainee_for_an/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-28 17:31:48 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b6j8pp/yoga_pants_gods_have_heard_my_prayers/'," datetime.datetime(2019, 3, 28, 17, 38, 32, 577348)",'2019-03-28',u'Gain',u'144','wallstreetbets',u'Yoga Pants gods have heard my prayers'," u'https://old.reddit.com/r/wallstreetbets/comments/b6j8pp/yoga_pants_gods_have_heard_my_prayers/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b6j4hx/is_it_possible_for_retail_investors_to_get/'," datetime.datetime(2019, 3, 28, 17, 38, 34, 928560)",'2019-03-28',u'Discussion',u'0','wallstreetbets',u'Is it possible for retail investors to get substantial exposure to self driving car businesses?'," u'https://old.reddit.com/r/wallstreetbets/comments/b6j4hx/is_it_possible_for_retail_investors_to_get/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b6jijq/turkey_burns_through_a_third_of_foreign_reserves/'," datetime.datetime(2019, 3, 28, 17, 42, 57, 482966)",'2019-03-28',u'Discussion',u'10','wallstreetbets',u'Turkey burns through a third of foreign reserves as lira plummets 5%'," u'https://coinrivet.com/turkey-burns-through-a-third-of-foreign-reserves-as-lira-plummets-5/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/thewallstreet/comments/b3zdeu/deviations_poc_and_value_area_for_friday_march_22/'," datetime.datetime(2019, 3, 28, 17, 45, 44, 87731)",'2019-03-28',None,u'9','thewallstreet',"u'Deviations, POC, and Value Area for Friday, March 22, 2019'"," u'https://i.imgur.com/Jyu5AW2.png'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-28 17:46:14 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b6jduo/thank_you_fidelity_for_the_strong_msft_dd_very/'," datetime.datetime(2019, 3, 28, 17, 48, 0, 309369)",'2019-03-28',u'Shitpost',u'32','wallstreetbets',"u'Thank you Fidelity for the strong $MSFT DD, very cool!'"," u'https://old.reddit.com/r/wallstreetbets/comments/b6jduo/thank_you_fidelity_for_the_strong_msft_dd_very/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b6d456/thoughts_on_prmtx/'," datetime.datetime(2019, 3, 28, 17, 54, 26, 34168)",'2019-03-28',None,u'0','stocks',u'Thoughts on PRMTX'," u'https://old.reddit.com/r/stocks/comments/b6d456/thoughts_on_prmtx/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b6n6vs/remember_how_aapl_ran_up_before_unveiling_their/'," datetime.datetime(2019, 3, 28, 18, 0, 2, 626550)",'2019-03-28',u'Discussion',u'9','wallstreetbets',u'Remember how $AAPL ran up before unveiling their streaming service? Well Disney ($DIS) unveils their\u2019s 4/11. Not overpriced right now.'," u'https://www.barrons.com/articles/buy-disney-stock-before-its-investor-day-says-analyst-51553784472'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
2019-03-28 18:00:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b6n4fw/memes_are_more_real_than_my_profits/'," datetime.datetime(2019, 3, 28, 18, 0, 32, 880116)",'2019-03-28',u'Meme',u'61','wallstreetbets',u'Memes are more real than my profits'," u'https://imgur.com/55M91TV.jpg'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 2 pages (at 2 pages/min), scraped 0 items (at 0 items/min)
2019-03-28 18:01:03 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 2 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b6mudk/bcky_update_wsb_was_right_diversification_bad/'," datetime.datetime(2019, 3, 28, 18, 1, 3, 137876)",'2019-03-28',u'Stocks',u'16','wallstreetbets',u'($BCKY Update) WSB was right: diversification = bad'," u'https://old.reddit.com/r/wallstreetbets/comments/b6mudk/bcky_update_wsb_was_right_diversification_bad/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
2019-03-28 18:01:33 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b6mgca/we_all_know_becky_etf_but_what_about_the_becky_fd/'," datetime.datetime(2019, 3, 28, 21, 2, 0, 375019)",'2019-03-28',u'Gain',u'40','wallstreetbets',"u'We all know $BECKY ETF, but what about the $BECKY FD ETF'"," u'https://old.reddit.com/r/wallstreetbets/comments/b6mgca/we_all_know_becky_etf_but_what_about_the_becky_fd/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b6ngyy/t_ba_cost_v_if_you_had_to_drop_one_for_a_blind_10/'," datetime.datetime(2019, 3, 29, 9, 23, 42, 770326)",'2019-03-29',None,u'0','investing',"u'$T, $BA, $COST, $V; If you had to DROP one for a blind 10 year old, which would you drop?'"," u'https://old.reddit.com/r/investing/comments/b6ngyy/t_ba_cost_v_if_you_had_to_drop_one_for_a_blind_10/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b6m36l/a_tale_of_hubris_and_autism/'," datetime.datetime(2019, 3, 29, 9, 24, 13, 226471)",'2019-03-29',u'Meme',u'95','wallstreetbets',u'a tale of hubris and autism'," u'https://old.reddit.com/r/wallstreetbets/comments/b6m36l/a_tale_of_hubris_and_autism/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 2 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-29 09:24:43 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b6m2n0/beautiful_double_top_at_tmus_short/'," datetime.datetime(2019, 3, 29, 9, 25, 13, 518153)",'2019-03-29',u'Technicals',u'0','wallstreetbets',u'Beautiful Double Top at $TMUS #short'," u'https://old.reddit.com/r/wallstreetbets/comments/b6m2n0/beautiful_double_top_at_tmus_short/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 2 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-29 09:25:43 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b6vvi1/robinhood_snacks_is_a_pos_and_wont_let_me/'," datetime.datetime(2019, 3, 29, 9, 25, 33, 54330)",'2019-03-29',None,u'0','wallstreetbets',u'Robinhood Snacks is a pos and won\u2019t let me unsubscribe!!'," u'https://old.reddit.com/r/wallstreetbets/comments/b6vvi1/robinhood_snacks_is_a_pos_and_wont_let_me/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 2 pages (at 2 pages/min), scraped 0 items (at 0 items/min)
2019-03-29 09:26:03 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 2 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b6mzwu/with_all_this_doom_and_gloom_talk_of_an_upcoming/'," datetime.datetime(2019, 3, 29, 9, 25, 44, 127378)",'2019-03-29',None,u'0','investing',"u'With all this doom and gloom talk of an upcoming recession, in your opinion, what do you consider to be ""recession-proof"" companies?'"," u'https://old.reddit.com/r/investing/comments/b6mzwu/with_all_this_doom_and_gloom_talk_of_an_upcoming/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b6lwl8/guess_china_came_out_on_top_of_the_trade_deal/'," datetime.datetime(2019, 3, 29, 9, 26, 14, 59089)",'2019-03-29',u'Shitpost',u'19','wallstreetbets',u'Guess China came out on top of the trade deal.'," u'https://old.reddit.com/r/wallstreetbets/comments/b6lwl8/guess_china_came_out_on_top_of_the_trade_deal/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 2 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-29 09:26:44 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
INFO:scrapy.extensions.logstats:Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-29 09:26:44 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
INFO:scrapy.extensions.logstats:Crawled 8 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-29 09:26:44 [scrapy.extensions.logstats] INFO: Crawled 8 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b6lucs/buffett_on_the_economy_it_looks_like_things_have/'," datetime.datetime(2019, 3, 29, 9, 27, 14, 630053)",'2019-03-29',u'Discussion',u'5','wallstreetbets',"u'Buffett on the economy: ""It looks like things have slowed down""'"," u'https://old.reddit.com/r/wallstreetbets/comments/b6lucs/buffett_on_the_economy_it_looks_like_things_have/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 2 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-29 09:27:44 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
DEBUG:scrapy.downloadermiddlewares.retry:Retrying <GET https://old.reddit.com/r/investing/new/> (failed 1 times): User timeout caused connection failure: Getting https://old.reddit.com/r/investing/new/ took longer than 180.0 seconds..
2019-03-29 09:27:44 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://old.reddit.com/r/investing/new/> (failed 1 times): User timeout caused connection failure: Getting https://old.reddit.com/r/investing/new/ took longer than 180.0 seconds..
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b6ldl7/teva_thoughts/'," datetime.datetime(2019, 3, 29, 9, 28, 15, 174811)",'2019-03-29',u'Discussion',u'5','wallstreetbets',u'$TEVA Thoughts'," u'https://old.reddit.com/r/wallstreetbets/comments/b6ldl7/teva_thoughts/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b6mvek/company_buying_subordinate_shares/'," datetime.datetime(2019, 3, 29, 9, 28, 15, 556470)",'2019-03-29',None,u'1','investing',u'Company Buying Subordinate Shares'," u'https://old.reddit.com/r/investing/comments/b6mvek/company_buying_subordinate_shares/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b6kndw/little_money_how_to_invest/'," datetime.datetime(2019, 3, 29, 9, 28, 45, 459174)",'2019-03-29',u'Advice Request',u'1','stocks',"u'Little money, how to invest'"," u'https://old.reddit.com/r/stocks/comments/b6kndw/little_money_how_to_invest/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b6l8ib/fnma_slow_and_steady_wall_street_sees_hurdles_for/'," datetime.datetime(2019, 3, 29, 9, 29, 15, 789726)",'2019-03-29',u'Fundamentals',u'4','wallstreetbets',"u""FNMA: Slow and Steady: Wall Street Sees Hurdles for Trump's \u2018Vague\u2019 Fannie-Freddie Plan"""," u'https://www.bloomberg.com/news/articles/2019-03-28/wall-street-sees-hurdles-for-trump-s-vague-fannie-freddie-plan?srnd=premium'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 2 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-29 09:29:46 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
DEBUG:scrapy.downloadermiddlewares.retry:Retrying <GET https://old.reddit.com/r/ValueInvesting/new/> (failed 1 times): User timeout caused connection failure: Getting https://old.reddit.com/r/ValueInvesting/new/ took longer than 180.0 seconds..
2019-03-29 09:29:46 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://old.reddit.com/r/ValueInvesting/new/> (failed 1 times): User timeout caused connection failure: Getting https://old.reddit.com/r/ValueInvesting/new/ took longer than 180.0 seconds..
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b6k6zy/buying_stocks_during_a_recession/'," datetime.datetime(2019, 3, 29, 9, 29, 46, 107265)",'2019-03-29',u'Question',u'0','stocks',u'Buying Stocks During a Recession?'," u'https://old.reddit.com/r/stocks/comments/b6k6zy/buying_stocks_during_a_recession/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b6tdyp/china_regulator_will_set_time_limits_on_popular/'," datetime.datetime(2019, 3, 29, 9, 30, 5, 560453)",'2019-03-29',u'DD',u'0','wallstreetbets',u'China regulator will set time limits on popular short-video apps'," u'https://old.reddit.com/r/wallstreetbets/comments/b6tdyp/china_regulator_will_set_time_limits_on_popular/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Daytrading/new/> from <GET https://old.reddit.com/r/daytrading/new/>
2019-03-29 09:30:35 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Daytrading/new/> from <GET https://old.reddit.com/r/daytrading/new/>
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b6l68m/just_bought_1000_shares_of_tsla_at_276/'," datetime.datetime(2019, 3, 29, 9, 30, 16, 499798)",'2019-03-29',u'YOLO',u'30','wallstreetbets',u'Just bought 1000 shares of tsla at 276'," u'https://old.reddit.com/r/wallstreetbets/comments/b6l68m/just_bought_1000_shares_of_tsla_at_276/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 2 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-29 09:30:46 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
DEBUG:scrapy.downloadermiddlewares.retry:Retrying <GET https://old.reddit.com/r/thewallstreet/new/> (failed 1 times): User timeout caused connection failure: Getting https://old.reddit.com/r/thewallstreet/new/ took longer than 180.0 seconds..
2019-03-29 09:30:46 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://old.reddit.com/r/thewallstreet/new/> (failed 1 times): User timeout caused connection failure: Getting https://old.reddit.com/r/thewallstreet/new/ took longer than 180.0 seconds..
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b6mv17/im_trying_to_invest_with_fundamental_analysis_but/'," datetime.datetime(2019, 3, 29, 9, 30, 46, 908388)",'2019-03-29',u'Help',u'1','investing',"u""I'm trying to invest with fundamental analysis but everyone keeps telling its useless?"""," u'https://old.reddit.com/r/investing/comments/b6mv17/im_trying_to_invest_with_fundamental_analysis_but/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 5 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-29 09:31:17 [scrapy.extensions.logstats] INFO: Crawled 5 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
DEBUG:scrapy.downloadermiddlewares.retry:Retrying <GET https://old.reddit.com/r/investing/new/> (failed 2 times): User timeout caused connection failure: Getting https://old.reddit.com/r/investing/new/ took longer than 180.0 seconds..
2019-03-29 09:31:17 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://old.reddit.com/r/investing/new/> (failed 2 times): User timeout caused connection failure: Getting https://old.reddit.com/r/investing/new/ took longer than 180.0 seconds..
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b6jtaq/can_i_get_some_recommendations_on_what_to_buy/'," datetime.datetime(2019, 3, 29, 9, 31, 17, 201527)",'2019-03-29',None,u'0','stocks',u'Can I get some recommendations on what to buy?'," u'https://old.reddit.com/r/stocks/comments/b6jtaq/can_i_get_some_recommendations_on_what_to_buy/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
2019-03-29 09:31:47 [scrapy.core.scraper] ERROR: Error processing "
https://old.reddit.com/r/investing/comments/b6msnr/leveraging_up_the_permanent_portfolio/'," datetime.datetime(2019, 3, 29, 9, 33, 18, 428720)",'2019-03-29',None,u'1','investing',u'Leveraging up the permanent portfolio'," u'https://old.reddit.com/r/investing/comments/b6msnr/leveraging_up_the_permanent_portfolio/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
2019-03-29 09:33:48 [scrapy.core.scraper] ERROR: Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b6kmrj/wto_has_ruled_that_the_us_failed_to_remove_some/'," datetime.datetime(2019, 3, 29, 9, 34, 19, 80519)",'2019-03-29',u'Stocks',u'20','wallstreetbets',u'WTO has ruled that the US failed to remove some illegal subsidies on Boeing (BA)'," u'https://old.reddit.com/r/wallstreetbets/comments/b6kmrj/wto_has_ruled_that_the_us_failed_to_remove_some/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b6meer/best_stock_picks_for_development_and/'," datetime.datetime(2019, 3, 29, 9, 35, 49, 944885)",'2019-03-29',None,u'1','investing',u'Best stock picks for development and implementation of 5G?'," u'https://old.reddit.com/r/investing/comments/b6meer/best_stock_picks_for_development_and/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 8 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-29 09:36:20 [scrapy.extensions.logstats] INFO: Crawled 8 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b6jq8x/1953_stock_certificate_company_still_exists_but/'," datetime.datetime(2019, 3, 29, 9, 36, 20, 167016)",'2019-03-29',None,u'1','stocks',u'1953 Stock Certificate - Company still exists but has gone private'," u'https://old.reddit.com/r/stocks/comments/b6jq8x/1953_stock_certificate_company_still_exists_but/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b6k3gl/wto_is_said_to_plan_a_ruling_on_using_national/'," datetime.datetime(2019, 3, 29, 9, 36, 50, 471941)",'2019-03-29',u'Discussion',u'3','wallstreetbets',u'WTO is said to plan a ruling on using national security as a basis for setting tariffs'," u'https://old.reddit.com/r/wallstreetbets/comments/b6k3gl/wto_is_said_to_plan_a_ruling_on_using_national/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 8 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-29 09:37:20 [scrapy.extensions.logstats] INFO: Crawled 8 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b6mbov/has_anyone_heard_about_futurenet/'," datetime.datetime(2019, 3, 29, 9, 38, 21, 314720)",'2019-03-29',None,u'1','investing',u'Has anyone heard about FutureNet?'," u'https://old.reddit.com/r/investing/comments/b6mbov/has_anyone_heard_about_futurenet/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b6irqi/amazon_says_it_will_bring_800_new_tech_jobs_to/'," datetime.datetime(2019, 3, 29, 9, 38, 51, 539438)",'2019-03-29',None,u'24','stocks',u'Amazon says it will bring 800 new tech jobs to Austin'," u'https://old.reddit.com/r/stocks/comments/b6irqi/amazon_says_it_will_bring_800_new_tech_jobs_to/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
DEBUG:scrapy.downloadermiddlewares.retry:Retrying <GET https://old.reddit.com/r/thewallstreet/new/> (failed 2 times): User timeout caused connection failure: Getting https://old.reddit.com/r/thewallstreet/new/ took longer than 180.0 seconds..
2019-03-29 09:39:21 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://old.reddit.com/r/thewallstreet/new/> (failed 2 times): User timeout caused connection failure: Getting https://old.reddit.com/r/thewallstreet/new/ took longer than 180.0 seconds..
INFO:scrapy.core.engine:Closing spider (finished)
2019-03-29 09:39:21 [scrapy.core.engine] INFO: Closing spider (finished)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b6k2j5/bullish_nominating_kudlow_to_a_vp_position_in_the/'," datetime.datetime(2019, 3, 29, 9, 39, 21, 895816)",'2019-03-29',u'Shitpost',u'4','wallstreetbets',"u""Bullish. Nominating Kudlow to a VP position in the company I work for. He'd fit right in."""," u'https://old.reddit.com/r/wallstreetbets/comments/b6k2j5/bullish_nominating_kudlow_to_a_vp_position_in_the/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b6m4rc/what_are_your_favorite_technologydigital_reits/'," datetime.datetime(2019, 3, 29, 9, 40, 52, 514351)",'2019-03-29',u'Discussion',u'0','investing',u'What are your favorite technology/digital REITs?'," u'https://old.reddit.com/r/investing/comments/b6m4rc/what_are_your_favorite_technologydigital_reits/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 5 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-29 09:41:22 [scrapy.extensions.logstats] INFO: Crawled 5 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b6igxl/best_stock_to_buy_now_for_a_day_trade/'," datetime.datetime(2019, 3, 29, 9, 41, 22, 864794)",'2019-03-29',None,u'0','stocks',u'Best stock to buy now for a day trade'," u'https://old.reddit.com/r/stocks/comments/b6igxl/best_stock_to_buy_now_for_a_day_trade/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b6m1v5/buffett_on_the_economy_it_looks_like_things_have/'," datetime.datetime(2019, 3, 29, 9, 43, 24, 127842)",'2019-03-29',None,u'24','investing',"u""Buffett on the economy: 'It looks like things have slowed down'"""," u'https://old.reddit.com/r/investing/comments/b6m1v5/buffett_on_the_economy_it_looks_like_things_have/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b6i71m/mcdonalds_shares_purchase_1989/'," datetime.datetime(2019, 3, 29, 9, 43, 54, 375178)",'2019-03-29',None,u'7','stocks',u'McDonalds shares purchase 1989'," u'https://old.reddit.com/r/stocks/comments/b6i71m/mcdonalds_shares_purchase_1989/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
2019-03-29 09:44:24 [scrapy.core.scraper] ERROR: Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b6jtpu/ladybaybee_recommended_stock_picks_20190328/'," datetime.datetime(2019, 3, 29, 9, 44, 23, 947724)",'2019-03-29',u'Stocks',u'3','wallstreetbets',u'Ladybaybee recommended stock picks 2019-03-28'," u'https://imgur.com/a/a85EwpP'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-29 09:44:54 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b6m19x/my_lyft_shares_are_locked_up_for_6_month/'," datetime.datetime(2019, 3, 29, 9, 45, 55, 751232)",'2019-03-29',None,u'4','investing',u'My Lyft shares are locked up for 6 month'," u'https://old.reddit.com/r/investing/comments/b6m19x/my_lyft_shares_are_locked_up_for_6_month/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b6i5y4/how_severe_will_the_impact_of_brexit_be/'," datetime.datetime(2019, 3, 29, 9, 46, 26, 88445)",'2019-03-29',u'Question',u'2','stocks',u'How severe will the impact of Brexit be?'," u'https://old.reddit.com/r/stocks/comments/b6i5y4/how_severe_will_the_impact_of_brexit_be/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b6lsxx/jpmorgan_chase_ceo_jamie_dimon_on_jobs_tariffs/'," datetime.datetime(2019, 3, 29, 9, 48, 27, 277280)",'2019-03-29',None,u'2','investing',"u'JPMorgan Chase CEO Jamie Dimon on jobs, tariffs and politics'"," u'https://old.reddit.com/r/investing/comments/b6lsxx/jpmorgan_chase_ceo_jamie_dimon_on_jobs_tariffs/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b6jsvr/this_guy_seems_to_be_the_sports_version_of_rwsb/'," datetime.datetime(2019, 3, 29, 9, 49, 26, 719433)",'2019-03-29',u'Shitpost',u'33','wallstreetbets',u'This guy seems to be the sports version of /r/wsb. Throwing down $10k on Mich-TTU tonight to get out of the hole.'," u'https://www.reddit.com/r/sportsbook/comments/b6dd3d/ncaabb_daily_discussion_32819_thursday/ejkcmwi/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-29 09:49:56 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b6hzy3/wow_airlines_downfall/'," datetime.datetime(2019, 3, 29, 9, 51, 28, 843587)",'2019-03-29',None,u'1','stocks',u'WOW Airlines downfall'," u'https://old.reddit.com/r/stocks/comments/b6hzy3/wow_airlines_downfall/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
DEBUG:scrapy.downloadermiddlewares.retry:Gave up retrying <GET https://old.reddit.com/r/thewallstreet/new/> (failed 3 times): User timeout caused connection failure: Getting https://old.reddit.com/r/thewallstreet/new/ took longer than 180.0 seconds..
2019-03-29 09:51:59 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET https://old.reddit.com/r/thewallstreet/new/> (failed 3 times): User timeout caused connection failure: Getting https://old.reddit.com/r/thewallstreet/new/ took longer than 180.0 seconds..
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b6lpiw/why_has_a_vietnam_etf_like_vnm_done_so_poorly/'," datetime.datetime(2019, 3, 29, 9, 53, 29, 14936)",'2019-03-29',None,u'10','investing',"u""Why has a Vietnam ETF like VNM done so poorly over the last few years while vietnam's economy has been growing well?"""," u'https://old.reddit.com/r/investing/comments/b6lpiw/why_has_a_vietnam_etf_like_vnm_done_so_poorly/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b6hysj/rstocks_daily_discussion_thursday_mar_28_2019/'," datetime.datetime(2019, 3, 29, 9, 53, 59, 298519)",'2019-03-29',None,u'1','stocks',"u'r/Stocks Daily Discussion Thursday - Mar 28, 2019'"," u'https://old.reddit.com/r/stocks/comments/b6hysj/rstocks_daily_discussion_thursday_mar_28_2019/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b6jky6/this_weeks_meme_theme/'," datetime.datetime(2019, 3, 29, 9, 54, 29, 604371)",'2019-03-29',u'Shitpost',u'1','wallstreetbets',u'This weeks meme theme'," u'https://youtu.be/BW-pFoYgaUs'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-29 09:54:59 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b6li3b/new_jersey_jury_clears_jj_of_liability_in_latest/'," datetime.datetime(2019, 3, 29, 9, 58, 31, 859040)",'2019-03-29',None,u'2','investing',u'New Jersey jury clears J&J of liability in latest talc cancer trial'," u'https://old.reddit.com/r/investing/comments/b6li3b/new_jersey_jury_clears_jj_of_liability_in_latest/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b6hrsh/what_happen_to_stock_when_it_gets_merged/'," datetime.datetime(2019, 3, 29, 9, 59, 2, 141189)",'2019-03-29',None,u'0','stocks',u'What happen to stock when it gets merged?'," u'https://old.reddit.com/r/stocks/comments/b6hrsh/what_happen_to_stock_when_it_gets_merged/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b6jkwc/as_requested_the_amd_stuff_from_aws_summit/'," datetime.datetime(2019, 3, 29, 9, 59, 32, 485337)",'2019-03-29',u'Discussion',u'47','wallstreetbets',"u'as requested, the $AMD stuff from AWS Summit Silicon Valley yesterday... long $AMD in AWS'"," u'https://imgur.com/a/rhnBo5r'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
2019-03-29 10:00:02 [scrapy.core.scraper] ERROR: Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b6v0qw/kl_may_30_puts/'," datetime.datetime(2019, 3, 29, 10, 1, 3, 373671)",'2019-03-29',None,u'1','wallstreetbets',u'KL May 30 puts'," u'https://old.reddit.com/r/wallstreetbets/comments/b6v0qw/kl_may_30_puts/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
2019-03-29 10:01:33 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b63uee/investing_in_artificial_intelligence/'," datetime.datetime(2019, 3, 29, 10, 2, 4, 658554)",'2019-03-29',None,u'3','StockMarket',u'Investing in artificial intelligence'," u'https://old.reddit.com/r/StockMarket/comments/b63uee/investing_in_artificial_intelligence/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-29 10:02:35 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b6lhqz/splk_neutral_on_this_one_for_now/'," datetime.datetime(2019, 3, 29, 10, 3, 34, 850194)",'2019-03-29',None,u'0','investing',u'$SPLK Neutral on this one for now'," u'https://old.reddit.com/r/investing/comments/b6lhqz/splk_neutral_on_this_one_for_now/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b6uzv3/taxes/'," datetime.datetime(2019, 3, 29, 10, 3, 44, 321607)",'2019-03-29',None,u'0','investing',u'Taxes'," u'https://old.reddit.com/r/investing/comments/b6uzv3/taxes/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b6gyuj/where_to_steer_during_downturnspossible_recession/'," datetime.datetime(2019, 3, 29, 10, 4, 5, 101341)",'2019-03-29',None,u'13','stocks',u'Where to steer during downturns/possible recession'," u'https://old.reddit.com/r/stocks/comments/b6gyuj/where_to_steer_during_downturnspossible_recession/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b6j7ir/chipotle_vs_walmart/'," datetime.datetime(2019, 3, 29, 10, 5, 5, 988230)",'2019-03-29',None,u'0','investing',u'Chipotle vs Walmart'," u'https://old.reddit.com/r/investing/comments/b6j7ir/chipotle_vs_walmart/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-29 10:05:36 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b6lho4/tool_request_portfolio_transactions_logger/'," datetime.datetime(2019, 3, 29, 10, 8, 7, 857489)",'2019-03-29',u'Help',u'0','investing',u'Tool Request: Portfolio transactions logger'," u'https://old.reddit.com/r/investing/comments/b6lho4/tool_request_portfolio_transactions_logger/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b6eqck/can_someone_help_me_understand_why_technical/'," datetime.datetime(2019, 3, 29, 10, 8, 38, 148191)",'2019-03-29',u'Question',u'4','stocks',u'Can someone help me understand why technical analysis is needed in the stock market?'," u'https://old.reddit.com/r/stocks/comments/b6eqck/can_someone_help_me_understand_why_technical/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b63z24/gcgx_news_coming_as_per_ceo_ceo_emailed_back_to/'," datetime.datetime(2019, 3, 29, 10, 8, 38, 976080)",'2019-03-29',None,u'2','StockMarket',u'$GCGX News coming as per CEO. CEO emailed back to shareholder saying they are in process of uplisting to higher exchange with updates coming.'," u'https://old.reddit.com/r/StockMarket/comments/b63z24/gcgx_news_coming_as_per_ceo_ceo_emailed_back_to/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b6ktmj/what_are_the_good_reasons_to_becoming_a_trader/'," datetime.datetime(2019, 3, 29, 10, 12, 40, 330546)",'2019-03-29',None,u'4','investing',u'What are the good reasons to becoming a trader?'," u'https://old.reddit.com/r/investing/comments/b6ktmj/what_are_the_good_reasons_to_becoming_a_trader/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b6e92h/i_know_now_nike_is_a_falling_dumpster_fire_going/'," datetime.datetime(2019, 3, 29, 10, 13, 10, 596850)",'2019-03-29',None,u'0','stocks',u'I know now nike is a falling dumpster fire going down hill on the rollercoaster.'," u'https://old.reddit.com/r/stocks/comments/b6e92h/i_know_now_nike_is_a_falling_dumpster_fire_going/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b6kdkf/sp_top_25_and_bottom_25_of_the_sp_500_since_the/'," datetime.datetime(2019, 3, 29, 10, 17, 12, 958957)",'2019-03-29',None,u'19','investing',"u""S&P: Top 25 and Bottom 25 of the S&P 500 since the equities crash of '09"""," u'https://old.reddit.com/r/investing/comments/b6kdkf/sp_top_25_and_bottom_25_of_the_sp_500_since_the/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b6e0p4/mcdonalds_will_no_longer_lobby_against_minimum/'," datetime.datetime(2019, 3, 29, 10, 17, 43, 228597)",'2019-03-29',None,u'409','stocks',"u""McDonald's will no longer lobby against minimum wage hikes"""," u'https://old.reddit.com/r/stocks/comments/b6e0p4/mcdonalds_will_no_longer_lobby_against_minimum/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b650fh/my_version_of_irk_trades_how_i_use_intrinistic/'," datetime.datetime(2019, 3, 29, 10, 18, 43, 804392)",'2019-03-29',None,u'2','StockMarket',u'My version of irk trades/ how i use intrinistic stock values'," u'https://old.reddit.com/r/StockMarket/comments/b650fh/my_version_of_irk_trades_how_i_use_intrinistic/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b64fwo/brokerageadvisor_hell/'," datetime.datetime(2019, 3, 29, 10, 21, 15, 512298)",'2019-03-29',None,u'3','StockMarket',u'Brokerage/Advisor Hell'," u'https://old.reddit.com/r/StockMarket/comments/b64fwo/brokerageadvisor_hell/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b6k14u/bank_preferred_vs_common/'," datetime.datetime(2019, 3, 29, 10, 21, 45, 566007)",'2019-03-29',u'Discussion',u'0','investing',u'Bank preferred vs. common'," u'https://old.reddit.com/r/investing/comments/b6k14u/bank_preferred_vs_common/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Daytrading/comments/b50fbk/amd_mu_aapl_trades_recapping_biggest_losers_and/'," datetime.datetime(2019, 3, 29, 10, 21, 46, 386811)",'2019-03-29',None,u'1','Daytrading',"u'$AMD $MU $AAPL Trades, Recapping biggest losers and winners from the week'"," u'https://www.youtube.com/watch?v=Bun0UsQ5TBc&t=18s'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-29 10:22:16 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
INFO:scrapy.core.engine:Closing spider (finished)
2019-03-29 10:22:16 [scrapy.core.engine] INFO: Closing spider (finished)
INFO:scrapy.statscollectors:Dumping Scrapy stats:
"
https://old.reddit.com/r/StockMarket/comments/b654k3/pertaining_to_speculation_about_an_incoming/'," datetime.datetime(2019, 3, 29, 10, 23, 16, 42387)",'2019-03-29',None,u'1','StockMarket',"u'Pertaining to speculation about an incoming recession, how are you guys planning and directing your investments? Any tips for fellow redditors/investors?'"," u'https://old.reddit.com/r/StockMarket/comments/b654k3/pertaining_to_speculation_about_an_incoming/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Stock_Picks/comments/avs35f/suggestions_for_long_term_investments/'," datetime.datetime(2019, 3, 29, 10, 25, 17, 764117)",'2019-03-29',u'Discussion',u'1','Stock_Picks',u'Suggestions For Long Term Investments'," u'https://old.reddit.com/r/Stock_Picks/comments/avs35f/suggestions_for_long_term_investments/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-29 10:25:48 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b6jnjp/still_no_way_for_normal_investors_like_myself_to/'," datetime.datetime(2019, 3, 29, 10, 25, 47, 924243)",'2019-03-29',None,u'1','investing',u'Still no way for normal investors (like myself) to take part in IPOs?'," u'https://old.reddit.com/r/investing/comments/b6jnjp/still_no_way_for_normal_investors_like_myself_to/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b6jjfm/why_is_the_dow_still_relevant/'," datetime.datetime(2019, 3, 29, 10, 29, 50, 234946)",'2019-03-29',u'Discussion',u'22','investing',u'Why is the DOW still relevant?'," u'https://old.reddit.com/r/investing/comments/b6jjfm/why_is_the_dow_still_relevant/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Daytrading/comments/b51ek0/steven_dux_software/'," datetime.datetime(2019, 3, 29, 10, 33, 23, 113364)",'2019-03-29',None,u'1','Daytrading',u'Steven Dux software'," u'https://old.reddit.com/r/Daytrading/comments/b51ek0/steven_dux_software/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.core.engine:Closing spider (finished)
2019-03-29 10:33:54 [scrapy.core.engine] INFO: Closing spider (finished)
INFO:scrapy.statscollectors:Dumping Scrapy stats:
"
https://old.reddit.com/r/investing/comments/b6jicg/the_african_continent_is_projected_to_experience/'," datetime.datetime(2019, 3, 29, 10, 33, 52, 491163)",'2019-03-29',None,u'25','investing',"u""The African continent is projected to experience an explosive population growth over the next 30 years. Which companies are best positioned to meet the needs of this new, stabilizing population? How would you invest in Africa's future?"""," u'https://old.reddit.com/r/investing/comments/b6jicg/the_african_continent_is_projected_to_experience/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b6jahb/question_on_the_logic_behind_bond_valuation_why/'," datetime.datetime(2019, 3, 29, 10, 37, 55, 253970)",'2019-03-29',None,u'1','investing',"u'Question on the logic behind bond valuation: why do 1-year US treasury bonds have higher yields than Municipals, Corporate (AAA) 1-year bonds, and European treasury bonds. Seems counterintuitive.'"," u'https://old.reddit.com/r/investing/comments/b6jahb/question_on_the_logic_behind_bond_valuation_why/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/thewallstreet/comments/b4f9eq/daily_spx_tpos_03222019/'," datetime.datetime(2019, 3, 29, 10, 40, 56, 942101)",'2019-03-29',None,u'26','thewallstreet',"u""Daily SPX TPO's 03-22-2019"""," u'https://i.imgur.com/NBhVPAE.png'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/ValueInvesting/comments/axzyis/where_in_the_world_is_jim_chanos_tesla_analysis/'," datetime.datetime(2019, 3, 29, 10, 41, 27, 854716)",'2019-03-29',None,u'3','ValueInvesting',u'Where in the World is Jim Chanos? (Tesla analysis)'," u'https://cleantechnica.com/2019/02/10/where-in-the-world-is-jim-chanos-tesla-pravduh-king/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-29 10:41:58 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b6660q/why_is_the_market_down_today/'," datetime.datetime(2019, 3, 29, 10, 41, 58, 244107)",'2019-03-29',None,u'1','StockMarket',u'Why is the market down today?'," u'https://old.reddit.com/r/StockMarket/comments/b6660q/why_is_the_market_down_today/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b65qkj/weed_stocks_taking_a_downturn_what_are_your/'," datetime.datetime(2019, 3, 29, 10, 45, 30, 17234)",'2019-03-29',None,u'87','StockMarket',u'Weed stocks taking a downturn. What are your opinions on its future prices?'," u'https://old.reddit.com/r/StockMarket/comments/b65qkj/weed_stocks_taking_a_downturn_what_are_your/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/thewallstreet/comments/b49ph5/post_market_discussion_march_22/'," datetime.datetime(2019, 3, 29, 10, 48, 1, 309671)",'2019-03-29',u'Daily',u'10','thewallstreet',u'Post Market Discussion - (March 22)'," u'https://old.reddit.com/r/thewallstreet/comments/b49ph5/post_market_discussion_march_22/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-29 10:48:31 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Daytrading/comments/b5nfk4/how_do_you_pay_the_bid_ask_spread/'," datetime.datetime(2019, 3, 29, 10, 48, 31, 668847)",'2019-03-29',None,u'10','Daytrading',u'How do you pay the bid ask spread?'," u'https://old.reddit.com/r/Daytrading/comments/b5nfk4/how_do_you_pay_the_bid_ask_spread/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/thewallstreet/comments/b43zf3/daily_discussion_march_22/'," datetime.datetime(2019, 3, 29, 10, 50, 34, 217789)",'2019-03-29',u'Daily',u'12','thewallstreet',u'Daily Discussion - (March 22)'," u'https://old.reddit.com/r/thewallstreet/comments/b43zf3/daily_discussion_march_22/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Daytrading/comments/b5l8ej/am_i_really_daytrading_if_im_just_flipping_shares/'," datetime.datetime(2019, 3, 29, 10, 51, 4, 926259)",'2019-03-29',None,u'1','Daytrading',u'Am I really daytrading if I\u2019m just flipping shares on Robinhood using mobile? Looking for advice'," u'https://old.reddit.com/r/Daytrading/comments/b5l8ej/am_i_really_daytrading_if_im_just_flipping_shares/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-29 10:51:35 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Trading/comments/az1whm/what_kind_of_trader_am_i/'," datetime.datetime(2019, 3, 29, 10, 51, 35, 138034)",'2019-03-29',None,u'1','Trading',u'What kind of trader am I?'," u'https://old.reddit.com/r/Trading/comments/az1whm/what_kind_of_trader_am_i/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/thewallstreet/comments/b40ygi/daily_spx_tpos_03212019/'," datetime.datetime(2019, 3, 29, 10, 52, 35, 968643)",'2019-03-29',None,u'18','thewallstreet',"u""Daily SPX TPO's 03-21-2019"""," u'https://i.imgur.com/dECfz1S.png'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Daytrading/comments/b5f404/what_is_a_beat_chart/'," datetime.datetime(2019, 3, 29, 10, 53, 6, 247882)",'2019-03-29',None,u'1','Daytrading',"u'What is a ""beat chart""'"," u'https://old.reddit.com/r/Daytrading/comments/b5f404/what_is_a_beat_chart/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Trading/comments/az05um/want_to_quit_a_job_to_trade_full_time_think_twice/'," datetime.datetime(2019, 3, 29, 10, 53, 36, 499272)",'2019-03-29',None,u'7','Trading',u'Want to quit a job to trade full time? Think twice!'," u'https://old.reddit.com/r/Trading/comments/az05um/want_to_quit_a_job_to_trade_full_time_think_twice/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-29 10:54:06 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Daytrading/comments/b5dyeh/fairly_new_to_daytrading_couple_questions_and/'," datetime.datetime(2019, 3, 29, 10, 54, 37, 151468)",'2019-03-29',None,u'9','Daytrading',u'Fairly new to daytrading. Couple questions and advice?'," u'https://old.reddit.com/r/Daytrading/comments/b5dyeh/fairly_new_to_daytrading_couple_questions_and/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Trading/comments/aysnbv/your_preffered_market_to_trade/'," datetime.datetime(2019, 3, 29, 10, 55, 7, 440321)",'2019-03-29',None,u'3','Trading',u'Your preffered market to trade?'," u'https://old.reddit.com/r/Trading/comments/aysnbv/your_preffered_market_to_trade/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-29 10:55:37 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Daytrading/comments/b57mw0/how_to_find_stocks_while_paper_trading_during/'," datetime.datetime(2019, 3, 29, 10, 56, 7, 980210)",'2019-03-29',None,u'7','Daytrading',u'How to find stocks while paper trading during off-market hours?'," u'https://old.reddit.com/r/Daytrading/comments/b57mw0/how_to_find_stocks_while_paper_trading_during/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-29 10:56:38 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
INFO:scrapy.core.engine:Closing spider (finished)
2019-03-29 10:56:38 [scrapy.core.engine] INFO: Closing spider (finished)
INFO:scrapy.statscollectors:Dumping Scrapy stats:
"
https://old.reddit.com/r/stocks/comments/b6l3k8/movado_group_inc_nyse_mov_reports_fourth_quarter/'," datetime.datetime(2019, 3, 29, 10, 57, 16, 616595)",'2019-03-29',None,u'2','stocks',"u'Movado Group, Inc. (NYSE: MOV) reports fourth quarter and fiscal year 2019 earnings results'"," u'https://old.reddit.com/r/stocks/comments/b6l3k8/movado_group_inc_nyse_mov_reports_fourth_quarter/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Daytrading/comments/b6ufmm/question_about_brokers_and_trading_habits/'," datetime.datetime(2019, 3, 29, 11, 17, 1, 360228)",'2019-03-29',None,u'1','Daytrading',u'Question about brokers and trading habits'," u'https://www.reddit.com/r/personalfinance/comments/b6u97n/question_about_brokers_and_trading_habits/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b6o96q/mcdonalds_will_no_longer_lobby_against_minimum/'," datetime.datetime(2019, 3, 29, 11, 23, 54, 141455)",'2019-03-29',u'Stocks',u'24','wallstreetbets',"u""McDonald's will no longer lobby against minimum wage hikes - CNN"""," u'https://www.cnn.com/2019/03/27/business/mcdonalds-minimum-wage/index.html'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b6oxbr/bb_anyone_on_this/'," datetime.datetime(2019, 3, 29, 11, 38, 45, 191098)",'2019-03-29',u'Discussion',u'7','wallstreetbets',u'$BB Anyone on this?'," u'https://old.reddit.com/r/wallstreetbets/comments/b6oxbr/bb_anyone_on_this/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b6ocjw/lyft_ipo_at_72/'," datetime.datetime(2019, 3, 29, 11, 39, 22, 165187)",'2019-03-29',u'Stocks',u'26','wallstreetbets',u'LYFT IPO at $72'," u'https://old.reddit.com/r/wallstreetbets/comments/b6ocjw/lyft_ipo_at_72/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b6obkb/what_are_your_moves_tomorrow_march_29/'," datetime.datetime(2019, 3, 29, 11, 43, 47, 799917)",'2019-03-29',u'Daily Discussion',u'46','wallstreetbets',"u'What Are Your Moves Tomorrow, March 29'"," u'https://old.reddit.com/r/wallstreetbets/comments/b6obkb/what_are_your_moves_tomorrow_march_29/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b6ofqe/circle_of_life/'," datetime.datetime(2019, 3, 29, 11, 43, 48, 93177)",'2019-03-29',u'Meme',u'267','wallstreetbets',u'Circle of Life'," u'https://old.reddit.com/r/wallstreetbets/comments/b6ofqe/circle_of_life/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b6lefp/rstocks_options_trading_thursday_mar_28_2019/'," datetime.datetime(2019, 3, 29, 11, 52, 53, 281214)",'2019-03-29',None,u'2','stocks',"u'r/Stocks Options Trading Thursday - Mar 28, 2019'"," u'https://old.reddit.com/r/stocks/comments/b6lefp/rstocks_options_trading_thursday_mar_28_2019/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b6pcqw/investment_strategy_vs_economic_cycle/'," datetime.datetime(2019, 3, 29, 11, 57, 44, 462010)",'2019-03-29',None,u'2','investing',u'Investment strategy vs economic cycle'," u'https://old.reddit.com/r/investing/comments/b6pcqw/investment_strategy_vs_economic_cycle/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b6lm7y/gilead_and_galapagos/'," datetime.datetime(2019, 3, 29, 12, 1, 58, 754980)",'2019-03-29',None,u'10','stocks',u'Gilead and Galapagos'," u'https://old.reddit.com/r/stocks/comments/b6lm7y/gilead_and_galapagos/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-29 12:02:29 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b6lpbb/what_companies_are_on_your_watchlist_but_you_see/'," datetime.datetime(2019, 3, 29, 12, 5, 59, 554364)",'2019-03-29',None,u'4','stocks',"u'What companies are on your watchlist, but you see as overpriced?'"," u'https://old.reddit.com/r/stocks/comments/b6lpbb/what_companies_are_on_your_watchlist_but_you_see/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b6ql5w/about_the_right_broker/'," datetime.datetime(2019, 3, 29, 12, 13, 15, 625013)",'2019-03-29',None,u'1','stocks',u'about the right Broker'," u'https://old.reddit.com/r/stocks/comments/b6ql5w/about_the_right_broker/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b67lvi/1000_to_invest/'," datetime.datetime(2019, 3, 29, 12, 15, 23, 605972)",'2019-03-29',None,u'1','StockMarket',u'$1000 to invest.'," u'https://old.reddit.com/r/StockMarket/comments/b67lvi/1000_to_invest/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b68rvw/which_is_your_favourite_weed_stock_and_why/'," datetime.datetime(2019, 3, 29, 12, 18, 56, 954472)",'2019-03-29',None,u'3','StockMarket',u'Which is your favourite Weed Stock and why?'," u'https://old.reddit.com/r/StockMarket/comments/b68rvw/which_is_your_favourite_weed_stock_and_why/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b69xnq/apple_aapl_technical_analysis_report/'," datetime.datetime(2019, 3, 29, 12, 20, 49, 586)",'2019-03-29',None,u'75','StockMarket',u'Apple (AAPL) \u2013 Technical Analysis Report'," u'https://old.reddit.com/r/StockMarket/comments/b69xnq/apple_aapl_technical_analysis_report/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b6vcoc/left_my_old_job_and_i_think_the_company_will_tank/'," datetime.datetime(2019, 3, 29, 12, 22, 20, 145563)",'2019-03-29',None,u'16','investing',"u'Left my old job and I think the company will tank (not because I left, but because of their strategy). Am I allowed to take a position against the company, given that I worked for them (and thus know a lot about their strategy)?'"," u'https://old.reddit.com/r/investing/comments/b6vcoc/left_my_old_job_and_i_think_the_company_will_tank/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b6pcu6/morgan_stanley_markets_large_stake_in_lululemon/'," datetime.datetime(2019, 3, 29, 12, 33, 27, 668611)",'2019-03-29',u'Stocks',u'4','wallstreetbets',u'Morgan Stanley Markets Large Stake in Lululemon'," u'https://finance.yahoo.com/news/morgan-stanley-markets-large-stake-211100512.html'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b6p0a7/wells_fargo_ceo_steps_down_wfc_up_2_ah/'," datetime.datetime(2019, 3, 29, 12, 34, 1, 29790)",'2019-03-29',u'Stocks',u'54','wallstreetbets',"u'Wells Fargo CEO steps down, $WFC up 2% AH'"," u'https://www.google.com/amp/s/www.washingtonpost.com/amphtml/business/2019/03/28/wells-fargo-ceo-tim-sloan-step-down-immediately/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Trading/comments/az3jxh/question_about_list_of_stocks_on_exchange/'," datetime.datetime(2019, 3, 29, 12, 45, 22, 678479)",'2019-03-29',None,u'2','Trading',u'Question about list of stocks on exchange'," u'https://old.reddit.com/r/Trading/comments/az3jxh/question_about_list_of_stocks_on_exchange/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Daytrading/comments/b5pml8/question_staging_lots_to_exit_a_position/'," datetime.datetime(2019, 3, 29, 12, 51, 14, 679727)",'2019-03-29',None,u'1','Daytrading',u'[Question] Staging lots to exit a position'," u'https://old.reddit.com/r/Daytrading/comments/b5pml8/question_staging_lots_to_exit_a_position/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-29 12:51:45 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
INFO:scrapy.core.engine:Closing spider (finished)
2019-03-29 12:51:45 [scrapy.core.engine] INFO: Closing spider (finished)
INFO:scrapy.statscollectors:Dumping Scrapy stats:
"
https://old.reddit.com/r/wallstreetbets/comments/b6r67s/new_chinese_meme_stonk_basic_dd_inside/'," datetime.datetime(2019, 3, 29, 13, 2, 27, 110365)",'2019-03-29',u'DD',u'65','wallstreetbets',u'New Chinese Meme Stonk (basic DD inside)'," u'https://old.reddit.com/r/wallstreetbets/comments/b6r67s/new_chinese_meme_stonk_basic_dd_inside/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b6r6yk/the_initial_gdp_report_is_not_a_good_arbiter_of/'," datetime.datetime(2019, 3, 29, 13, 2, 52, 504754)",'2019-03-29',u'Fundamentals',u'19','wallstreetbets',u'The initial GDP report is not a good arbiter of whether a recession is coming or even if it is underway.'," u'https://old.reddit.com/r/wallstreetbets/comments/b6r6yk/the_initial_gdp_report_is_not_a_good_arbiter_of/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b6q6qk/short_wmt_dd_inside/'," datetime.datetime(2019, 3, 29, 13, 7, 30, 368565)",'2019-03-29',u'Storytime',u'12','wallstreetbets',u'Short $WMT DD inside'," u'https://old.reddit.com/r/wallstreetbets/comments/b6q6qk/short_wmt_dd_inside/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b6qem2/does_anyone_know_the_tax_rule_on_capital_gains_if/'," datetime.datetime(2019, 3, 29, 13, 16, 55, 589245)",'2019-03-29',None,u'4','investing',u'Does anyone know the tax rule on capital gains if you reinvest repeatedly over the year?'," u'https://old.reddit.com/r/investing/comments/b6qem2/does_anyone_know_the_tax_rule_on_capital_gains_if/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b6rf1q/why_does_no_one_care_about_cvs_its_dirt_cheap/'," datetime.datetime(2019, 3, 29, 13, 18, 20, 684476)",'2019-03-29',u'Discussion',u'56','wallstreetbets',u'Why does no one care about CVS??? It\u2019s dirt cheap right now. Bought out the largest healthcare company in the country. You pay them for your insurance. They prescribe the medicine. Then they sell you that medicine and you pay for it with their insurance. Seems like a full proof system to me.'," u'https://old.reddit.com/r/wallstreetbets/comments/b6rf1q/why_does_no_one_care_about_cvs_its_dirt_cheap/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b6pzrm/elbow_shift_please_explain/'," datetime.datetime(2019, 3, 29, 13, 20, 0, 69471)",'2019-03-29',None,u'0','investing',u'Elbow shift -please explain'," u'https://old.reddit.com/r/investing/comments/b6pzrm/elbow_shift_please_explain/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b6qdbx/e_jones_bad_experience_investing/'," datetime.datetime(2019, 3, 29, 13, 20, 57, 913059)",'2019-03-29',None,u'0','investing',u'E Jones bad experience investing'," u'https://old.reddit.com/r/investing/comments/b6qdbx/e_jones_bad_experience_investing/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.utils.log:Scrapy 1.5.1 started (bot: redditcrawler)
2019-03-29 13:21:29 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: redditcrawler)
INFO:scrapy.utils.log:Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 2.7.14 |Anaconda, Inc.| (default, Dec  7 2017, 11:07:58) - [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)], pyOpenSSL 16.2.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Darwin-15.3.0-x86_64-64bit
2019-03-29 13:21:29 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 2.7.14 |Anaconda, Inc.| (default, Dec  7 2017, 11:07:58) - [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)], pyOpenSSL 16.2.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Darwin-15.3.0-x86_64-64bit
INFO:scrapy.crawler:Overridden settings: "
https://old.reddit.com/r/investing/comments/b6r2rr/utilities_and_healthcare/'," datetime.datetime(2019, 3, 29, 13, 22, 24, 571665)",'2019-03-29',None,u'2','investing',u'Utilities and Healthcare'," u'https://old.reddit.com/r/investing/comments/b6r2rr/utilities_and_healthcare/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b6rlzs/atvi_wtw_and_k_dd/'," datetime.datetime(2019, 3, 29, 13, 23, 21, 456140)",'2019-03-29',u'DD',u'5','wallstreetbets',"u'ATVI, WTW and K DD'"," u'https://old.reddit.com/r/wallstreetbets/comments/b6rlzs/atvi_wtw_and_k_dd/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b6s8vt/brics_nations_good_time_to_long_brazil_and_india/'," datetime.datetime(2019, 3, 29, 13, 24, 19, 179573)",'2019-03-29',u'Discussion',u'5','wallstreetbets',u'BRICS Nations: Good Time to Long Brazil and India?'," u'https://old.reddit.com/r/wallstreetbets/comments/b6s8vt/brics_nations_good_time_to_long_brazil_and_india/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b6sdyd/shouldnt_you_be_driving_something_nicer/'," datetime.datetime(2019, 3, 29, 13, 27, 14, 486985)",'2019-03-29',u'Meme',u'4572','wallstreetbets',u'Shouldn\u2019t you be driving something nicer?'," u'https://old.reddit.com/r/wallstreetbets/comments/b6sdyd/shouldnt_you_be_driving_something_nicer/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b6s4gi/lyft_ipo_tomorrow_72_per_share_loses_900mil_in/'," datetime.datetime(2019, 3, 29, 13, 28, 52, 227066)",'2019-03-29',u'Stocks',u'39','wallstreetbets',"u'Lyft IPO tomorrow! $72 per share, loses 900mil in 2018.'"," u'https://youtu.be/7gSK_6UFosM'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b6lr43/lost_in_translation_how_verbal_mishaps_and_lack/'," datetime.datetime(2019, 3, 29, 13, 29, 46, 188325)",'2019-03-29',u'News',u'2','stocks',u'Lost in translation? How verbal mishaps and lack of Chinese-language document threaten US-China trade deal'," u'https://old.reddit.com/r/stocks/comments/b6lr43/lost_in_translation_how_verbal_mishaps_and_lack/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b6nsmy/confused/'," datetime.datetime(2019, 3, 29, 13, 30, 14, 337230)",'2019-03-29',None,u'0','stocks',u'Confused'," u'https://old.reddit.com/r/stocks/comments/b6nsmy/confused/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b6sh1z/even_rh_knows/'," datetime.datetime(2019, 3, 29, 13, 30, 14, 512705)",'2019-03-29',u'Shitpost',u'167','wallstreetbets',u'Even RH knows...'," u'https://old.reddit.com/r/wallstreetbets/comments/b6sh1z/even_rh_knows/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/Daytrading/new/> (referer: None)
2019-03-29 13:30:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/Daytrading/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Daytrading/comments/b5qr3a/question_are_there_analytical_indicator_that_use/'," datetime.datetime(2019, 3, 29, 13, 30, 46, 350408)",'2019-03-29',None,u'1','Daytrading',u'[Question] Are there analytical indicator that use the tape?'," u'https://old.reddit.com/r/Daytrading/comments/b5qr3a/question_are_there_analytical_indicator_that_use/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b6nwh3/wells_fargo_ceo_tim_sloan_is_retiring_and_shares/'," datetime.datetime(2019, 3, 29, 13, 31, 3, 977354)",'2019-03-29',None,u'292','stocks',"u'Wells Fargo CEO Tim Sloan is retiring, and shares jump'"," u'https://old.reddit.com/r/stocks/comments/b6nwh3/wells_fargo_ceo_tim_sloan_is_retiring_and_shares/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b6m4hi/what_are_you_favorite_technologydigital_reits/'," datetime.datetime(2019, 3, 29, 13, 34, 47, 66199)",'2019-03-29',u'Discussion',u'0','stocks',u'What are you favorite technology/digital REITS?'," u'https://old.reddit.com/r/stocks/comments/b6m4hi/what_are_you_favorite_technologydigital_reits/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b6skhk/panic_and_profit/'," datetime.datetime(2019, 3, 29, 13, 36, 14, 351422)",'2019-03-29',u'Shitpost',u'143','wallstreetbets',u'Panic and profit'," u'https://old.reddit.com/r/wallstreetbets/comments/b6skhk/panic_and_profit/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/StockMarket/new/> (referer: None)
2019-03-29 13:36:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/StockMarket/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Stock_Picks/comments/avuccp/insurance_stocks_plunge_as_medicare_for_all_bill/'," datetime.datetime(2019, 3, 29, 13, 44, 43, 997950)",'2019-03-29',u'News',u'22','Stock_Picks',u'Insurance Stocks Plunge as Medicare for All Bill Unveiled With Major Democratic Support'," u'https://www.commondreams.org/news/2019/02/28/insurance-stocks-plunge-medicare-all-bill-unveiled-major-democratic-support'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-29 13:45:14 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
INFO:scrapy.core.engine:Closing spider (finished)
2019-03-29 13:45:14 [scrapy.core.engine] INFO: Closing spider (finished)
INFO:scrapy.statscollectors:Dumping Scrapy stats:
"
https://old.reddit.com/r/stocks/comments/b6p683/buying_growth_stocks_is_it_all_just_greater_fool/'," datetime.datetime(2019, 3, 29, 13, 50, 10, 946748)",'2019-03-29',None,u'1','stocks',"u'Buying Growth Stocks: Is It All Just ""Greater Fool"" Theory?'"," u'https://old.reddit.com/r/stocks/comments/b6p683/buying_growth_stocks_is_it_all_just_greater_fool/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b6ascq/peter_thiel_on_moats_valuation_how_to_think/'," datetime.datetime(2019, 3, 29, 14, 0, 8, 886128)",'2019-03-29',None,u'3','StockMarket',"u'Peter Thiel on Moats, Valuation & How to Think'"," u'https://old.reddit.com/r/StockMarket/comments/b6ascq/peter_thiel_on_moats_valuation_how_to_think/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-29 14:00:39 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b6ae7f/i_have_2000_to_invest/'," datetime.datetime(2019, 3, 29, 14, 3, 10, 515363)",'2019-03-29',None,u'1','StockMarket',"u'I have $2,000 to invest'"," u'https://old.reddit.com/r/StockMarket/comments/b6ae7f/i_have_2000_to_invest/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-29 14:03:40 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b6pcnt/i_have_very_little_money_in_my_ira_account_but_a/'," datetime.datetime(2019, 3, 29, 14, 3, 40, 768227)",'2019-03-29',u'Discussion',u'1','stocks',u'I have very little money in my IRA account but a lot in my normal account. I want to invest in two REITs. How bad of an idea is it to get the REITs in my normal account instead of my IRA?'," u'https://old.reddit.com/r/stocks/comments/b6pcnt/i_have_very_little_money_in_my_ira_account_but_a/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b6t5za/lawsuit_filed_against_boeing_in_illinois_by/'," datetime.datetime(2019, 3, 29, 14, 9, 43, 943580)",'2019-03-29',u'Stocks',u'13','wallstreetbets',u'Lawsuit filed against Boeing in Illinois by estate of Rwandan victim over Ethiopian Airlines crash'," u'https://www.usnews.com/news/best-states/illinois/articles/2019-03-28/lawsuit-filed-against-boeing-over-crash-in-ethiopia'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/Daytrading/new/> (referer: None)
2019-03-29 14:10:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/Daytrading/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b6c2u6/rstockmarket_april_2019_stock_picking_contest/'," datetime.datetime(2019, 3, 29, 14, 29, 8, 427814)",'2019-03-29',u'Contest',u'1','StockMarket',u'r/StockMarket April 2019 Stock Picking Contest'," u'https://docs.google.com/forms/d/e/1FAIpQLSdN2qYwZILLoxEM2cC6plIvV7NcBpsKd1nhqTtFrWTiXxQNuA/viewform?usp=pp_url'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b6t9p4/china_bending_the_knee_again_china_considering/'," datetime.datetime(2019, 3, 29, 14, 38, 47, 641153)",'2019-03-29',u'Futures',u'18','wallstreetbets',u'China bending the knee again?! China considering letting cloud computing concessions'," u'https://www.wsj.com/articles/china-floats-cloud-concession-to-foreign-tech-firms-in-u-s-trade-talks-11553773127?mod=e2tw'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b6c8l6/2000_to_invest_short_term_investing/'," datetime.datetime(2019, 3, 29, 14, 47, 38, 892886)",'2019-03-29',None,u'3','StockMarket',u'$2000 to invest. short term investing.'," u'https://old.reddit.com/r/StockMarket/comments/b6c8l6/2000_to_invest_short_term_investing/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b6wfda/update_2jp_morgan_nomura_get_nod_for_china/'," datetime.datetime(2019, 3, 29, 14, 53, 18, 69164)",'2019-03-29',None,u'3','stocks',"u'UPDATE 2-JP Morgan, Nomura get nod for China brokerage JVs as market access expands'"," u'https://old.reddit.com/r/stocks/comments/b6wfda/update_2jp_morgan_nomura_get_nod_for_china/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b6ctxp/stt_thoughts/'," datetime.datetime(2019, 3, 29, 15, 7, 37, 830039)",'2019-03-29',None,u'0','StockMarket',u'Stt - \U0001f4ad thoughts?'," u'https://old.reddit.com/r/StockMarket/comments/b6ctxp/stt_thoughts/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-29 15:08:08 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b6yay2/wells_fargo_wfc_is_in_advanced_talks_to_sell_its/'," datetime.datetime(2019, 3, 29, 15, 10, 38, 990106)",'2019-03-29',u'Discussion',u'2','wallstreetbets',"u'Wells Fargo (WFC) is in advanced talks to sell its real estate brokerage, Eastdil'"," u'https://old.reddit.com/r/wallstreetbets/comments/b6yay2/wells_fargo_wfc_is_in_advanced_talks_to_sell_its/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/Trading/new/> (referer: None)
2019-03-29 15:11:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/Trading/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b6ywtc/friday_shitpost/'," datetime.datetime(2019, 3, 29, 15, 11, 40, 201010)",'2019-03-29',u'Shitpost',u'122','wallstreetbets',u'Friday Shitpost'," u'https://imgur.com/a/VnYjqjW'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-03-29 15:12:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b6vwj1/daily_discussion_thread_march_29_2019/'," datetime.datetime(2019, 3, 29, 15, 12, 48, 134439)",'2019-03-29',u'Daily Discussion',u'37','wallstreetbets',"u'Daily Discussion Thread - March 29, 2019'"," u'https://old.reddit.com/r/wallstreetbets/comments/b6vwj1/daily_discussion_thread_march_29_2019/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b6y74t/rh_news_algo_has_the_boomer_market_locked/'," datetime.datetime(2019, 3, 29, 15, 13, 10, 357800)",'2019-03-29',u'Shitpost',u'49','wallstreetbets',u'RH News Algo has the Boomer Market Locked'," u'https://old.reddit.com/r/wallstreetbets/comments/b6y74t/rh_news_algo_has_the_boomer_market_locked/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/Stock_Picks/new/> (referer: None)
2019-03-29 15:13:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/Stock_Picks/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b6yqe4/when_its_2_hours_into_open_and_the_sauce_hasnt/'," datetime.datetime(2019, 3, 29, 15, 13, 41, 263827)",'2019-03-29',u'Meme',u'304','wallstreetbets',"u""When its 2 hours into open and the sauce hasn't dropped"""," u'https://old.reddit.com/r/wallstreetbets/comments/b6yqe4/when_its_2_hours_into_open_and_the_sauce_hasnt/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/thewallstreet/new/> (referer: None)
2019-03-29 15:14:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/thewallstreet/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b6yfc7/thonking/'," datetime.datetime(2019, 3, 29, 15, 15, 42, 316417)",'2019-03-29',u'Spicy Meme',u'268','wallstreetbets',u'THONKING'," u'https://cdn.discordapp.com/attachments/556106261238644739/561204621058768917/unknown.png'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/Trading/new/> (referer: None)
2019-03-29 15:16:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/Trading/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b6y3up/lyft_snap_explained/'," datetime.datetime(2019, 3, 29, 15, 16, 11, 869913)",'2019-03-29',u'Discussion',u'11','wallstreetbets',u'$LYFT ....... $SNAP explained?'," u'https://old.reddit.com/r/wallstreetbets/comments/b6y3up/lyft_snap_explained/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/StockMarket/new/> (referer: None)
2019-03-29 15:16:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/StockMarket/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b6vga3/netflix_bois/'," datetime.datetime(2019, 3, 29, 15, 17, 20, 425686)",'2019-03-29',u'Discussion',u'10','wallstreetbets',u'Netflix bois'," u'https://variety.com/2019/digital/news/netflix-brand-value-ranking-2019-1203175244/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b6ucgq/doesnt_matter_just_pump/'," datetime.datetime(2019, 3, 29, 15, 18, 25, 448279)",'2019-03-29',u'Shitpost',u'36','wallstreetbets',"u""Doesn't matter, just pump"""," u'https://old.reddit.com/r/wallstreetbets/comments/b6ucgq/doesnt_matter_just_pump/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-29 15:18:55 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b6wmty/tmobilesprint_merger_deal_in_danger_of_lawsuit/'," datetime.datetime(2019, 3, 29, 15, 19, 43, 891650)",'2019-03-29',u'Discussion',u'18','wallstreetbets',u'T-Mobile-Sprint merger deal in danger of Lawsuit from State attorneys'," u'https://techleak.video.blog/2019/03/29/t-mobile-sprint-merger-deal-in-danger-of-lawsuit-from-state-attorneys/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/Daytrading/new/> (referer: None)
2019-03-29 15:20:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/Daytrading/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b6tx9i/why_own_stonks_when_you_can_sling_contracts/'," datetime.datetime(2019, 3, 29, 15, 23, 28, 461274)",'2019-03-29',u'Shitpost',u'237','wallstreetbets',u'Why own stonks when you can sling contracts?'," u'https://old.reddit.com/r/wallstreetbets/comments/b6tx9i/why_own_stonks_when_you_can_sling_contracts/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-29 15:23:58 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b6zdh8/the_yield_curve_just_uninverted_again/'," datetime.datetime(2019, 3, 29, 15, 24, 5, 389834)",'2019-03-29',None,u'16','wallstreetbets',u'The Yield Curve just Uninverted Again!'," u'https://old.reddit.com/r/wallstreetbets/comments/b6zdh8/the_yield_curve_just_uninverted_again/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/investing/new/> (referer: None)
2019-03-29 15:24:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/investing/new/> (referer: None)
INFO:scrapy.extensions.logstats:Crawled 4 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2019-03-29 15:24:35 [scrapy.extensions.logstats] INFO: Crawled 4 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b6z5je/the_truth_hurts_boys/'," datetime.datetime(2019, 3, 29, 15, 25, 5, 911771)",'2019-03-29',u'Meme',u'121','wallstreetbets',u'The Truth Hurts Boys'," u'https://old.reddit.com/r/wallstreetbets/comments/b6z5je/the_truth_hurts_boys/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Daytrading/new/> from <GET https://old.reddit.com/r/daytrading/new/>
2019-03-29 15:25:36 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Daytrading/new/> from <GET https://old.reddit.com/r/daytrading/new/>
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b6yzmc/when_can_i_buy_puts_thank_god_that_its_another/'," datetime.datetime(2019, 3, 29, 15, 26, 37, 89179)",'2019-03-29',u'Shitpost',u'20','wallstreetbets',"u'""When can I buy puts?"" Thank God that its another week or there would be a lot of broke(r) boys in the group right now. Next week tho...'"," u'https://old.reddit.com/r/wallstreetbets/comments/b6yzmc/when_can_i_buy_puts_thank_god_that_its_another/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b6romz/im_interested_in_bonds_commodities_and_currencies/'," datetime.datetime(2019, 3, 29, 15, 30, 24, 964094)",'2019-03-29',None,u'1','investing',"u'I\u2019m interested in bonds, commodities, and currencies....'"," u'https://old.reddit.com/r/investing/comments/b6romz/im_interested_in_bonds_commodities_and_currencies/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b6cxhk/whos_your_favorite_cnbc_commentator_with_the_best/'," datetime.datetime(2019, 3, 29, 15, 41, 1, 260306)",'2019-03-29',None,u'1','StockMarket',"u""Who's your favorite cnbc commentator with the best opinions on stocks and the market?"""," u'https://old.reddit.com/r/StockMarket/comments/b6cxhk/whos_your_favorite_cnbc_commentator_with_the_best/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b6stzx/new_investor_and_filing_taxes/'," datetime.datetime(2019, 3, 29, 15, 55, 1, 357420)",'2019-03-29',None,u'0','investing',u'New investor and filing taxes'," u'https://old.reddit.com/r/investing/comments/b6stzx/new_investor_and_filing_taxes/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b6s5bw/is_it_a_bad_idea_to_hold_dividend_etfs_like_schd/'," datetime.datetime(2019, 3, 29, 15, 59, 3, 868289)",'2019-03-29',None,u'1','investing',u'Is it a bad idea to hold dividend ETFs like SCHD in a taxable account?'," u'https://old.reddit.com/r/investing/comments/b6s5bw/is_it_a_bad_idea_to_hold_dividend_etfs_like_schd/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b6pejg/potential_buys/'," datetime.datetime(2019, 3, 29, 16, 32, 51, 867666)",'2019-03-29',None,u'1','stocks',u'Potential buys?'," u'https://old.reddit.com/r/stocks/comments/b6pejg/potential_buys/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b6pxwn/share_your_dna_get_shares_startup_files_an/'," datetime.datetime(2019, 3, 29, 16, 49, 20, 184347)",'2019-03-29',None,u'5','stocks',"u'Share Your DNA, Get Shares: Startup Files an Unusual Offering'"," u'https://old.reddit.com/r/stocks/comments/b6pxwn/share_your_dna_get_shares_startup_files_an/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b6qal3/question_for_all_german_traders/'," datetime.datetime(2019, 3, 29, 16, 57, 46, 933681)",'2019-03-29',None,u'1','stocks',u'Question for all German traders'," u'https://old.reddit.com/r/stocks/comments/b6qal3/question_for_all_german_traders/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Daytrading/comments/b5slr9/what_brokers_do_you_recommend_for_a_small_account/'," datetime.datetime(2019, 3, 29, 17, 9, 15, 613407)",'2019-03-29',None,u'2','Daytrading',u'What brokers do you recommend for a small account'," u'https://old.reddit.com/r/Daytrading/comments/b5slr9/what_brokers_do_you_recommend_for_a_small_account/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    def process_item(self, item, spider):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b70p1r/they_remove_our_shitposts_yet_they_post_this/'," datetime.datetime(2019, 3, 29, 17, 21, 11, 79238)",'2019-03-29',u'Shitpost',u'10','wallstreetbets',u'They remove our Shitposts yet they post this:'," u'https://old.reddit.com/r/wallstreetbets/comments/b70p1r/they_remove_our_shitposts_yet_they_post_this/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 42, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/Stock_Picks/new/> (referer: None)
2019-03-29 17:21:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/Stock_Picks/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b6symw/mutual_funds_with_stakes_in_companies_preipo_or/'," datetime.datetime(2019, 3, 29, 17, 27, 56, 608024)",'2019-03-29',None,u'4','investing',u'Mutual funds with stakes in companies pre-IPO or in hedge funds'," u'https://old.reddit.com/r/investing/comments/b6symw/mutual_funds_with_stakes_in_companies_preipo_or/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    def process_item(self, item, spider):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b6suai/in_light_of_political_developments_is_healthcare/'," datetime.datetime(2019, 3, 29, 17, 31, 59, 139531)",'2019-03-29',None,u'8','investing',"u'In light of political developments, is healthcare at a significant risk going into the 2020 presidency?'"," u'https://old.reddit.com/r/investing/comments/b6suai/in_light_of_political_developments_is_healthcare/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    def process_item(self, item, spider):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b6zguq/wondering_why_your_pot_stocks_moved_today/'," datetime.datetime(2019, 3, 29, 17, 35, 5, 98186)",'2019-03-29',u'Discussion',u'17','wallstreetbets',u'Wondering why your pot stocks moved today?'," u'https://old.reddit.com/r/wallstreetbets/comments/b6zguq/wondering_why_your_pot_stocks_moved_today/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 29, 17, 35, 5, 98186) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
2019-03-29 17:35:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b6ec5u/i_now_know_nike_stock_is_a_dumpster_fire_falling/'," datetime.datetime(2019, 3, 29, 17, 37, 11, 623595)",'2019-03-29',None,u'0','StockMarket',u'I now know nike stock is a dumpster fire falling down the rollercoaster fast.'," u'https://old.reddit.com/r/StockMarket/comments/b6ec5u/i_now_know_nike_stock_is_a_dumpster_fire_falling/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    def process_item(self, item, spider):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b6sz2l/easy_profit_call_spread/'," datetime.datetime(2019, 3, 29, 17, 37, 57, 453101)",'2019-03-29',u'Education',u'1','investing',u'Easy profit call spread?'," u'https://old.reddit.com/r/investing/comments/b6sz2l/easy_profit_call_spread/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    def process_item(self, item, spider):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b6zo22/sec_posts_letter_from_melon_tusk_regarding/'," datetime.datetime(2019, 3, 29, 17, 40, 2, 419762)",'2019-03-29',u'Fundamentals',u'45','wallstreetbets',"u'SEC Posts letter from ""Melon Tusk"" regarding Quarterly Reporting'"," u'https://www.sec.gov/comments/s7-26-18/s72618-5240975-183709.pdf'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 29, 17, 40, 2, 419762) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
2019-03-29 17:40:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/thewallstreet/comments/b5242l/nightly_trading_discussion_march_2425/'," datetime.datetime(2019, 3, 29, 17, 42, 41, 718525)",'2019-03-29',u'Daily',u'4','thewallstreet',u'Nightly Trading Discussion - (March 24/25)'," u'https://old.reddit.com/r/thewallstreet/comments/b5242l/nightly_trading_discussion_march_2425/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    def process_item(self, item, spider):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b6t9c1/call_spread_with_0_risk_pls_help_me_understand/'," datetime.datetime(2019, 3, 29, 17, 47, 54, 773516)",'2019-03-29',None,u'0','investing',u'Call spread with 0 risk? pls help me understand'," u'https://old.reddit.com/r/investing/comments/b6t9c1/call_spread_with_0_risk_pls_help_me_understand/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    def process_item(self, item, spider):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b6zx44/ubs_banker_spied_on_eurostar_neighbor_to_crack_15/'," datetime.datetime(2019, 3, 29, 17, 50, 2, 331874)",'2019-03-29',u'Storytime',u'11','wallstreetbets',u'UBS Banker Spied on Eurostar Neighbor to Crack $15 Billion Deal'," u'https://www.bloomberg.com/news/articles/2019-03-28/ubs-banker-spied-on-eurostar-neighbor-to-crack-15-billion-deal?srnd=premium-europe'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 29, 17, 50, 2, 331874) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b6zsua/329_calls_and_puts/'," datetime.datetime(2019, 3, 29, 17, 50, 2, 335087)",'2019-03-29',u'Options',u'60','wallstreetbets',u'3/29 Calls and Puts'," u'https://old.reddit.com/r/wallstreetbets/comments/b6zsua/329_calls_and_puts/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 29, 17, 50, 2, 335087) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
2019-03-29 17:50:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b6qhui/does_anyone_buy_into_dot_com_bomb_new_bubble/'," datetime.datetime(2019, 3, 29, 17, 49, 46, 536412)",'2019-03-29',None,u'1','stocks',u'Does anyone buy into Dot Com Bomb new bubble theory?'," u'https://old.reddit.com/r/stocks/comments/b6qhui/does_anyone_buy_into_dot_com_bomb_new_bubble/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    def process_item(self, item, spider):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b71sfa/need_a_quick_10000/'," datetime.datetime(2019, 3, 29, 18, 0, 2, 289211)",'2019-03-29',None,u'2','wallstreetbets',"u'Need a quick $10,000'"," u'https://old.reddit.com/r/wallstreetbets/comments/b71sfa/need_a_quick_10000/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 29, 18, 0, 2, 289211) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b71nw6/forget_everything_you_remember_about_blackberry/'," datetime.datetime(2019, 3, 29, 18, 0, 2, 294318)",'2019-03-29',u'Gain',u'2','wallstreetbets',u'Forget everything you remember about BlackBerry'," u'https://www.cnn.com/2019/03/29/tech/blackberry-earnings-john-chen/index.html'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 29, 18, 0, 2, 294318) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b71kre/my_ta_is_saying_its_going_up_get_in_now/'," datetime.datetime(2019, 3, 29, 18, 0, 2, 296067)",'2019-03-29',u'Shitpost',u'5','wallstreetbets',"u""My TA is saying it's going up, get in now!"""," u'https://old.reddit.com/r/wallstreetbets/comments/b71kre/my_ta_is_saying_its_going_up_get_in_now/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 29, 18, 0, 2, 296067) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b71bv3/tendies_you_could_say/'," datetime.datetime(2019, 3, 29, 18, 0, 2, 299077)",'2019-03-29',u'Gain',u'6','wallstreetbets',u'Tendies you could say'," u'https://old.reddit.com/r/wallstreetbets/comments/b71bv3/tendies_you_could_say/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 29, 18, 0, 2, 299077) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b71a7p/three_hours_after_investing_in_a_startup/'," datetime.datetime(2019, 3, 29, 18, 0, 2, 300922)",'2019-03-29',u'Meme',u'285','wallstreetbets',u'Three hours after investing in a startup'," u'https://old.reddit.com/r/wallstreetbets/comments/b71a7p/three_hours_after_investing_in_a_startup/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 29, 18, 0, 2, 300922) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b71a1u/floating_mansion_starting_at_55_million_not_bad/'," datetime.datetime(2019, 3, 29, 18, 0, 2, 302535)",'2019-03-29',u'Shitpost',u'1','wallstreetbets',"u'Floating Mansion Starting at $5.5 million, Not Bad & Not A Yacht!!!'"," u'https://www.sacbee.com/news/state/california/article228548159.html'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 29, 18, 0, 2, 302535) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b719hl/it_all_starts_with_a_random_phone_call/'," datetime.datetime(2019, 3, 29, 18, 0, 2, 304135)",'2019-03-29',u'Discussion',u'0','wallstreetbets',u'It all starts with a random phone call...'," u'https://old.reddit.com/r/wallstreetbets/comments/b719hl/it_all_starts_with_a_random_phone_call/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 29, 18, 0, 2, 304135) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b71883/grubhub_long_hungry_laziness/'," datetime.datetime(2019, 3, 29, 18, 0, 2, 305756)",'2019-03-29',u'DD',u'8','wallstreetbets',u'GrubHub - long hungry laziness'," u'https://old.reddit.com/r/wallstreetbets/comments/b71883/grubhub_long_hungry_laziness/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 29, 18, 0, 2, 305756) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b717r1/every_3_minutes_larry_kudlow_forgets_how_to/'," datetime.datetime(2019, 3, 29, 18, 0, 2, 309172)",'2019-03-29',u'Meme',u'45','wallstreetbets',u'Every 3 Minutes Larry Kudlow Forgets how to Economist'," u'https://old.reddit.com/r/wallstreetbets/comments/b717r1/every_3_minutes_larry_kudlow_forgets_how_to/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 29, 18, 0, 2, 309172) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b71637/my_portfolio_with_emotions_over_time/'," datetime.datetime(2019, 3, 29, 18, 0, 2, 312666)",'2019-03-29',u'Loss',u'21','wallstreetbets',u'My portfolio with emotions over time.'," u'https://imgur.com/bwtx0r5'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 29, 18, 0, 2, 312666) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b715si/im_still_buying_crm_calls_but_any_thoughts/'," datetime.datetime(2019, 3, 29, 18, 0, 2, 316464)",'2019-03-29',u'Discussion',u'0','wallstreetbets',u'I\u2019m still buying $CRM calls but any thoughts?'," u'https://www.google.com/amp/s/seekingalpha.com/amp/news/3445688-adobe-microsoft-team-take-salesforce'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 29, 18, 0, 2, 316464) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b712wj/hrs_lll_arbitrage_opportunity/'," datetime.datetime(2019, 3, 29, 18, 0, 2, 320012)",'2019-03-29',u'Discussion',u'0','wallstreetbets',u'HRS LLL arbitrage opportunity?'," u'https://old.reddit.com/r/wallstreetbets/comments/b712wj/hrs_lll_arbitrage_opportunity/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 29, 18, 0, 2, 320012) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b711og/everytime_you_fomo_into_a_trade/'," datetime.datetime(2019, 3, 29, 18, 0, 2, 323118)",'2019-03-29',u'Shitpost',u'48','wallstreetbets',u'Everytime you FOMO into a trade'," u'https://old.reddit.com/r/wallstreetbets/comments/b711og/everytime_you_fomo_into_a_trade/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 29, 18, 0, 2, 323118) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b70oht/lyft_me_up/'," datetime.datetime(2019, 3, 29, 18, 0, 2, 326731)",'2019-03-29',u'Stocks',u'53','wallstreetbets',u'Lyft me up \U0001f481\u200d\u2642\ufe0f'," u'https://old.reddit.com/r/wallstreetbets/comments/b70oht/lyft_me_up/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 29, 18, 0, 2, 326731) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b70h98/makes_me_worried_all_the_time/'," datetime.datetime(2019, 3, 29, 18, 0, 2, 328832)",'2019-03-29',u'Meme',u'678','wallstreetbets',u'Makes me worried all the time'," u'https://old.reddit.com/r/wallstreetbets/comments/b70h98/makes_me_worried_all_the_time/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 29, 18, 0, 2, 328832) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b70h4p/feel_good_story_of_the_day/'," datetime.datetime(2019, 3, 29, 18, 0, 2, 332418)",'2019-03-29',u'Discussion',u'1','wallstreetbets',u'Feel good story of the day'," u'https://www.cnbc.com/2019/03/29/alphabet-has-more-than-doubled-its-money-on-lyft.html'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 29, 18, 0, 2, 332418) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b70den/overnight_mu_tendies_thanks_wsb/'," datetime.datetime(2019, 3, 29, 18, 0, 2, 335720)",'2019-03-29',u'Gain',u'19','wallstreetbets',"u'Overnight MU tendies, thanks WSB!'"," u'https://imgur.com/npHhsCz'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 29, 18, 0, 2, 335720) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b7072y/since_half_of_you_dont_use_a_real_broker_heres/'," datetime.datetime(2019, 3, 29, 18, 0, 2, 340046)",'2019-03-29',u'DD',u'44','wallstreetbets',u'Since half of you don\u2019t use a real broker here\u2019s your LYfT IpO 411 for shorting'," u'https://old.reddit.com/r/wallstreetbets/comments/b7072y/since_half_of_you_dont_use_a_real_broker_heres/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 29, 18, 0, 2, 340046) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
2019-03-29 18:00:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b6y8ga/what_happens_to_my_stock_when_a_company_makes_a/'," datetime.datetime(2019, 3, 29, 18, 0, 2, 504747)",'2019-03-29',None,u'2','stocks',u'What happens to my stock when a company makes a transformation split?'," u'https://old.reddit.com/r/stocks/comments/b6y8ga/what_happens_to_my_stock_when_a_company_makes_a/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 29, 18, 0, 2, 504747) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b6y5x7/big_pharma_about_to_be_approved/'," datetime.datetime(2019, 3, 29, 18, 0, 2, 507385)",'2019-03-29',u'Advice',u'0','stocks',u'Big pharma about to be approved'," u'https://old.reddit.com/r/stocks/comments/b6y5x7/big_pharma_about_to_be_approved/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 29, 18, 0, 2, 507385) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b6y3bj/acb_will_it_rise/'," datetime.datetime(2019, 3, 29, 18, 0, 2, 510850)",'2019-03-29',None,u'3','stocks',u'ACB will it rise?'," u'https://old.reddit.com/r/stocks/comments/b6y3bj/acb_will_it_rise/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 29, 18, 0, 2, 510850) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b6xhwj/could_anyone_translate_this_trend_report_aka_eli5/'," datetime.datetime(2019, 3, 29, 18, 0, 2, 515115)",'2019-03-29',None,u'6','stocks',u'Could anyone translate this Trend Report aka ELI5'," u'https://old.reddit.com/r/stocks/comments/b6xhwj/could_anyone_translate_this_trend_report_aka_eli5/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 29, 18, 0, 2, 515115) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b6x0wn/bpt/'," datetime.datetime(2019, 3, 29, 18, 0, 2, 519032)",'2019-03-29',None,u'1','stocks',u'BPT'," u'https://old.reddit.com/r/stocks/comments/b6x0wn/bpt/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 29, 18, 0, 2, 519032) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b6wsa5/lyft_ipo/'," datetime.datetime(2019, 3, 29, 18, 0, 2, 522730)",'2019-03-29',None,u'117','stocks',u'LYFT IPO!'," u'https://old.reddit.com/r/stocks/comments/b6wsa5/lyft_ipo/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 29, 18, 0, 2, 522730) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b6wgvh/rstocks_daily_discussion_friday_mar_29_2019/'," datetime.datetime(2019, 3, 29, 18, 0, 2, 525440)",'2019-03-29',None,u'3','stocks',"u'r/Stocks Daily Discussion Friday - Mar 29, 2019'"," u'https://old.reddit.com/r/stocks/comments/b6wgvh/rstocks_daily_discussion_friday_mar_29_2019/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 29, 18, 0, 2, 525440) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b6w8jo/surely_no_quarterend_padding_hereglobal_stocks/'," datetime.datetime(2019, 3, 29, 18, 0, 2, 527329)",'2019-03-29',u'News',u'75','stocks',u'Surely no quarter-end padding here!...Global stocks rallying as quarter comes to a close'," u'https://old.reddit.com/r/stocks/comments/b6w8jo/surely_no_quarterend_padding_hereglobal_stocks/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 29, 18, 0, 2, 527329) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b6vgf9/ba_dividend_over_the_past_decades/'," datetime.datetime(2019, 3, 29, 18, 0, 2, 529553)",'2019-03-29',None,u'19','stocks',u'BA Dividend over the past decades.'," u'https://old.reddit.com/r/stocks/comments/b6vgf9/ba_dividend_over_the_past_decades/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 29, 18, 0, 2, 529553) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b70zxd/where_would_you_look_for_investors_in_europe/'," datetime.datetime(2019, 3, 29, 18, 0, 3, 935170)",'2019-03-29',None,u'0','investing',u'Where would you look for investors in Europe UK/Ireland/Germany?'," u'https://old.reddit.com/r/investing/comments/b70zxd/where_would_you_look_for_investors_in_europe/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 29, 18, 0, 3, 935170) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b70s8d/what_would_happen_to_stocks_like_rare_if_bernie/'," datetime.datetime(2019, 3, 29, 18, 0, 3, 937035)",'2019-03-29',None,u'0','investing',u'What would happen to stocks like $RARE if Bernie Sanders gets elected?'," u'https://old.reddit.com/r/investing/comments/b70s8d/what_would_happen_to_stocks_like_rare_if_bernie/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 29, 18, 0, 3, 937035) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b70mhc/what_is_the_best_source_for_things_like_the_lyft/'," datetime.datetime(2019, 3, 29, 18, 0, 3, 938827)",'2019-03-29',None,u'0','investing',u'What is the best source for things like the LYFT listing?'," u'https://old.reddit.com/r/investing/comments/b70mhc/what_is_the_best_source_for_things_like_the_lyft/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 29, 18, 0, 3, 938827) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b6z5ca/warren_buffett_interview_wbecky_quick_2019/'," datetime.datetime(2019, 3, 29, 17, 59, 34, 437559)",'2019-03-29',None,u'0','investing',u'Warren Buffett Interview w/Becky Quick (2019)'," u'https://old.reddit.com/r/investing/comments/b6z5ca/warren_buffett_interview_wbecky_quick_2019/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 42, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/thewallstreet/new/> (referer: None)
2019-03-29 18:00:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/thewallstreet/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Trading/comments/b6ycoh/questions_about_brokers_and_trading_habits/'," datetime.datetime(2019, 3, 29, 18, 0, 5, 204055)",'2019-03-29',None,u'1','Trading',u'Questions about brokers and trading habits'," u'https://old.reddit.com/r/Trading/comments/b6ycoh/questions_about_brokers_and_trading_habits/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 29, 18, 0, 5, 204055) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b70vjv/stock_buyback_binge_2019_is_setting_a_record_pace/'," datetime.datetime(2019, 3, 29, 18, 0, 5, 785812)",'2019-03-29',None,u'1','StockMarket',u'Stock buyback binge: 2019 is setting a record pace'," u'https://old.reddit.com/r/StockMarket/comments/b70vjv/stock_buyback_binge_2019_is_setting_a_record_pace/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 29, 18, 0, 5, 785812) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b6zxuq/lyft_just_started_trading/'," datetime.datetime(2019, 3, 29, 18, 2, 36, 351139)",'2019-03-29',None,u'1','StockMarket',u'Lyft Just started trading.'," u'https://old.reddit.com/r/StockMarket/comments/b6zxuq/lyft_just_started_trading/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 42, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b6z41x/nio_at_its_current_value_of_500_is_a_steal_and_a/'," datetime.datetime(2019, 3, 29, 18, 3, 6, 610171)",'2019-03-29',u'Discussion',u'0','investing',u'$NIO at its current value of ~5.00 is a steal and a once in a lifetime opportunity. Why the outlook looks strong.'," u'https://old.reddit.com/r/investing/comments/b6z41x/nio_at_its_current_value_of_500_is_a_steal_and_a/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 42, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b6uhfv/my_top_15_stock_trader_quotes/'," datetime.datetime(2019, 3, 29, 18, 3, 7, 856892)",'2019-03-29',None,u'24','stocks',u'My Top 15 Stock Trader Quotes'," u'https://old.reddit.com/r/stocks/comments/b6uhfv/my_top_15_stock_trader_quotes/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    def process_item(self, item, spider):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b6z2v2/reporting_a_wash_sale_when_filing_taxes/'," datetime.datetime(2019, 3, 30, 14, 4, 56, 439927)",'2019-03-30',None,u'6','investing',u'Reporting a wash sale when filing taxes'," u'https://old.reddit.com/r/investing/comments/b6z2v2/reporting_a_wash_sale_when_filing_taxes/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 42, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
2019-03-30 14:05:26 [scrapy.core.scraper] ERROR: Error processing "
https://old.reddit.com/r/stocks/comments/b6svrr/fwona_vs_fwonk/'," datetime.datetime(2019, 3, 30, 14, 5, 56, 953966)",'2019-03-30',None,u'1','stocks',u'FWONA vs FWONK'," u'https://old.reddit.com/r/stocks/comments/b6svrr/fwona_vs_fwonk/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    def process_item(self, item, spider):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b6yzj3/government_of_pakistan_is_offering_675_return_in/'," datetime.datetime(2019, 3, 30, 14, 8, 28, 570991)",'2019-03-30',u'Discussion',u'11','investing',u'Government of Pakistan is offering 6.75% return in USD?!'," u'https://old.reddit.com/r/investing/comments/b6yzj3/government_of_pakistan_is_offering_675_return_in/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 42, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b6rjo9/anyone_know_appsprograms_with_zero_fees_that/'," datetime.datetime(2019, 3, 30, 14, 10, 29, 722764)",'2019-03-30',u'Question',u'1','stocks',"u'Anyone know apps/programs with zero fees that allow you to invest, round up, your spare change on purchases into etfs/funds/stocks?'"," u'https://old.reddit.com/r/stocks/comments/b6rjo9/anyone_know_appsprograms_with_zero_fees_that/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    def process_item(self, item, spider):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-30 14:10:59 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b6yx93/lyft_ipo_opens_at_8724_in_trading_debut_after/'," datetime.datetime(2019, 3, 30, 14, 12, 0, 146747)",'2019-03-30',None,u'69','investing',u'Lyft IPO opens at $87.24 in trading debut after pricing at $72'," u'https://old.reddit.com/r/investing/comments/b6yx93/lyft_ipo_opens_at_8724_in_trading_debut_after/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 42, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 9 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-30 14:12:30 [scrapy.extensions.logstats] INFO: Crawled 9 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b6qkze/lyft_ipo/'," datetime.datetime(2019, 3, 30, 14, 15, 1, 991171)",'2019-03-30',None,u'1','stocks',u'$LYFT IPO'," u'https://old.reddit.com/r/stocks/comments/b6qkze/lyft_ipo/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    def process_item(self, item, spider):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b6ytos/tax_liability_of_vanguard_indexed_mutual_funds/'," datetime.datetime(2019, 3, 30, 14, 15, 31, 736253)",'2019-03-30',None,u'1','investing',u'Tax liability of Vanguard indexed mutual funds and counterpart ETFs seem the same. Am I overlooking something?'," u'https://old.reddit.com/r/investing/comments/b6ytos/tax_liability_of_vanguard_indexed_mutual_funds/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 42, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b6xx2c/dow_finds_a_groove/'," datetime.datetime(2019, 3, 30, 14, 19, 3, 553881)",'2019-03-30',None,u'0','investing',u'Dow finds a groove'," u'https://old.reddit.com/r/investing/comments/b6xx2c/dow_finds_a_groove/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 42, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b6xbss/what_time_does_lyft_start_trading/'," datetime.datetime(2019, 3, 30, 14, 22, 35, 54200)",'2019-03-30',u'Discussion',u'19','investing',u'What time does LYFT start trading?'," u'https://old.reddit.com/r/investing/comments/b6xbss/what_time_does_lyft_start_trading/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 42, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b6x1md/i_am_up_early_to_short_lyft/'," datetime.datetime(2019, 3, 30, 14, 26, 6, 945677)",'2019-03-30',None,u'0','investing',u'I am up early to short $Lyft'," u'https://old.reddit.com/r/investing/comments/b6x1md/i_am_up_early_to_short_lyft/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 42, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
2019-03-30 14:26:37 [scrapy.core.scraper] ERROR: Error processing "
https://old.reddit.com/r/investing/comments/b6uuge/the_national_bank_of_belgium_bnb_offers_a/'," datetime.datetime(2019, 3, 30, 14, 27, 7, 931121)",'2019-03-30',u'Education',u'10','investing',u'The national bank of Belgium (BNB) offers a divividend of 6% Which is extraordinary in these TINA times. But what is the catch (flipside?)'," u'https://old.reddit.com/r/investing/comments/b6uuge/the_national_bank_of_belgium_bnb_offers_a/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    def process_item(self, item, spider):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b6wxyp/does_a_backdoor_roth_ira_always_make_sense/'," datetime.datetime(2019, 3, 30, 14, 29, 8, 243204)",'2019-03-30',u'Help',u'5','investing',u'Does a backdoor Roth IRA always make sense?'," u'https://old.reddit.com/r/investing/comments/b6wxyp/does_a_backdoor_roth_ira_always_make_sense/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 42, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b6twwx/interactive_brokers_web_ui_is_it_not_working_only/'," datetime.datetime(2019, 3, 30, 14, 31, 9, 925811)",'2019-03-30',None,u'3','investing',u'Interactive Brokers web UI: is it not working only for me?'," u'https://old.reddit.com/r/investing/comments/b6twwx/interactive_brokers_web_ui_is_it_not_working_only/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    def process_item(self, item, spider):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.utils.log:Scrapy 1.5.1 started (bot: redditcrawler)
2019-03-30 14:31:43 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: redditcrawler)
INFO:scrapy.utils.log:Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 2.7.14 |Anaconda, Inc.| (default, Dec  7 2017, 11:07:58) - [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)], pyOpenSSL 16.2.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Darwin-15.3.0-x86_64-64bit
2019-03-30 14:31:43 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 2.7.14 |Anaconda, Inc.| (default, Dec  7 2017, 11:07:58) - [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)], pyOpenSSL 16.2.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Darwin-15.3.0-x86_64-64bit
INFO:scrapy.crawler:Overridden settings: "
https://old.reddit.com/r/stocks/comments/b6ygmw/wells_fargo_ceo_finally_retires/'," datetime.datetime(2019, 3, 30, 15, 20, 9, 686659)",'2019-03-30',None,u'0','stocks',u'Wells Fargo CEO finally retires'," u'https://old.reddit.com/r/stocks/comments/b6ygmw/wells_fargo_ceo_finally_retires/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 30, 15, 20, 9, 686659) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/investing/new/> (referer: None)
2019-03-30 15:20:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/investing/new/> (referer: None)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
2019-03-30 15:20:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b75veh/which_ipos_are_you_looking_forward_to/'," datetime.datetime(2019, 3, 30, 15, 20, 10, 403020)",'2019-03-30',None,u'0','investing',u'Which IPOs are you looking forward to?'," u'https://old.reddit.com/r/investing/comments/b75veh/which_ipos_are_you_looking_forward_to/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 30, 15, 20, 10, 403020) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b75u4w/how_will_a_brexit_with_no_deal_affect_worlds/'," datetime.datetime(2019, 3, 30, 15, 20, 10, 422581)",'2019-03-30',None,u'2','investing',u'how will a brexit with no deal affect world\u2019s stock market?'," u'https://old.reddit.com/r/investing/comments/b75u4w/how_will_a_brexit_with_no_deal_affect_worlds/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 30, 15, 20, 10, 422581) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b75oxf/vfiax_vs_vfinx/'," datetime.datetime(2019, 3, 30, 15, 20, 10, 429792)",'2019-03-30',None,u'1','investing',u'VFIAX vs VFINX'," u'https://old.reddit.com/r/investing/comments/b75oxf/vfiax_vs_vfinx/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 30, 15, 20, 10, 429792) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b75gj2/say_goodbye_to_the_cbd_foodbeverage_industry/'," datetime.datetime(2019, 3, 30, 15, 20, 10, 450251)",'2019-03-30',None,u'0','investing',u'Say goodbye to the CBD food/beverage industry...'," u'https://old.reddit.com/r/investing/comments/b75gj2/say_goodbye_to_the_cbd_foodbeverage_industry/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 30, 15, 20, 10, 450251) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b75e7s/fidelity_roth_ira_question/'," datetime.datetime(2019, 3, 30, 15, 20, 10, 454384)",'2019-03-30',None,u'2','investing',u'Fidelity Roth IRA question???'," u'https://old.reddit.com/r/investing/comments/b75e7s/fidelity_roth_ira_question/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 30, 15, 20, 10, 454384) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b7d32e/executives_and_directors_are_insider_trading_on/'," datetime.datetime(2019, 3, 30, 15, 20, 11, 907843)",'2019-03-30',None,u'2','StockMarket',"u'Executives and directors are insider trading on advance knowledge of audit issues, new study finds'"," u'https://old.reddit.com/r/StockMarket/comments/b7d32e/executives_and_directors_are_insider_trading_on/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 30, 15, 20, 11, 907843) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b77iu8/my_friend_likes_playing_the_lottery_should_i_just/'," datetime.datetime(2019, 3, 30, 15, 20, 13, 765453)",'2019-03-30',u'Shitpost',u'25','wallstreetbets',u'My friend likes playing the lottery. should I just tell her to randomly gamble on SPY instead?'," u'https://old.reddit.com/r/wallstreetbets/comments/b77iu8/my_friend_likes_playing_the_lottery_should_i_just/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 30, 15, 20, 13, 765453) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b77g46/taking_out_200_margin_to_average_down/'," datetime.datetime(2019, 3, 30, 15, 20, 13, 767849)",'2019-03-30',u'Gain',u'44','wallstreetbets',u'Taking out 200% margin to average down.'," u'https://old.reddit.com/r/wallstreetbets/comments/b77g46/taking_out_200_margin_to_average_down/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 30, 15, 20, 13, 767849) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b771r3/puts_on_jcp/'," datetime.datetime(2019, 3, 30, 15, 20, 13, 770392)",'2019-03-30',u'Shitpost',u'2','wallstreetbets',u'Puts on $JCP?'," u'https://old.reddit.com/r/wallstreetbets/comments/b771r3/puts_on_jcp/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 30, 15, 20, 13, 770392) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b76mhj/i_like_making_money_never_any_huge_plays_but_you/'," datetime.datetime(2019, 3, 30, 15, 20, 13, 772712)",'2019-03-30',u'Shitpost',u'34','wallstreetbets',u'I like making money. Never any huge plays but you make a couple percent every couple weeks and it really adds up.'," u'https://imgur.com/a/G2SDGrj'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 30, 15, 20, 13, 772712) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b75nba/every_millennial_investor_today/'," datetime.datetime(2019, 3, 30, 15, 20, 13, 776624)",'2019-03-30',u'Meme',u'26','wallstreetbets',u'Every Millennial Investor Today'," u'https://old.reddit.com/r/wallstreetbets/comments/b75nba/every_millennial_investor_today/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 30, 15, 20, 13, 776624) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b75ge7/bb_earnings_did_me_well/'," datetime.datetime(2019, 3, 30, 15, 20, 13, 782417)",'2019-03-30',u'Stocks',u'13','wallstreetbets',u'BB earnings did me well'," u'https://old.reddit.com/r/wallstreetbets/comments/b75ge7/bb_earnings_did_me_well/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 30, 15, 20, 13, 782417) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b758ce/going_long_thats_what_she_said/'," datetime.datetime(2019, 3, 30, 15, 20, 13, 785649)",'2019-03-30',u'Meme',u'312','wallstreetbets',u'Going long... thats what she said'," u'https://old.reddit.com/r/wallstreetbets/comments/b758ce/going_long_thats_what_she_said/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 30, 15, 20, 13, 785649) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b754p2/interview_with_a_lyft_ipo_investor/'," datetime.datetime(2019, 3, 30, 15, 20, 13, 788248)",'2019-03-30',u'Meme',u'595','wallstreetbets',u'Interview with a Lyft IPO investor.'," u'https://old.reddit.com/r/wallstreetbets/comments/b754p2/interview_with_a_lyft_ipo_investor/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 30, 15, 20, 13, 788248) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/Stock_Picks/new/> (referer: None)
2019-03-30 15:20:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/Stock_Picks/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b6wiay/how_to_actually_buy_lyft_at_its_ipo_price_72/'," datetime.datetime(2019, 3, 30, 15, 22, 9, 677159)",'2019-03-30',None,u'21','investing',"u""How to actually buy Lyft at it's ipo price $72"""," u'https://old.reddit.com/r/investing/comments/b6wiay/how_to_actually_buy_lyft_at_its_ipo_price_72/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 42, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Daytrading/comments/b6awpb/youtube_channel_that_shows_level_two_of_daily/'," datetime.datetime(2019, 3, 30, 15, 22, 39, 953904)",'2019-03-30',None,u'7','Daytrading',u'YouTube Channel that shows Level Two of daily runners'," u'https://youtu.be/gLA3h7yin7A'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    def process_item(self, item, spider):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-30 15:23:10 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b6w3fy/anyone_into_agriculture_or_aquaculture_stocks_or/'," datetime.datetime(2019, 3, 30, 18, 4, 7, 252448)",'2019-03-30',u'Discussion',u'12','investing',u'Anyone into agriculture or aquaculture stocks or funds ?'," u'https://old.reddit.com/r/investing/comments/b6w3fy/anyone_into_agriculture_or_aquaculture_stocks_or/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 42, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Daytrading/comments/b68t1b/robinhood_challenge_100_investment_challenge_ep_1/'," datetime.datetime(2019, 3, 30, 18, 5, 38, 32213)",'2019-03-30',None,u'19','Daytrading',u'Robinhood Challenge - $100 Investment Challenge Ep 1'," u'https://youtu.be/D0IP6Ra_QmE'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    def process_item(self, item, spider):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b6vxd4/question_about_bond_etfs/'," datetime.datetime(2019, 3, 30, 18, 6, 38, 535393)",'2019-03-30',None,u'8','investing',u'Question about bond ETFs'," u'https://old.reddit.com/r/investing/comments/b6vxd4/question_about_bond_etfs/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 42, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Daytrading/comments/b5ugix/help_with_tradezero/'," datetime.datetime(2019, 3, 30, 18, 6, 38, 649961)",'2019-03-30',None,u'1','Daytrading',u'Help with Tradezero'," u'https://old.reddit.com/r/Daytrading/comments/b5ugix/help_with_tradezero/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    def process_item(self, item, spider):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-30 18:07:08 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
INFO:scrapy.core.engine:Closing spider (finished)
2019-03-30 18:07:09 [scrapy.core.engine] INFO: Closing spider (finished)
INFO:scrapy.statscollectors:Dumping Scrapy stats:
"
https://old.reddit.com/r/investing/comments/b6v78l/softbank_ceo_masayoshi_son_says_he_offered_jeff/'," datetime.datetime(2019, 3, 30, 18, 9, 9, 835770)",'2019-03-30',None,u'1537','investing',u'SoftBank CEO Masayoshi Son says he offered Jeff Bezos $100M for a 30 percent stake in Amazon during its early years -- Son notes he says he was unable to secure the deal because Bezos wanted $130M'," u'https://old.reddit.com/r/investing/comments/b6v78l/softbank_ceo_masayoshi_son_says_he_offered_jeff/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 42, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
2019-03-30 18:09:40 [scrapy.core.scraper] ERROR: Error processing "
https://old.reddit.com/r/Daytrading/comments/b6601y/will_erdogan_success_to_strength_the_turkish_lira/'," datetime.datetime(2019, 3, 30, 18, 9, 9, 844301)",'2019-03-30',None,u'0','Daytrading',u'will Erdogan success to strength the Turkish lira by preventing and restricting foreign investors to sell'," u'https://old.reddit.com/r/Daytrading/comments/b6601y/will_erdogan_success_to_strength_the_turkish_lira/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    def process_item(self, item, spider):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
app.py:12: ScrapyDeprecationWarning: Importing from scrapy.xlib.pydispatch is deprecated and will no longer be supported in future Scrapy versions. If you just want to connect signals use the from_crawler class method, otherwise import pydispatch directly if needed. See: https://github.com/scrapy/scrapy/issues/1762
  from scrapy.xlib.pydispatch import dispatcher
INFO:scrapy.utils.log:Scrapy 1.5.1 started (bot: redditcrawler)
2019-03-30 18:10:01 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: redditcrawler)
INFO:scrapy.utils.log:Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 2.7.14 |Anaconda, Inc.| (default, Dec  7 2017, 11:07:58) - [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)], pyOpenSSL 16.2.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Darwin-15.3.0-x86_64-64bit
2019-03-30 18:10:01 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 2.7.14 |Anaconda, Inc.| (default, Dec  7 2017, 11:07:58) - [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)], pyOpenSSL 16.2.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Darwin-15.3.0-x86_64-64bit
INFO:scrapy.crawler:Overridden settings: "
https://old.reddit.com/r/investing/comments/b6uy46/when_to_cut_loss/'," datetime.datetime(2019, 3, 30, 18, 10, 40, 732184)",'2019-03-30',None,u'2','investing',u'When to cut loss?'," u'https://old.reddit.com/r/investing/comments/b6uy46/when_to_cut_loss/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 42, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 9 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-30 18:11:10 [scrapy.extensions.logstats] INFO: Crawled 9 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b6moyg/whats_the_next_stock_market_shark/'," datetime.datetime(2019, 3, 30, 18, 11, 41, 277277)",'2019-03-30',None,u'1','StockMarket',u'What\u2019s the next stock market \u201cshark\u201d?'," u'https://old.reddit.com/r/StockMarket/comments/b6moyg/whats_the_next_stock_market_shark/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 42, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Daytrading/comments/b62smx/lightspeed_trading/'," datetime.datetime(2019, 3, 30, 18, 12, 11, 311298)",'2019-03-30',None,u'6','Daytrading',u'Lightspeed trading?'," u'https://old.reddit.com/r/Daytrading/comments/b62smx/lightspeed_trading/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    def process_item(self, item, spider):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b6jkg1/your_favourite_chip_stock_longterm/'," datetime.datetime(2019, 3, 30, 18, 12, 41, 853530)",'2019-03-30',None,u'4','StockMarket',u'Your favourite chip stock long-term?'," u'https://old.reddit.com/r/StockMarket/comments/b6jkg1/your_favourite_chip_stock_longterm/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 42, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b6jjs0/cedex_purchase_50m_worth_of_diamonds_ahead_of/'," datetime.datetime(2019, 3, 30, 18, 13, 42, 333380)",'2019-03-30',None,u'0','StockMarket',u'CEDEX purchase $50m worth of diamonds ahead of first diamond ETF'," u'https://old.reddit.com/r/StockMarket/comments/b6jjs0/cedex_purchase_50m_worth_of_diamonds_ahead_of/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 42, in process_item
    self.db[self.collection_name].insert(dict(item))
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 9 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-30 18:14:12 [scrapy.extensions.logstats] INFO: Crawled 9 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/thewallstreet/comments/b57i1t/weekly_question_thread_week_12_2019/'," datetime.datetime(2019, 3, 30, 18, 14, 12, 311385)",'2019-03-30',None,u'9','thewallstreet',"u'Weekly Question Thread - Week 12, 2019'"," u'https://old.reddit.com/r/thewallstreet/comments/b57i1t/weekly_question_thread_week_12_2019/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    def process_item(self, item, spider):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b7fffc/new_nsa_backdoors_found_in_intel_chipsets_its/'," datetime.datetime(2019, 3, 30, 18, 15, 2, 254888)",'2019-03-30',u'Fundamentals',u'9','wallstreetbets',"u""New NSA backdoors found in Intel chipsets. It's time to buy $INTC leap 50 puts (never a bad decision)"""," u'https://www.tomshardware.com/news/intel-visa-undocumented-feature-chipsets-cpus,38954.html'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 30, 18, 15, 2, 254888) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b7epbc/my_brother_had_autism_until_he_downloaded_the_hood/'," datetime.datetime(2019, 3, 30, 18, 15, 2, 259661)",'2019-03-30',u'Shitpost',u'79','wallstreetbets',"u'My brother had autism until he downloaded the hood,'"," u'https://old.reddit.com/r/wallstreetbets/comments/b7epbc/my_brother_had_autism_until_he_downloaded_the_hood/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 30, 18, 15, 2, 259661) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b7ebef/when_am_i_going_to_be_able_to_short_the_shit_out/'," datetime.datetime(2019, 3, 30, 18, 15, 2, 264037)",'2019-03-30',u'Discussion',u'1','wallstreetbets',u'When am i going to be able to short the shit out of lyft?'," u'https://old.reddit.com/r/wallstreetbets/comments/b7ebef/when_am_i_going_to_be_able_to_short_the_shit_out/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 30, 18, 15, 2, 264037) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b7dp4i/dd_the_case_for_ttwo_as_a_potential_buyout/'," datetime.datetime(2019, 3, 30, 18, 15, 2, 265905)",'2019-03-30',u'DD',u'1','wallstreetbets',u'[DD] The case for TTWO as a potential buyout candidate.'," u'https://old.reddit.com/r/wallstreetbets/comments/b7dp4i/dd_the_case_for_ttwo_as_a_potential_buyout/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 30, 18, 15, 2, 265905) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b7dmy0/sir_we_are_calling_today_in_regards_to_your_naked/'," datetime.datetime(2019, 3, 30, 18, 15, 2, 267454)",'2019-03-30',u'Meme',u'21','wallstreetbets',u'Sir We are calling today in regards to your Naked Option positions.....'," u'https://old.reddit.com/r/wallstreetbets/comments/b7dmy0/sir_we_are_calling_today_in_regards_to_your_naked/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 30, 18, 15, 2, 267454) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b7dg2u/definitely_going_to_be_a_good_deliveries_number/'," datetime.datetime(2019, 3, 30, 18, 15, 2, 268959)",'2019-03-30',u'Shitpost',u'42','wallstreetbets',"u""Definitely going to be a good deliveries number this week, Elong don't let me down!"""," u'https://old.reddit.com/r/wallstreetbets/comments/b7dg2u/definitely_going_to_be_a_good_deliveries_number/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 30, 18, 15, 2, 268959) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b7d2gg/anyone_joining_me_in_losing_money_on_gamestop/'," datetime.datetime(2019, 3, 30, 18, 15, 2, 270552)",'2019-03-30',u'Shitpost',u'4','wallstreetbets',u'Anyone joining me in losing money on GameStop Calls?'," u'https://old.reddit.com/r/wallstreetbets/comments/b7d2gg/anyone_joining_me_in_losing_money_on_gamestop/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 30, 18, 15, 2, 270552) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b7co48/dae_spy_weekly_straddles/'," datetime.datetime(2019, 3, 30, 18, 15, 2, 272615)",'2019-03-30',u'Options',u'4','wallstreetbets',u'DAE SPY Weekly Straddles?'," u'https://old.reddit.com/r/wallstreetbets/comments/b7co48/dae_spy_weekly_straddles/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 30, 18, 15, 2, 272615) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b7bdij/pot_plays_if_fed_passes_bill/'," datetime.datetime(2019, 3, 30, 18, 15, 2, 274236)",'2019-03-30',u'Shitpost',u'4','wallstreetbets',u'Pot plays if Fed passes bill?'," u'https://old.reddit.com/r/wallstreetbets/comments/b7bdij/pot_plays_if_fed_passes_bill/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 30, 18, 15, 2, 274236) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b7b0b1/my_first_time/'," datetime.datetime(2019, 3, 30, 18, 15, 2, 277399)",'2019-03-30',u'Gain',u'17','wallstreetbets',u'My first time'," u'https://old.reddit.com/r/wallstreetbets/comments/b7b0b1/my_first_time/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 30, 18, 15, 2, 277399) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b7at0y/instagram_trader_portfolio_vs_his_real_portfolio/'," datetime.datetime(2019, 3, 30, 18, 15, 2, 279262)",'2019-03-30',u'Shitpost',u'4374','wallstreetbets',u'Instagram trader portfolio vs. his real portfolio'," u'https://old.reddit.com/r/wallstreetbets/comments/b7at0y/instagram_trader_portfolio_vs_his_real_portfolio/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 30, 18, 15, 2, 279262) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b7af7k/lyft_valued_at_254_billion_in_market_debut/'," datetime.datetime(2019, 3, 30, 18, 15, 2, 281171)",'2019-03-30',u'Stocks',u'8','wallstreetbets',u'Lyft Valued at $25.4 billion in market debut'," u'https://techleak.video.blog/2019/03/30/lyft-valued-at-25-4-billion-in-market-debut/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 30, 18, 15, 2, 281171) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b79abu/if_i_have_learned_anything_from_here_and_from_my/'," datetime.datetime(2019, 3, 30, 18, 15, 2, 282727)",'2019-03-30',u'Discussion',u'41','wallstreetbets',"u'If I have learned anything from here and from my books, this is a signal of the end'"," u'https://old.reddit.com/r/wallstreetbets/comments/b79abu/if_i_have_learned_anything_from_here_and_from_my/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 30, 18, 15, 2, 282727) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b78rg2/weekly_tendies_thread_march_30_2019/'," datetime.datetime(2019, 3, 30, 18, 15, 2, 284249)",'2019-03-30',None,u'1','wallstreetbets',"u'Weekly Tendies Thread - March 30, 2019'"," u'https://old.reddit.com/r/wallstreetbets/comments/b78rg2/weekly_tendies_thread_march_30_2019/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 30, 18, 15, 2, 284249) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b78m05/eric_weinstein_economic_thinking_in_a_fallible/'," datetime.datetime(2019, 3, 30, 18, 15, 2, 285748)",'2019-03-30',u'Discussion',u'3','wallstreetbets',u'Eric Weinstein: Economic Thinking In A Fallible World'," u'https://www.youtube.com/watch?v=TrZmq1Ti3Po'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 30, 18, 15, 2, 285748) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
2019-03-30 18:15:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
2019-03-30 18:15:02 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b75hc9/pinterest_ipo/'," datetime.datetime(2019, 3, 30, 18, 15, 2, 511224)",'2019-03-30',None,u'4','stocks',u'Pinterest IPO'," u'https://old.reddit.com/r/stocks/comments/b75hc9/pinterest_ipo/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 30, 18, 15, 2, 511224) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b74bs8/eli5_how_did_lyft_finish_up_when_its_worth_less/'," datetime.datetime(2019, 3, 30, 18, 15, 2, 514328)",'2019-03-30',None,u'1','stocks',"u'ELI5. How did Lyft ""finish up"" when it\'s worth less now than when it opened?'"," u'https://old.reddit.com/r/stocks/comments/b74bs8/eli5_how_did_lyft_finish_up_when_its_worth_less/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 30, 18, 15, 2, 514328) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b72yqk/as_a_new_msft_employee_should_i_hold_or_sell/'," datetime.datetime(2019, 3, 30, 18, 15, 2, 517279)",'2019-03-30',None,u'9','stocks',"u'As a new MSFT employee, should I hold or sell their stock?'"," u'https://old.reddit.com/r/stocks/comments/b72yqk/as_a_new_msft_employee_should_i_hold_or_sell/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 30, 18, 15, 2, 517279) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b72gyh/apple_cancels_airpower_the_wireless_charging_pad/'," datetime.datetime(2019, 3, 30, 18, 15, 2, 520545)",'2019-03-30',None,u'418','stocks',"u'Apple cancels AirPower, the wireless charging pad it announced over a year ago'"," u'https://old.reddit.com/r/stocks/comments/b72gyh/apple_cancels_airpower_the_wireless_charging_pad/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 30, 18, 15, 2, 520545) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b71n3g/once_puts/'," datetime.datetime(2019, 3, 30, 18, 15, 2, 535999)",'2019-03-30',None,u'1','stocks',u'$ONCE Puts'," u'https://old.reddit.com/r/stocks/comments/b71n3g/once_puts/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 30, 18, 15, 2, 535999) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b71g82/thoughts_on_ea_electronic_arts/'," datetime.datetime(2019, 3, 30, 18, 15, 2, 538794)",'2019-03-30',u'Advice',u'11','stocks',u'Thoughts on EA - Electronic arts?'," u'https://old.reddit.com/r/stocks/comments/b71g82/thoughts_on_ea_electronic_arts/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 30, 18, 15, 2, 538794) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b702iq/was_it_possible_to_buy_lyft_at_72/'," datetime.datetime(2019, 3, 30, 18, 15, 2, 541897)",'2019-03-30',None,u'5','stocks',u'Was it possible to buy LYFT at $72?'," u'https://old.reddit.com/r/stocks/comments/b702iq/was_it_possible_to_buy_lyft_at_72/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 30, 18, 15, 2, 541897) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b6zufm/rstocks_fundamentals_friday_mar_29_2019/'," datetime.datetime(2019, 3, 30, 18, 15, 2, 545086)",'2019-03-30',None,u'3','stocks',"u'r/Stocks Fundamentals Friday Mar 29, 2019'"," u'https://old.reddit.com/r/stocks/comments/b6zufm/rstocks_fundamentals_friday_mar_29_2019/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 30, 18, 15, 2, 545086) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b6zoxt/looking_for_a_new_mans_guide/'," datetime.datetime(2019, 3, 30, 18, 15, 2, 548238)",'2019-03-30',None,u'1','stocks',"u'Looking for a new mans ""guide""'"," u'https://old.reddit.com/r/stocks/comments/b6zoxt/looking_for_a_new_mans_guide/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 30, 18, 15, 2, 548238) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b6zlpe/raytheon_rtn_partners_up_with_patriot_one/'," datetime.datetime(2019, 3, 30, 18, 15, 2, 551261)",'2019-03-30',None,u'3','stocks',u'Raytheon (RTN) partners up with Patriot one technologies (PAT.V)'," u'https://old.reddit.com/r/stocks/comments/b6zlpe/raytheon_rtn_partners_up_with_patriot_one/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 30, 18, 15, 2, 551261) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b6zi9j/ladybaybee_recommended_stock_picks_20190329/'," datetime.datetime(2019, 3, 30, 18, 15, 2, 554190)",'2019-03-30',None,u'12','stocks',u'Ladybaybee recommended stock picks 2019-03-29'," u'https://old.reddit.com/r/stocks/comments/b6zi9j/ladybaybee_recommended_stock_picks_20190329/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 30, 18, 15, 2, 554190) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b6zcp1/what_do_you_value_lyft_at/'," datetime.datetime(2019, 3, 30, 18, 15, 2, 558067)",'2019-03-30',None,u'1','stocks',u'What do you value LYFT at?'," u'https://old.reddit.com/r/stocks/comments/b6zcp1/what_do_you_value_lyft_at/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 30, 18, 15, 2, 558067) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b6za87/suggest_some_stock_newsletters_to_subscribe_to/'," datetime.datetime(2019, 3, 30, 18, 15, 2, 562769)",'2019-03-30',u'Resources',u'1','stocks',u'Suggest some stock newsletters to subscribe to'," u'https://old.reddit.com/r/stocks/comments/b6za87/suggest_some_stock_newsletters_to_subscribe_to/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 30, 18, 15, 2, 562769) is not JSON serializable
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
2019-03-30 18:15:02 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
2019-03-30 18:15:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b7etn6/why_is_everybody_including_the_media_focusing_on/'," datetime.datetime(2019, 3, 30, 18, 15, 3, 849447)",'2019-03-30',None,u'4','investing',"u'Why is everybody, including the media, focusing on market cap instead of enterprise value?'"," u'https://old.reddit.com/r/investing/comments/b7etn6/why_is_everybody_including_the_media_focusing_on/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 30, 18, 15, 3, 849447) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b7ep7n/trading_in_a_roth_ira/'," datetime.datetime(2019, 3, 30, 18, 15, 3, 852846)",'2019-03-30',None,u'2','investing',u'Trading in a Roth IRA?'," u'https://old.reddit.com/r/investing/comments/b7ep7n/trading_in_a_roth_ira/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 30, 18, 15, 3, 852846) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b7em3w/is_there_hope_for_bitcoin_to_go_back_up/'," datetime.datetime(2019, 3, 30, 18, 15, 3, 856205)",'2019-03-30',None,u'1','investing',u'Is there hope for bitcoin to go back up?'," u'https://old.reddit.com/r/investing/comments/b7em3w/is_there_hope_for_bitcoin_to_go_back_up/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 30, 18, 15, 3, 856205) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b7e92x/buyback_question/'," datetime.datetime(2019, 3, 30, 18, 15, 3, 859529)",'2019-03-30',None,u'1','investing',u'Buyback Question'," u'https://old.reddit.com/r/investing/comments/b7e92x/buyback_question/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 30, 18, 15, 3, 859529) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b7d0n9/thoughts_on_marketgrader_reports/'," datetime.datetime(2019, 3, 30, 18, 15, 3, 861351)",'2019-03-30',None,u'2','investing',u'Thoughts on MarketGrader reports.'," u'https://old.reddit.com/r/investing/comments/b7d0n9/thoughts_on_marketgrader_reports/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 30, 18, 15, 3, 861351) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b7cthe/lazy_portfolio_returns_march_2019_update/'," datetime.datetime(2019, 3, 30, 18, 15, 3, 864687)",'2019-03-30',None,u'15','investing',u'Lazy Portfolio Returns March 2019 Update'," u'https://old.reddit.com/r/investing/comments/b7cthe/lazy_portfolio_returns_march_2019_update/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 30, 18, 15, 3, 864687) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b7cmsj/starboard_value_and_other_activist_investors_do/'," datetime.datetime(2019, 3, 30, 18, 15, 3, 868839)",'2019-03-30',None,u'3','investing',u'Starboard value and other activist investors: do they do anything besides take a big stake in a company and then try to get it sold?'," u'https://old.reddit.com/r/investing/comments/b7cmsj/starboard_value_and_other_activist_investors_do/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 30, 18, 15, 3, 868839) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b7cb3f/what_is_to_stop_a_market_order_from_being/'," datetime.datetime(2019, 3, 30, 18, 15, 3, 872293)",'2019-03-30',None,u'9','investing',u'What is to stop a market order from being completed at an unfair price?'," u'https://old.reddit.com/r/investing/comments/b7cb3f/what_is_to_stop_a_market_order_from_being/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 30, 18, 15, 3, 872293) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b7c3jn/should_i_hold_my_stocks_for_more_than_a_year/'," datetime.datetime(2019, 3, 30, 18, 15, 3, 874138)",'2019-03-30',None,u'1','investing',u'Should I hold my stocks for more than a year?'," u'https://old.reddit.com/r/investing/comments/b7c3jn/should_i_hold_my_stocks_for_more_than_a_year/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 30, 18, 15, 3, 874138) is not JSON serializable
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Daytrading/new/> from <GET https://old.reddit.com/r/daytrading/new/>
2019-03-30 18:15:03 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Daytrading/new/> from <GET https://old.reddit.com/r/daytrading/new/>
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b7c39s/ishares_core_msci_em_imi_ucits_etf_vs_msci_em/'," datetime.datetime(2019, 3, 30, 18, 15, 3, 878076)",'2019-03-30',None,u'0','investing',u'iShares: Core MSCI EM IMI UCITS ETF vs. MSCI EM UCITS ETF USD (Dist)'," u'https://old.reddit.com/r/investing/comments/b7c39s/ishares_core_msci_em_imi_ucits_etf_vs_msci_em/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 30, 18, 15, 3, 878076) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b7bx7r/how_do_you_pay_taxes_on_the_capital_gains/'," datetime.datetime(2019, 3, 30, 18, 15, 3, 879671)",'2019-03-30',None,u'2','investing',u'How do you pay taxes on the capital gains?'," u'https://old.reddit.com/r/investing/comments/b7bx7r/how_do_you_pay_taxes_on_the_capital_gains/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 30, 18, 15, 3, 879671) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b7bs0s/google_parent_company_alphabet_has_more_than/'," datetime.datetime(2019, 3, 30, 18, 15, 3, 881158)",'2019-03-30',None,u'85','investing',u'Google parent company Alphabet has more than doubled its money on Lyft to $1 billion in just 17 months (CNBC)'," u'https://old.reddit.com/r/investing/comments/b7bs0s/google_parent_company_alphabet_has_more_than/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 30, 18, 15, 3, 881158) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b7bppt/how_does_m1_finance_let_you_buy_fractional_shares/'," datetime.datetime(2019, 3, 30, 18, 15, 3, 882963)",'2019-03-30',None,u'3','investing',u'How does M1 Finance let you buy fractional shares?'," u'https://old.reddit.com/r/investing/comments/b7bppt/how_does_m1_finance_let_you_buy_fractional_shares/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 30, 18, 15, 3, 882963) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b7bgwe/lyft_is_wildly_unprofitable_and_lost_911_million/'," datetime.datetime(2019, 3, 30, 18, 15, 3, 885208)",'2019-03-30',None,u'1132','investing',u'Lyft is wildly unprofitable and lost $911 million last year \u2014 here\u2019s how other unprofitable companies fared after they went public'," u'https://old.reddit.com/r/investing/comments/b7bgwe/lyft_is_wildly_unprofitable_and_lost_911_million/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 30, 18, 15, 3, 885208) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b7bet4/looming_recession/'," datetime.datetime(2019, 3, 30, 18, 15, 3, 886910)",'2019-03-30',None,u'0','investing',u'Looming recession'," u'https://old.reddit.com/r/investing/comments/b7bet4/looming_recession/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 30, 18, 15, 3, 886910) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b7b12c/alumni_ventures_group_any_first_hand_experience/'," datetime.datetime(2019, 3, 30, 18, 15, 3, 889167)",'2019-03-30',None,u'1','investing',u'Alumni Ventures Group - Any first hand experience to share?'," u'https://old.reddit.com/r/investing/comments/b7b12c/alumni_ventures_group_any_first_hand_experience/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 30, 18, 15, 3, 889167) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b7az1q/buying_etfs_vs_individual_stocks/'," datetime.datetime(2019, 3, 30, 18, 15, 3, 890910)",'2019-03-30',None,u'4','investing',u'Buying ETFs vs Individual stocks'," u'https://old.reddit.com/r/investing/comments/b7az1q/buying_etfs_vs_individual_stocks/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 30, 18, 15, 3, 890910) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b7ak8u/are_there_any_investment_options_for_people_with/'," datetime.datetime(2019, 3, 30, 18, 15, 3, 894153)",'2019-03-30',u'Help',u'0','investing',u'Are there any investment options for people with a few hundred dollars?'," u'https://old.reddit.com/r/investing/comments/b7ak8u/are_there_any_investment_options_for_people_with/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 30, 18, 15, 3, 894153) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b77jkz/etrade_prebuilt_etf_portfolios/'," datetime.datetime(2019, 3, 30, 18, 15, 3, 896533)",'2019-03-30',None,u'2','investing',u'E*TRADE prebuilt ETF Portfolios'," u'https://old.reddit.com/r/investing/comments/b77jkz/etrade_prebuilt_etf_portfolios/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 30, 18, 15, 3, 896533) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-03-30 18:15:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/thewallstreet/comments/b5flgt/post_market_discussion_march_25/'," datetime.datetime(2019, 3, 30, 18, 15, 5, 21290)",'2019-03-30',u'Daily',u'7','thewallstreet',u'Post Market Discussion - (March 25)'," u'https://old.reddit.com/r/thewallstreet/comments/b5flgt/post_market_discussion_march_25/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 30, 18, 15, 5, 21290) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/thewallstreet/comments/b59ju2/daily_discussion_march_25/'," datetime.datetime(2019, 3, 30, 18, 15, 5, 23034)",'2019-03-30',u'Daily',u'11','thewallstreet',u'Daily Discussion - (March 25)'," u'https://old.reddit.com/r/thewallstreet/comments/b59ju2/daily_discussion_march_25/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 30, 18, 15, 5, 23034) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/Trading/new/> (referer: None)
2019-03-30 18:15:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/Trading/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b6parx/aurora_cannabis_acb_will_it_stand_strong_against/'," datetime.datetime(2019, 3, 30, 18, 15, 5, 841101)",'2019-03-30',None,u'31','StockMarket',u'Aurora cannabis (ACB) Will It stand strong against the growing market?'," u'https://old.reddit.com/r/StockMarket/comments/b6parx/aurora_cannabis_acb_will_it_stand_strong_against/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 30, 18, 15, 5, 841101) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b6omo0/my_top_picks_for_value_investing/'," datetime.datetime(2019, 3, 30, 18, 15, 5, 842559)",'2019-03-30',None,u'6','StockMarket',u'My top picks for value investing'," u'https://old.reddit.com/r/StockMarket/comments/b6omo0/my_top_picks_for_value_investing/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 30, 18, 15, 5, 842559) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b6olm6/does_anyone_know_any_website_where_i_can_see/'," datetime.datetime(2019, 3, 30, 18, 15, 5, 844037)",'2019-03-30',None,u'0','StockMarket',u'Does anyone know any website where I can see historical Crude Oil prices on a weekly or daily basis? Preferably somewhere where I can download a CSV or excel file for free'," u'https://old.reddit.com/r/StockMarket/comments/b6olm6/does_anyone_know_any_website_where_i_can_see/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 30, 18, 15, 5, 844037) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b6olbk/what_source_do_you_use_to_find_the_pre_market/'," datetime.datetime(2019, 3, 30, 18, 15, 5, 845497)",'2019-03-30',None,u'5','StockMarket',u'What source do you use to find the pre market news and earnings reports for the london and german stock exchange?'," u'https://old.reddit.com/r/StockMarket/comments/b6olbk/what_source_do_you_use_to_find_the_pre_market/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 30, 18, 15, 5, 845497) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b6odkt/treasury_yields_fall_pulling_stocks_down/'," datetime.datetime(2019, 3, 30, 18, 15, 5, 847012)",'2019-03-30',None,u'0','StockMarket',"u'Treasury Yields Fall, Pulling Stocks Down'"," u'https://old.reddit.com/r/StockMarket/comments/b6odkt/treasury_yields_fall_pulling_stocks_down/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 30, 18, 15, 5, 847012) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b6o8gb/just_bought_a_great_airline_stock_what_do_you/'," datetime.datetime(2019, 3, 30, 18, 15, 5, 848503)",'2019-03-30',None,u'18','StockMarket',u'Just bought a great airline stock. What do you think?'," u'https://old.reddit.com/r/StockMarket/comments/b6o8gb/just_bought_a_great_airline_stock_what_do_you/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 30, 18, 15, 5, 848503) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b6ngml/question/'," datetime.datetime(2019, 3, 30, 18, 15, 5, 850070)",'2019-03-30',None,u'0','StockMarket',u'Question'," u'https://old.reddit.com/r/StockMarket/comments/b6ngml/question/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 30, 18, 15, 5, 850070) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Daytrading/comments/b6bm6n/beginner_here_needing_direction_thanks/'," datetime.datetime(2019, 3, 30, 18, 15, 6, 236435)",'2019-03-30',None,u'2','Daytrading',"u'Beginner here needing direction, thanks.'"," u'https://old.reddit.com/r/Daytrading/comments/b6bm6n/beginner_here_needing_direction_thanks/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 30, 18, 15, 6, 236435) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Daytrading/comments/b6ba05/thoughts_on_rbz/'," datetime.datetime(2019, 3, 30, 18, 15, 6, 238356)",'2019-03-30',None,u'1','Daytrading',u'Thoughts on RBZ?'," u'https://old.reddit.com/r/Daytrading/comments/b6ba05/thoughts_on_rbz/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 30, 18, 15, 6, 238356) is not JSON serializable
INFO:scrapy.core.engine:Closing spider (finished)
2019-03-30 18:15:06 [scrapy.core.engine] INFO: Closing spider (finished)
INFO:scrapy.statscollectors:Dumping Scrapy stats:
"
https://old.reddit.com/r/StockMarket/comments/b6imac/gcgx_big_news_out_global_consortium_inc_going/'," datetime.datetime(2019, 3, 30, 18, 14, 42, 581964)",'2019-03-30',None,u'0','StockMarket',"u'$GCGX Big news out: ""Global Consortium, Inc., going public in Canada, uplisting to the CSE""'"," u'https://old.reddit.com/r/StockMarket/comments/b6imac/gcgx_big_news_out_global_consortium_inc_going/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    def process_item(self, item, spider):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Daytrading/comments/b5z3y0/dealing_with_large_losses/'," datetime.datetime(2019, 3, 30, 18, 15, 12, 839387)",'2019-03-30',None,u'11','Daytrading',u'Dealing with large losses'," u'https://old.reddit.com/r/Daytrading/comments/b5z3y0/dealing_with_large_losses/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    def process_item(self, item, spider):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/thewallstreet/comments/b552to/deviations_poc_and_value_area_for_monday_march_25/'," datetime.datetime(2019, 3, 30, 18, 17, 13, 823215)",'2019-03-30',None,u'12','thewallstreet',"u'Deviations, POC, and Value Area for Monday, March 25, 2019'"," u'https://i.imgur.com/xBPvhaB.png'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    def process_item(self, item, spider):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b6i5i7/todays_premarket_news_thursday_march_28th_2019/'," datetime.datetime(2019, 3, 30, 18, 17, 44, 136523)",'2019-03-30',u'News',u'2','StockMarket',"u""Today's Pre-Market News [Thursday, March 28th, 2019]"""," u'https://old.reddit.com/r/StockMarket/comments/b6i5i7/todays_premarket_news_thursday_march_28th_2019/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    def process_item(self, item, spider):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Daytrading/comments/b5y6pw/tips_for_university_student_club/'," datetime.datetime(2019, 3, 30, 18, 32, 21, 17711)",'2019-03-30',None,u'10','Daytrading',u'Tips for university student club'," u'https://old.reddit.com/r/Daytrading/comments/b5y6pw/tips_for_university_student_club/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    def process_item(self, item, spider):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Trading/comments/azgucm/news_feedsapps/'," datetime.datetime(2019, 3, 30, 18, 32, 51, 258774)",'2019-03-30',None,u'7','Trading',u'News Feeds/Apps'," u'https://old.reddit.com/r/Trading/comments/azgucm/news_feedsapps/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    def process_item(self, item, spider):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b6hklc/icelandic_airline_wow_air_collapses_and_cancels/'," datetime.datetime(2019, 3, 30, 18, 33, 51, 880585)",'2019-03-30',None,u'346','StockMarket',u'Icelandic airline Wow Air collapses and cancels all flights'," u'https://old.reddit.com/r/StockMarket/comments/b6hklc/icelandic_airline_wow_air_collapses_and_cancels/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    def process_item(self, item, spider):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Daytrading/comments/b5y22m/whats_the_best_site_to_daytrading_crypto_in_new/'," datetime.datetime(2019, 3, 30, 18, 34, 22, 9248)",'2019-03-30',None,u'0','Daytrading',u'Whats the best site to Daytrading Crypto In New York'," u'https://old.reddit.com/r/Daytrading/comments/b5y22m/whats_the_best_site_to_daytrading_crypto_in_new/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    def process_item(self, item, spider):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
app.py:12: ScrapyDeprecationWarning: Importing from scrapy.xlib.pydispatch is deprecated and will no longer be supported in future Scrapy versions. If you just want to connect signals use the from_crawler class method, otherwise import pydispatch directly if needed. See: https://github.com/scrapy/scrapy/issues/1762
  from scrapy.xlib.pydispatch import dispatcher
INFO:scrapy.utils.log:Scrapy 1.5.1 started (bot: redditcrawler)
2019-03-30 18:35:01 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: redditcrawler)
INFO:scrapy.utils.log:Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 2.7.14 |Anaconda, Inc.| (default, Dec  7 2017, 11:07:58) - [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)], pyOpenSSL 16.2.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Darwin-15.3.0-x86_64-64bit
2019-03-30 18:35:01 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 2.7.14 |Anaconda, Inc.| (default, Dec  7 2017, 11:07:58) - [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)], pyOpenSSL 16.2.0 (OpenSSL 1.1.0h  27 Mar 2018), cryptography 2.2.2, Platform Darwin-15.3.0-x86_64-64bit
INFO:scrapy.crawler:Overridden settings: "
https://old.reddit.com/r/Trading/comments/az5tt5/beginner_resources/'," datetime.datetime(2019, 3, 30, 18, 34, 52, 243683)",'2019-03-30',None,u'5','Trading',u'Beginner resources'," u'https://old.reddit.com/r/Trading/comments/az5tt5/beginner_resources/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    def process_item(self, item, spider):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b6gisw/brazil_marketwhen_do_you_think_it_will_rebound/'," datetime.datetime(2019, 3, 30, 18, 35, 52, 771813)",'2019-03-30',None,u'2','StockMarket',u'Brazil market--when do you think it will rebound?'," u'https://old.reddit.com/r/StockMarket/comments/b6gisw/brazil_marketwhen_do_you_think_it_will_rebound/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    def process_item(self, item, spider):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Daytrading/comments/b5v7fu/are_you_ever_limited_to_how_many_trades_you_can/'," datetime.datetime(2019, 3, 30, 18, 36, 22, 957443)",'2019-03-30',None,u'4','Daytrading',u'Are you ever limited to how many trades you can make because of closed trades being settled days after execution?'," u'https://old.reddit.com/r/Daytrading/comments/b5v7fu/are_you_ever_limited_to_how_many_trades_you_can/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    def process_item(self, item, spider):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Stock_Picks/comments/avyy9f/tesla_will_close_most_of_its_stores_and_only_sell/'," datetime.datetime(2019, 3, 30, 18, 36, 53, 261140)",'2019-03-30',u'News',u'34','Stock_Picks',u'Tesla will close most of its stores and only sell cars online.'," u'https://www.theverge.com/2019/2/28/18245296/tesla-stores-closing-online-only-car-sales'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 40, in process_item
    def process_item(self, item, spider):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 3161, in insert
    check_keys, manipulate, write_concern)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 607, in _insert
    bypass_doc_val, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/collection.py"", line 595, in _insert_one
    acknowledged, _insert_command, session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1247, in _retryable_write
    with self._tmp_session(session) as s:
  File ""/Users/selenacordona/anaconda2/lib/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1576, in _tmp_session
    s = self._ensure_session(session)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1563, in _ensure_session
    return self.__start_session(True, causal_consistency=False)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1516, in __start_session
    server_session = self._get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/mongo_client.py"", line 1549, in _get_server_session
    return self._topology.get_server_session()
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 427, in get_server_session
    None)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/pymongo/topology.py"", line 199, in _select_servers_loop
    self._error_message(selector))
ServerSelectionTimeoutError: localhost:27017: [Errno 61] Connection refused
INFO:scrapy.extensions.logstats:Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-03-30 18:37:23 [scrapy.extensions.logstats] INFO: Crawled 10 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
INFO:scrapy.core.engine:Closing spider (finished)
2019-03-30 18:37:23 [scrapy.core.engine] INFO: Closing spider (finished)
INFO:scrapy.statscollectors:Dumping Scrapy stats:
"
https://old.reddit.com/r/investing/comments/b7v7jk/which_mass_mutual_401k_plan_should_i_choose/'," datetime.datetime(2019, 3, 31, 23, 0, 5, 146871)",'2019-03-31',None,u'2','investing',u'Which Mass Mutual 401k plan should I choose?'," u'https://old.reddit.com/r/investing/comments/b7v7jk/which_mass_mutual_401k_plan_should_i_choose/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 31, 23, 0, 5, 146871) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b7ku2c/connected_yahoo_finanace_to_robinhood/'," datetime.datetime(2019, 3, 31, 23, 5, 3, 258559)",'2019-03-31',u'Shitpost',u'10','wallstreetbets',u'Connected Yahoo finanace to robinhood'," u'https://old.reddit.com/r/wallstreetbets/comments/b7ku2c/connected_yahoo_finanace_to_robinhood/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 31, 23, 5, 3, 258559) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b7koc8/antoine_walker_mod_here_explains_how_he_lost_110/'," datetime.datetime(2019, 3, 31, 23, 5, 3, 260609)",'2019-03-31',u'Shitpost',u'134','wallstreetbets',u'Antoine Walker ( Mod Here )Explains How He Lost $110 Million'," u'https://www.youtube.com/watch?v=oNaw2bk39jY'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 31, 23, 5, 3, 260609) is not JSON serializable
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
2019-03-31 23:05:03 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
2019-03-31 23:05:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b7vbnj/where_can_i_trade_cattle_futures_nebraska_lost_1/'," datetime.datetime(2019, 3, 31, 23, 10, 2, 116434)",'2019-03-31',u'Discussion',u'8','wallstreetbets',u'Where can I trade cattle futures? Nebraska lost 1 million head of cattle in floods last month'," u'https://old.reddit.com/r/wallstreetbets/comments/b7vbnj/where_can_i_trade_cattle_futures_nebraska_lost_1/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 31, 23, 10, 2, 116434) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b7v2y4/wsb_meetup_omaha_edition/'," datetime.datetime(2019, 3, 31, 23, 10, 2, 122471)",'2019-03-31',u'Discussion',u'8','wallstreetbets',u'WSB Meetup Omaha Edition'," u'https://old.reddit.com/r/wallstreetbets/comments/b7v2y4/wsb_meetup_omaha_edition/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 31, 23, 10, 2, 122471) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b7tu1u/im_bearish_chip_stocks_but_encouraged_by_recent/'," datetime.datetime(2019, 3, 31, 23, 10, 2, 126019)",'2019-03-31',u'DD',u'5','wallstreetbets',"u""I'm bearish chip stocks but encouraged by recent reports. Foxconn (iphone builder) reported underwhelming growth Friday,Huawei reported outstanding growth Friday..Micron & Intel created a new product called Optane. The top left I tracked DRAM price decreasing on Amazon in March and reviews"""," u'https://old.reddit.com/r/wallstreetbets/comments/b7tu1u/im_bearish_chip_stocks_but_encouraged_by_recent/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 31, 23, 10, 2, 126019) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b7tp56/looking_for_thots_do_you_think_this_continued/'," datetime.datetime(2019, 3, 31, 23, 10, 2, 129633)",'2019-03-31',u'Discussion',u'5','wallstreetbets',u'[Looking for thots] Do you think this continued cheap debt reality is going to cause a huge flux of liquidity/default issues once rates rise in the future due to overconfidence in the FED?'," u'https://old.reddit.com/r/wallstreetbets/comments/b7tp56/looking_for_thots_do_you_think_this_continued/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 31, 23, 10, 2, 129633) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b7svx1/is_it_a_challenge_for_switzerland_based_hedge/'," datetime.datetime(2019, 3, 31, 23, 10, 2, 133172)",'2019-03-31',u'Discussion',u'2','wallstreetbets',"u'Is it a challenge for Switzerland based hedge funds to find talent outside of NYC, London or Hong Kong?'"," u'https://old.reddit.com/r/wallstreetbets/comments/b7svx1/is_it_a_challenge_for_switzerland_based_hedge/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 31, 23, 10, 2, 133172) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b7sr6v/what_are_your_moves_tomorrow_april_01/'," datetime.datetime(2019, 3, 31, 23, 10, 2, 136295)",'2019-03-31',u'Daily Discussion',u'32','wallstreetbets',"u'What Are Your Moves Tomorrow, April 01'"," u'https://old.reddit.com/r/wallstreetbets/comments/b7sr6v/what_are_your_moves_tomorrow_april_01/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 31, 23, 10, 2, 136295) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b7s7mn/intel_leak_discussion_and_implications_for/'," datetime.datetime(2019, 3, 31, 23, 10, 2, 139776)",'2019-03-31',u'Shitpost',u'19','wallstreetbets',u'Intel leak discussion and implications for Micron/AMD/Nvida.'," u'https://old.reddit.com/r/wallstreetbets/comments/b7s7mn/intel_leak_discussion_and_implications_for/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 31, 23, 10, 2, 139776) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b7rl36/where_are_your_calls_on_infosys_infy/'," datetime.datetime(2019, 3, 31, 23, 10, 2, 153595)",'2019-03-31',u'DD',u'13','wallstreetbets',u'Where are your calls on InfoSys ($INFY)?'," u'https://old.reddit.com/r/wallstreetbets/comments/b7rl36/where_are_your_calls_on_infosys_infy/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 31, 23, 10, 2, 153595) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b7qz14/ok_wsb_heres_some_gain_and_loss_porn_for_you_all/'," datetime.datetime(2019, 3, 31, 23, 10, 2, 156779)",'2019-03-31',u'Loss',u'683','wallstreetbets',"u""Ok, wsb. Here's some gain (and loss) porn for you all."""," u'https://old.reddit.com/r/wallstreetbets/comments/b7qz14/ok_wsb_heres_some_gain_and_loss_porn_for_you_all/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 31, 23, 10, 2, 156779) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b7qui2/how_does_everybody_plan_to_lose_money_this_week/'," datetime.datetime(2019, 3, 31, 23, 10, 2, 158913)",'2019-03-31',u'Discussion',u'33','wallstreetbets',u'How does everybody plan to lose money this week?'," u'https://old.reddit.com/r/wallstreetbets/comments/b7qui2/how_does_everybody_plan_to_lose_money_this_week/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 31, 23, 10, 2, 158913) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b7p7x9/video_of_the_tesla_semi_delivering_cars_from/'," datetime.datetime(2019, 3, 31, 23, 10, 2, 161009)",'2019-03-31',u'Fundamentals',u'12','wallstreetbets',u'Video of the Tesla Semi \u201cdelivering\u201d cars from yesterday'," u'https://twitter.com/evdefender/status/1112326449820823552?s=21'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 31, 23, 10, 2, 161009) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
2019-03-31 23:10:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b7p1jg/elon_you_sandbagging_son_of_a_bitch/'," datetime.datetime(2019, 3, 31, 23, 10, 2, 171132)",'2019-03-31',u'Discussion',u'42','wallstreetbets',"u'Elon, you sandbagging son of a bitch!'"," u'https://old.reddit.com/r/wallstreetbets/comments/b7p1jg/elon_you_sandbagging_son_of_a_bitch/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 31, 23, 10, 2, 171132) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b7oqu5/the_mega_spy_and_sp_500_ta_thread/'," datetime.datetime(2019, 3, 31, 23, 10, 2, 174737)",'2019-03-31',u'Technicals',u'22','wallstreetbets',u'The mega SPY and S&P 500 TA thread'," u'https://old.reddit.com/r/wallstreetbets/comments/b7oqu5/the_mega_spy_and_sp_500_ta_thread/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 31, 23, 10, 2, 174737) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b7o4m7/my_analysis_of_the_current_market_situation/'," datetime.datetime(2019, 3, 31, 23, 10, 2, 177747)",'2019-03-31',u'Meme',u'2706','wallstreetbets',u'My analysis of the current market situation'," u'https://i.imgflip.com/2xeawh.jpg'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 31, 23, 10, 2, 177747) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b7npcn/the_rise_and_fall_of_yelp_long_yelp_the_next_snap/'," datetime.datetime(2019, 3, 31, 23, 10, 2, 179523)",'2019-03-31',u'Shitpost',u'33','wallstreetbets',u'The Rise And Fall Of Yelp - Long $YELP - the Next $SNAP'," u'https://www.youtube.com/watch?v=bKka8ZoCDaw'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 31, 23, 10, 2, 179523) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b7nokb/call_vs_turbo_call/'," datetime.datetime(2019, 3, 31, 23, 10, 2, 181308)",'2019-03-31',u'Options',u'7','wallstreetbets',u'Call vs Turbo Call'," u'https://old.reddit.com/r/wallstreetbets/comments/b7nokb/call_vs_turbo_call/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 31, 23, 10, 2, 181308) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b7nfyd/long_and_hard_snap/'," datetime.datetime(2019, 3, 31, 23, 10, 2, 183001)",'2019-03-31',u'Shitpost',u'13','wallstreetbets',u'Long - and hard - SNAP?'," u'https://techcrunch.com/2019/03/29/quinn-porn-caroline-spiegel/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 31, 23, 10, 2, 183001) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b7l6so/test_yourself_can_you_even_analyze_a_perfectly/'," datetime.datetime(2019, 3, 31, 23, 10, 2, 184638)",'2019-03-31',u'Options',u'22','wallstreetbets',u'test yourself! can you even analyze a perfectly simplified options problem?'," u'https://old.reddit.com/r/wallstreetbets/comments/b7l6so/test_yourself_can_you_even_analyze_a_perfectly/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 31, 23, 10, 2, 184638) is not JSON serializable
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
2019-03-31 23:10:02 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b7d6ml/could_someone_point_me_to_resourcesreasons_many/'," datetime.datetime(2019, 3, 31, 23, 10, 2, 314518)",'2019-03-31',None,u'8','stocks',u'Could someone point me to resources/reasons many are so convinced another recession is inevitable in the coming months/year?'," u'https://old.reddit.com/r/stocks/comments/b7d6ml/could_someone_point_me_to_resourcesreasons_many/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 31, 23, 10, 2, 314518) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b7c25t/lyft_options/'," datetime.datetime(2019, 3, 31, 23, 10, 2, 316090)",'2019-03-31',u'Discussion',u'1','stocks',u'Lyft Options'," u'https://old.reddit.com/r/stocks/comments/b7c25t/lyft_options/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 31, 23, 10, 2, 316090) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b7bq3o/deloitte_independence_violation/'," datetime.datetime(2019, 3, 31, 23, 10, 2, 317634)",'2019-03-31',None,u'9','stocks',u'Deloitte Independence Violation?'," u'https://old.reddit.com/r/stocks/comments/b7bq3o/deloitte_independence_violation/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 31, 23, 10, 2, 317634) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b7bbsc/vz_vs_t_vs_tmus_long_term/'," datetime.datetime(2019, 3, 31, 23, 10, 2, 319142)",'2019-03-31',u'Discussion',u'13','stocks',u'$VZ vs $T vs $TMUS Long Term?'," u'https://old.reddit.com/r/stocks/comments/b7bbsc/vz_vs_t_vs_tmus_long_term/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 31, 23, 10, 2, 319142) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b79geq/preico_investment_or_private_equity/'," datetime.datetime(2019, 3, 31, 23, 10, 2, 320673)",'2019-03-31',None,u'0','stocks',u'PreICO Investment or private equity'," u'https://old.reddit.com/r/stocks/comments/b79geq/preico_investment_or_private_equity/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 31, 23, 10, 2, 320673) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b766bs/f/'," datetime.datetime(2019, 3, 31, 23, 10, 2, 322333)",'2019-03-31',None,u'3','stocks',u'F'," u'https://old.reddit.com/r/stocks/comments/b766bs/f/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 31, 23, 10, 2, 322333) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b75jmy/has_anyone_considered_schneider_electric_epa_su/'," datetime.datetime(2019, 3, 31, 23, 10, 2, 325020)",'2019-03-31',None,u'0','stocks',u'Has anyone considered Schneider Electric (EPA: SU)?'," u'https://old.reddit.com/r/stocks/comments/b75jmy/has_anyone_considered_schneider_electric_epa_su/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 31, 23, 10, 2, 325020) is not JSON serializable
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
2019-03-31 23:10:02 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
2019-03-31 23:10:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b7ui6y/downturn_in_europe_vs_potential_downturns_in_us/'," datetime.datetime(2019, 3, 31, 23, 10, 3, 327480)",'2019-03-31',None,u'15','investing',u'Downturn in Europe Vs. Potential Downturns in US'," u'https://old.reddit.com/r/investing/comments/b7ui6y/downturn_in_europe_vs_potential_downturns_in_us/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 31, 23, 10, 3, 327480) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b7uc04/safest_etfs_with_high_yields/'," datetime.datetime(2019, 3, 31, 23, 10, 3, 328923)",'2019-03-31',None,u'0','investing',u'Safest ETFs with high yields?'," u'https://old.reddit.com/r/investing/comments/b7uc04/safest_etfs_with_high_yields/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 31, 23, 10, 3, 328923) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b7ts5v/will_ibm_go_thru_with_the_red_hat_acquisition/'," datetime.datetime(2019, 3, 31, 23, 10, 3, 330357)",'2019-03-31',u'Discussion',u'10','investing',u'Will IBM go thru with the Red Hat acquisition?'," u'https://old.reddit.com/r/investing/comments/b7ts5v/will_ibm_go_thru_with_the_red_hat_acquisition/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 31, 23, 10, 3, 330357) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b7tiwf/is_amazon_underpriced_or_overpriced/'," datetime.datetime(2019, 3, 31, 23, 10, 3, 332595)",'2019-03-31',None,u'19','investing',u'Is Amazon underpriced or overpriced?'," u'https://old.reddit.com/r/investing/comments/b7tiwf/is_amazon_underpriced_or_overpriced/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 31, 23, 10, 3, 332595) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b7tg9x/for_those_primarily_skilled_experienced_in_equity/'," datetime.datetime(2019, 3, 31, 23, 10, 3, 334682)",'2019-03-31',None,u'9','investing',"u""For those primarily skilled, experienced in equity research, what are the best 'exit strategy' careers due to concern of automated trading and MIFID?"""," u'https://old.reddit.com/r/investing/comments/b7tg9x/for_those_primarily_skilled_experienced_in_equity/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 31, 23, 10, 3, 334682) is not JSON serializable
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Daytrading/new/> from <GET https://old.reddit.com/r/daytrading/new/>
2019-03-31 23:10:03 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Daytrading/new/> from <GET https://old.reddit.com/r/daytrading/new/>
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b7tej9/is_it_better_to_invest_all_your_money_at_once_or/'," datetime.datetime(2019, 3, 31, 23, 10, 3, 338390)",'2019-03-31',None,u'4','investing',u'Is it better to invest all your money at once or over time?'," u'https://old.reddit.com/r/investing/comments/b7tej9/is_it_better_to_invest_all_your_money_at_once_or/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 31, 23, 10, 3, 338390) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b7swv0/how_many_non_retirement_brokerage_accounts/'," datetime.datetime(2019, 3, 31, 23, 10, 3, 340183)",'2019-03-31',None,u'0','investing',u'How many non retirement brokerage accounts?'," u'https://old.reddit.com/r/investing/comments/b7swv0/how_many_non_retirement_brokerage_accounts/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 31, 23, 10, 3, 340183) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b7styj/was_there_news_about_a_coming_recession_before/'," datetime.datetime(2019, 3, 31, 23, 10, 3, 341830)",'2019-03-31',None,u'7','investing',u'Was there news about a coming recession before the last time interest rate inverted?'," u'https://old.reddit.com/r/investing/comments/b7styj/was_there_news_about_a_coming_recession_before/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 31, 23, 10, 3, 341830) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b7sqk0/rebalancing_across_multiple_retirement_accounts/'," datetime.datetime(2019, 3, 31, 23, 10, 3, 343542)",'2019-03-31',None,u'4','investing',u'Rebalancing across multiple retirement accounts'," u'https://old.reddit.com/r/investing/comments/b7sqk0/rebalancing_across_multiple_retirement_accounts/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 31, 23, 10, 3, 343542) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b7snhg/corporate_stock_buybacks/'," datetime.datetime(2019, 3, 31, 23, 10, 3, 345224)",'2019-03-31',u'Help',u'10','investing',u'Corporate stock buybacks'," u'https://old.reddit.com/r/investing/comments/b7snhg/corporate_stock_buybacks/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 31, 23, 10, 3, 345224) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b7skb6/whats_the_case_to_using_an_industrys_projected/'," datetime.datetime(2019, 3, 31, 23, 10, 3, 346882)",'2019-03-31',None,u'3','investing',"u""What's the case to using an industry's projected CAGR as a discount rate for DCF?"""," u'https://old.reddit.com/r/investing/comments/b7skb6/whats_the_case_to_using_an_industrys_projected/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 31, 23, 10, 3, 346882) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b7rvrh/question_about_turtle_trading/'," datetime.datetime(2019, 3, 31, 23, 10, 3, 349135)",'2019-03-31',None,u'1','investing',u'Question about Turtle Trading'," u'https://old.reddit.com/r/investing/comments/b7rvrh/question_about_turtle_trading/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 31, 23, 10, 3, 349135) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b7rvo6/how_bonds_work/'," datetime.datetime(2019, 3, 31, 23, 10, 3, 350892)",'2019-03-31',None,u'2','investing',u'How bonds work?'," u'https://old.reddit.com/r/investing/comments/b7rvo6/how_bonds_work/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 31, 23, 10, 3, 350892) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b7rrwo/what_are_the_key_things_to_look_for_when_reading/'," datetime.datetime(2019, 3, 31, 23, 10, 3, 353746)",'2019-03-31',None,u'12','investing',u'What are the key things to look for when reading financial statements?'," u'https://old.reddit.com/r/investing/comments/b7rrwo/what_are_the_key_things_to_look_for_when_reading/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 31, 23, 10, 3, 353746) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b7ria6/best_podcast_for_etf_investors/'," datetime.datetime(2019, 3, 31, 23, 10, 3, 366162)",'2019-03-31',None,u'0','investing',u'Best podcast for ETF investors?'," u'https://old.reddit.com/r/investing/comments/b7ria6/best_podcast_for_etf_investors/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 31, 23, 10, 3, 366162) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b7r6qc/good_investment_or_bad_idea/'," datetime.datetime(2019, 3, 31, 23, 10, 3, 369300)",'2019-03-31',None,u'1','investing',"u'Good Investment, or Bad Idea?'"," u'https://old.reddit.com/r/investing/comments/b7r6qc/good_investment_or_bad_idea/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 31, 23, 10, 3, 369300) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-03-31 23:10:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/ValueInvesting/comments/b6orc4/my_top_picks/'," datetime.datetime(2019, 3, 31, 23, 10, 3, 930490)",'2019-03-31',None,u'0','ValueInvesting',u'My top picks'," u'https://old.reddit.com/r/ValueInvesting/comments/b6orc4/my_top_picks/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 31, 23, 10, 3, 930490) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/thewallstreet/comments/b5jpo1/deviations_poc_and_value_area_for_tuesday_march/'," datetime.datetime(2019, 3, 31, 23, 10, 4, 342988)",'2019-03-31',None,u'11','thewallstreet',"u'Deviations, POC, and Value Area for Tuesday, March 26, 2019'"," u'https://i.imgur.com/ihhfmMU.png'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 31, 23, 10, 4, 342988) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/thewallstreet/comments/b5h0n4/nightly_trading_discussion_march_2526/'," datetime.datetime(2019, 3, 31, 23, 10, 4, 344493)",'2019-03-31',u'Daily',u'14','thewallstreet',u'Nightly Trading Discussion - (March 25/26)'," u'https://old.reddit.com/r/thewallstreet/comments/b5h0n4/nightly_trading_discussion_march_2526/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 31, 23, 10, 4, 344493) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/Trading/new/> (referer: None)
2019-03-31 23:10:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/Trading/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b70oq1/trying_to_find_stocks_related_to_grain_futures/'," datetime.datetime(2019, 3, 31, 23, 10, 5, 302563)",'2019-03-31',None,u'1','StockMarket',u'Trying to find stocks related to grain futures'," u'https://old.reddit.com/r/StockMarket/comments/b70oq1/trying_to_find_stocks_related_to_grain_futures/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 31, 23, 10, 5, 302563) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b6zpr1/algorithmic_trading_is_it_the_ideal_place_to_start/'," datetime.datetime(2019, 3, 31, 23, 10, 5, 304838)",'2019-03-31',None,u'6','StockMarket',u'Algorithmic Trading: Is it the ideal place to start?'," u'https://old.reddit.com/r/StockMarket/comments/b6zpr1/algorithmic_trading_is_it_the_ideal_place_to_start/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 31, 23, 10, 5, 304838) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b6zf42/the_yield_curve_just_uninverted_again/'," datetime.datetime(2019, 3, 31, 23, 10, 5, 306860)",'2019-03-31',None,u'247','StockMarket',u'The Yield Curve Just Uninverted Again!'," u'https://old.reddit.com/r/StockMarket/comments/b6zf42/the_yield_curve_just_uninverted_again/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 31, 23, 10, 5, 306860) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b6zdvo/lyft_ipo_entering/'," datetime.datetime(2019, 3, 31, 23, 10, 5, 308987)",'2019-03-31',None,u'1','StockMarket',u'Lyft IPO Entering'," u'https://old.reddit.com/r/StockMarket/comments/b6zdvo/lyft_ipo_entering/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 31, 23, 10, 5, 308987) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b6y2bn/dollar_is_being_dumped/'," datetime.datetime(2019, 3, 31, 23, 10, 5, 312315)",'2019-03-31',None,u'0','StockMarket',u'Dollar is being dumped.'," u'https://old.reddit.com/r/StockMarket/comments/b6y2bn/dollar_is_being_dumped/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 31, 23, 10, 5, 312315) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b6xy2w/where_is_lyft_ipo/'," datetime.datetime(2019, 3, 31, 23, 10, 5, 317154)",'2019-03-31',None,u'0','StockMarket',u'Where is lyft ipo?'," u'https://old.reddit.com/r/StockMarket/comments/b6xy2w/where_is_lyft_ipo/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 31, 23, 10, 5, 317154) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b6qyfi/does_the_china_trade_deal_get_done/'," datetime.datetime(2019, 3, 31, 23, 10, 5, 319686)",'2019-03-31',None,u'2','StockMarket',u'Does the china trade deal get done?'," u'https://old.reddit.com/r/StockMarket/comments/b6qyfi/does_the_china_trade_deal_get_done/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 31, 23, 10, 5, 319686) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/Daytrading/new/> (referer: None)
2019-03-31 23:10:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/Daytrading/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Daytrading/comments/b6gidn/dollar_index_used_to_predict_crude_oil_100_days/'," datetime.datetime(2019, 3, 31, 23, 10, 5, 611513)",'2019-03-31',None,u'0','Daytrading',u'Dollar Index used to predict Crude oil - 100 DAYS IN ADVANCE'," u'https://www.reddit.com/user/maverick91100/comments/b6fwzp/dollar_index_used_to_predict_crude_oil_100_days/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 31, 23, 10, 5, 611513) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Daytrading/comments/b6ev83/im_back_landed_a_killer_short_on_cron/'," datetime.datetime(2019, 3, 31, 23, 10, 5, 614660)",'2019-03-31',None,u'10','Daytrading',"u""I'm Back! Landed a killer short on CRON"""," u'https://youtu.be/6BkCRXyR9Qw'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 31, 23, 10, 5, 614660) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Daytrading/comments/b6dp8j/im_new_to_this_community_and_need_advice/'," datetime.datetime(2019, 3, 31, 23, 10, 5, 616963)",'2019-03-31',None,u'2','Daytrading',u'I\u2019m new to this community and need advice'," u'https://old.reddit.com/r/Daytrading/comments/b6dp8j/im_new_to_this_community_and_need_advice/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 3, 31, 23, 10, 5, 616963) is not JSON serializable
INFO:scrapy.core.engine:Closing spider (finished)
2019-03-31 23:10:05 [scrapy.core.engine] INFO: Closing spider (finished)
INFO:scrapy.statscollectors:Dumping Scrapy stats:
"
https://old.reddit.com/r/wallstreetbets/comments/b7wj93/investing_101/'," datetime.datetime(2019, 4, 1, 12, 10, 3, 54933)",'2019-04-01',u'Meme',u'55','wallstreetbets',u'Investing 101'," u'https://imgur.com/a/OJNtMW6'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 10, 3, 54933) is not JSON serializable
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
2019-04-01 12:10:03 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
2019-04-01 12:10:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b737rp/y_did_lyft_stock_tank/'," datetime.datetime(2019, 4, 1, 12, 10, 6, 361026)",'2019-04-01',None,u'0','StockMarket',u'Y did lyft stock tank ?'," u'https://old.reddit.com/r/StockMarket/comments/b737rp/y_did_lyft_stock_tank/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 10, 6, 361026) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/Daytrading/new/> (referer: None)
2019-04-01 12:10:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/Daytrading/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b7x00w/there_was_a_post_last_year_with_aggregate_data/'," datetime.datetime(2019, 4, 1, 12, 25, 2, 171979)",'2019-04-01',u'Discussion',u'52','wallstreetbets',u'There was a post last year with aggregate data about what would happen if you sold your holdings at the end of the day and rebought at market open. It had ridiculous figures/theoretical gains.'," u'https://old.reddit.com/r/wallstreetbets/comments/b7x00w/there_was_a_post_last_year_with_aggregate_data/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 25, 2, 171979) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b7wvef/serious_question_do_most_of_you_do_regular_95/'," datetime.datetime(2019, 4, 1, 12, 25, 2, 173694)",'2019-04-01',u'Discussion',u'25','wallstreetbets',"u'(Serious Question) Do most of you do regular 9-5 jobs? If yes, how do you get time during the day to trade?'"," u'https://old.reddit.com/r/wallstreetbets/comments/b7wvef/serious_question_do_most_of_you_do_regular_95/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 25, 2, 173694) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
2019-04-01 12:25:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
2019-04-01 12:25:02 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b7uog6/gold_buyin_as_an_investment_or_hedge/'," datetime.datetime(2019, 4, 1, 12, 40, 5, 152986)",'2019-04-01',None,u'8','investing',u'Gold Buyin as an Investment or Hedge?'," u'https://old.reddit.com/r/investing/comments/b7uog6/gold_buyin_as_an_investment_or_hedge/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 40, 5, 152986) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-04-01 12:40:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b7g8f6/free_visual_stock_watchlist/'," datetime.datetime(2019, 4, 1, 12, 45, 2, 217103)",'2019-04-01',None,u'1','stocks',u'Free Visual Stock Watchlist'," u'https://old.reddit.com/r/stocks/comments/b7g8f6/free_visual_stock_watchlist/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 45, 2, 217103) is not JSON serializable
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
2019-04-01 12:45:02 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
2019-04-01 12:45:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b7vf9s/investing_and_subjective_theory_of_value/'," datetime.datetime(2019, 4, 1, 12, 45, 3, 304285)",'2019-04-01',None,u'7','investing',u'Investing and Subjective Theory of Value'," u'https://old.reddit.com/r/investing/comments/b7vf9s/investing_and_subjective_theory_of_value/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 45, 3, 304285) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b7v39v/wash_sale_disallowed/'," datetime.datetime(2019, 4, 1, 12, 45, 3, 306168)",'2019-04-01',None,u'3','investing',u'Wash Sale Disallowed'," u'https://old.reddit.com/r/investing/comments/b7v39v/wash_sale_disallowed/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 45, 3, 306168) is not JSON serializable
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Daytrading/new/> from <GET https://old.reddit.com/r/daytrading/new/>
2019-04-01 12:45:03 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Daytrading/new/> from <GET https://old.reddit.com/r/daytrading/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-04-01 12:45:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b7yy6a/a_huge_step_for_marijuana_companies_buying_on/'," datetime.datetime(2019, 4, 1, 12, 50, 2, 16923)",'2019-04-01',u'Stocks',u'3','wallstreetbets',u'A huge step for marijuana companies buying on margin for $GTBIF'," u'https://old.reddit.com/r/wallstreetbets/comments/b7yy6a/a_huge_step_for_marijuana_companies_buying_on/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 50, 2, 16923) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b7xkdj/the_most_terrifying_statistic_of_the_lyft_ipo/'," datetime.datetime(2019, 4, 1, 12, 50, 2, 18516)",'2019-04-01',u'Discussion',u'39','wallstreetbets',u'The Most Terrifying Statistic of the Lyft IPO'," u'https://medium.com/@nicholas_hathaway/the-most-terrifying-statistic-of-the-lyft-ipo-cd5de724e5e2'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 50, 2, 18516) is not JSON serializable
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
2019-04-01 12:50:02 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
2019-04-01 12:50:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b83ejr/remember_18_months_ago_when_gm_said_it_was_going/'," datetime.datetime(2019, 4, 1, 12, 55, 2, 226442)",'2019-04-01',u'Stocks',u'12','wallstreetbets',u'Remember 18 months ago when GM said it was going to introduce 2 new electric vehicles in the next 18 months? Where are they? Short $GM'," u'https://media.gm.com/media/us/en/gm/news.detail.html/content/Pages/news/us/en/2017/oct/1002-electric.html'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 55, 2, 226442) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b837ub/what_happened_to_saudi_aramco_ipo_why_do_we_get_a/'," datetime.datetime(2019, 4, 1, 12, 55, 2, 229347)",'2019-04-01',u'Discussion',u'17','wallstreetbets',u'What happened to Saudi Aramco IPO? Why do we get a piece of shit Lfyt IPO but not Saudi Aramco?'," u'https://twitter.com/i/moments/1112677532787195904'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 55, 2, 229347) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b833pg/week_13_stock_picking_contest_results_the_winner/'," datetime.datetime(2019, 4, 1, 12, 55, 2, 233773)",'2019-04-01',u'Discussion',u'10','wallstreetbets',u'Week #13 Stock Picking Contest Results: The Winner is CrossDamon with $SEEL Pick and 24.3% Return In 1 Week!'," u'https://www.reddit.com/user/Fatherthinger/comments/b82wyo/week_13_stock_picking_contest_results_the_winner/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 55, 2, 233773) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b82sv3/the_premiums_are_going_to_be_insane/'," datetime.datetime(2019, 4, 1, 12, 55, 2, 236836)",'2019-04-01',u'Meme',u'928','wallstreetbets',u'The premiums are going to be insane.'," u'https://old.reddit.com/r/wallstreetbets/comments/b82sv3/the_premiums_are_going_to_be_insane/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 55, 2, 236836) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b82mwd/basket_trader_indicator_dashboard_provide_me/'," datetime.datetime(2019, 4, 1, 12, 55, 2, 239838)",'2019-04-01',u'Technicals',u'5','wallstreetbets',u'Basket Trader Indicator Dashboard- provide me feedback'," u'https://datastudio.google.com/embed/reporting/1ezZrjTHeOAQtjl18Gj2TR2FP-St_QuYc/page/z9sl'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 55, 2, 239838) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b82gs6/apple_stock_a_new_era_of_mobile_saturation/'," datetime.datetime(2019, 4, 1, 12, 55, 2, 241838)",'2019-04-01',u'Stocks',u'0','wallstreetbets',u'Apple Stock: A New Era of Mobile Saturation'," u'https://old.reddit.com/r/wallstreetbets/comments/b82gs6/apple_stock_a_new_era_of_mobile_saturation/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 55, 2, 241838) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b82ask/watch_for_the_double_top_on_spy_will_most_likely/'," datetime.datetime(2019, 4, 1, 12, 55, 2, 243412)",'2019-04-01',u'Technicals',u'5','wallstreetbets',"u'Watch for the double top on SPY, will most likely act as a point of resistance.'"," u'https://old.reddit.com/r/wallstreetbets/comments/b82ask/watch_for_the_double_top_on_spy_will_most_likely/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 55, 2, 243412) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8288d/beta_music_group_inc_bemg_completes_share/'," datetime.datetime(2019, 4, 1, 12, 55, 2, 245011)",'2019-04-01',u'Discussion',u'3','wallstreetbets',u'Beta Music Group Inc ($BEMG) Completes Share Reduction Of 75% Of Its Common Shares; Lyft Deal Complete'," u'https://old.reddit.com/r/wallstreetbets/comments/b8288d/beta_music_group_inc_bemg_completes_share/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 55, 2, 245011) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8214h/when_you_meet_your_soulmate_on_wsb/'," datetime.datetime(2019, 4, 1, 12, 55, 2, 246587)",'2019-04-01',u'Satire',u'23','wallstreetbets',u'When you meet your soulmate on WSB'," u'https://youtu.be/wFlKIQiARRE'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 55, 2, 246587) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b81ted/subscribe_to_pewdiepie_also_april_fools/'," datetime.datetime(2019, 4, 1, 12, 55, 2, 248175)",'2019-04-01',u'Mods',u'98','wallstreetbets',"u'Subscribe to Pewdiepie, also April Fools'"," u'https://old.reddit.com/r/wallstreetbets/comments/b81ted/subscribe_to_pewdiepie_also_april_fools/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 55, 2, 248175) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b817tq/chinese_govt_today_after_seeing_futures_up/'," datetime.datetime(2019, 4, 1, 12, 55, 2, 249761)",'2019-04-01',u'Shitpost',u'245','wallstreetbets',u'Chinese govt today after seeing futures up'," u'https://imgflip.com/i/2xglmb'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 55, 2, 249761) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b80u1x/daily_discussion_thread_april_01_2019/'," datetime.datetime(2019, 4, 1, 12, 55, 2, 251350)",'2019-04-01',u'Daily Discussion',u'33','wallstreetbets',"u'Daily Discussion Thread - April 01, 2019'"," u'https://old.reddit.com/r/wallstreetbets/comments/b80u1x/daily_discussion_thread_april_01_2019/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 55, 2, 251350) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b7zfhb/elon_musk_releases_rap_song_about_harambe/'," datetime.datetime(2019, 4, 1, 12, 55, 2, 252981)",'2019-04-01',u'Storytime',u'187','wallstreetbets',u'Elon Musk releases rap song about Harambe.'," u'https://old.reddit.com/r/wallstreetbets/comments/b7zfhb/elon_musk_releases_rap_song_about_harambe/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 55, 2, 252981) is not JSON serializable
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
2019-04-01 12:55:02 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b82zkp/apple_slashes_iphone_prices_in_china/'," datetime.datetime(2019, 4, 1, 12, 55, 2, 325279)",'2019-04-01',None,u'5','stocks',u'Apple slashes iPhone prices in China'," u'https://old.reddit.com/r/stocks/comments/b82zkp/apple_slashes_iphone_prices_in_china/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 55, 2, 325279) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b81vrb/todays_premarket_news_monday_april_1st_2019/'," datetime.datetime(2019, 4, 1, 12, 55, 2, 327091)",'2019-04-01',u'News',u'13','stocks',"u""Today's Pre-Market News [Monday, April 1st, 2019]"""," u'https://old.reddit.com/r/stocks/comments/b81vrb/todays_premarket_news_monday_april_1st_2019/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 55, 2, 327091) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b81hyy/rstocks_daily_discussion_monday_apr_01_2019/'," datetime.datetime(2019, 4, 1, 12, 55, 2, 328749)",'2019-04-01',None,u'3','stocks',"u'r/Stocks Daily Discussion Monday - Apr 01, 2019'"," u'https://old.reddit.com/r/stocks/comments/b81hyy/rstocks_daily_discussion_monday_apr_01_2019/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 55, 2, 328749) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b819vp/no_joke_on_this_april_fools_day_global_markets/'," datetime.datetime(2019, 4, 1, 12, 55, 2, 330520)",'2019-04-01',u'News',u'100','stocks',"u'No joke on this April Fools Day, global markets are rallying to open the quarter'"," u'https://old.reddit.com/r/stocks/comments/b819vp/no_joke_on_this_april_fools_day_global_markets/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 55, 2, 330520) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b7yigc/opiate_epidemic_stocksany_good_buy_opportunities/'," datetime.datetime(2019, 4, 1, 12, 55, 2, 332176)",'2019-04-01',None,u'1','stocks',u'Opiate epidemic stocks\u2014any good buy opportunities? ALKS TTNP?'," u'https://old.reddit.com/r/stocks/comments/b7yigc/opiate_epidemic_stocksany_good_buy_opportunities/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 55, 2, 332176) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b7vzxk/does_wtw_really_work/'," datetime.datetime(2019, 4, 1, 12, 55, 2, 334068)",'2019-04-01',u'Question',u'0','stocks',u'Does WTW really work?'," u'https://old.reddit.com/r/stocks/comments/b7vzxk/does_wtw_really_work/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 55, 2, 334068) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b7up04/stock_scanning/'," datetime.datetime(2019, 4, 1, 12, 55, 2, 335908)",'2019-04-01',None,u'2','stocks',u'Stock scanning'," u'https://old.reddit.com/r/stocks/comments/b7up04/stock_scanning/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 55, 2, 335908) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b7u3ng/just_found_out_i_can_claim_1000_shares_of/'," datetime.datetime(2019, 4, 1, 12, 55, 2, 337548)",'2019-04-01',None,u'2','stocks',u'Just found out I can claim 1000 shares of \u201cSilvergate Capital Corp\u201d. Any idea how much it\u2019s worth? Want to know if it\u2019s worth the trouble of claiming it.'," u'https://old.reddit.com/r/stocks/comments/b7u3ng/just_found_out_i_can_claim_1000_shares_of/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 55, 2, 337548) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b7s6bf/will_game_of_thrones_affect_atts_stock_price/'," datetime.datetime(2019, 4, 1, 12, 55, 2, 339109)",'2019-04-01',None,u'0','stocks',u'Will Game of Thrones Affect AT&T\u2018s stock price?'," u'https://old.reddit.com/r/stocks/comments/b7s6bf/will_game_of_thrones_affect_atts_stock_price/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 55, 2, 339109) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b7rgqc/have_never_invested_before_is_it_worth_picking/'," datetime.datetime(2019, 4, 1, 12, 55, 2, 340605)",'2019-04-01',None,u'10','stocks',u'Have never invested before. Is it worth picking individual stocks when an ETF holds all my picks?'," u'https://old.reddit.com/r/stocks/comments/b7rgqc/have_never_invested_before_is_it_worth_picking/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 55, 2, 340605) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b7qwyq/is_there_to_a_way_to_see_real_time_institutional/'," datetime.datetime(2019, 4, 1, 12, 55, 2, 342310)",'2019-04-01',None,u'1','stocks',u'Is there to a way to see real time institutional holdings?'," u'https://old.reddit.com/r/stocks/comments/b7qwyq/is_there_to_a_way_to_see_real_time_institutional/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 55, 2, 342310) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b7qhx2/asx_cannabis_stocks/'," datetime.datetime(2019, 4, 1, 12, 55, 2, 344031)",'2019-04-01',None,u'0','stocks',u'ASX Cannabis Stocks?'," u'https://old.reddit.com/r/stocks/comments/b7qhx2/asx_cannabis_stocks/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 55, 2, 344031) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b7qhec/fte_networks/'," datetime.datetime(2019, 4, 1, 12, 55, 2, 345720)",'2019-04-01',None,u'1','stocks',u'FTE networks'," u'https://old.reddit.com/r/stocks/comments/b7qhec/fte_networks/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 55, 2, 345720) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b7pcbj/how_to_determine_if_an_ipo_is_over_or_underpriced/'," datetime.datetime(2019, 4, 1, 12, 55, 2, 347361)",'2019-04-01',None,u'1','stocks',u'How to determine if an IPO is over or underpriced?'," u'https://old.reddit.com/r/stocks/comments/b7pcbj/how_to_determine_if_an_ipo_is_over_or_underpriced/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 55, 2, 347361) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b7ogc5/the_tqqq_etf/'," datetime.datetime(2019, 4, 1, 12, 55, 2, 349067)",'2019-04-01',None,u'0','stocks',u'The TQQQ etf'," u'https://old.reddit.com/r/stocks/comments/b7ogc5/the_tqqq_etf/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 55, 2, 349067) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b7nwb0/why_are_gas_prices_on_the_rise/'," datetime.datetime(2019, 4, 1, 12, 55, 2, 351373)",'2019-04-01',None,u'168','stocks',u'Why are gas prices on the rise?'," u'https://old.reddit.com/r/stocks/comments/b7nwb0/why_are_gas_prices_on_the_rise/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 55, 2, 351373) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b7nri9/how_apple_tv_plus_compares_to_netflix_amazon/'," datetime.datetime(2019, 4, 1, 12, 55, 2, 353388)",'2019-04-01',None,u'30','stocks',"u'How Apple TV Plus compares to Netflix, Amazon Prime Video, and Hulu'"," u'https://old.reddit.com/r/stocks/comments/b7nri9/how_apple_tv_plus_compares_to_netflix_amazon/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 55, 2, 353388) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b7mdi3/website_that_tells_you_what_type_of_stock_a_stock/'," datetime.datetime(2019, 4, 1, 12, 55, 2, 355476)",'2019-04-01',None,u'0','stocks',u'Website that tells you what type of stock a stock is?'," u'https://old.reddit.com/r/stocks/comments/b7mdi3/website_that_tells_you_what_type_of_stock_a_stock/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 55, 2, 355476) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b7lx0j/trying_to_do_some_research_for_my_ira_fsrpx_vti/'," datetime.datetime(2019, 4, 1, 12, 55, 2, 357330)",'2019-04-01',None,u'0','stocks',u'Trying to do some research for my IRA. $FSRPX + $VTI thoughts'," u'https://old.reddit.com/r/stocks/comments/b7lx0j/trying_to_do_some_research_for_my_ira_fsrpx_vti/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 55, 2, 357330) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b7hraa/deferred_comp_457/'," datetime.datetime(2019, 4, 1, 12, 55, 2, 359640)",'2019-04-01',None,u'11','stocks',u'Deferred Comp. 457'," u'https://old.reddit.com/r/stocks/comments/b7hraa/deferred_comp_457/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 55, 2, 359640) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b7glzn/stock_picks_for_2020_election/'," datetime.datetime(2019, 4, 1, 12, 55, 2, 361899)",'2019-04-01',None,u'0','stocks',u'Stock picks for 2020 election'," u'https://old.reddit.com/r/stocks/comments/b7glzn/stock_picks_for_2020_election/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 55, 2, 361899) is not JSON serializable
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
2019-04-01 12:55:02 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
2019-04-01 12:55:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b837fn/udemy_financial_courses/'," datetime.datetime(2019, 4, 1, 12, 55, 3, 354064)",'2019-04-01',None,u'2','investing',u'UDEMY Financial Courses'," u'https://old.reddit.com/r/investing/comments/b837fn/udemy_financial_courses/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 55, 3, 354064) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b8360d/taking_a_personal_loan_to_day_trade/'," datetime.datetime(2019, 4, 1, 12, 55, 3, 355830)",'2019-04-01',u'Help',u'0','investing',u'Taking a personal loan to day trade.'," u'https://old.reddit.com/r/investing/comments/b8360d/taking_a_personal_loan_to_day_trade/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 55, 3, 355830) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b831qa/is_security_analysis_by_graham_and_dodd_outdated/'," datetime.datetime(2019, 4, 1, 12, 55, 3, 358052)",'2019-04-01',None,u'3','investing',u'Is Security Analysis by Graham and Dodd outdated?'," u'https://old.reddit.com/r/investing/comments/b831qa/is_security_analysis_by_graham_and_dodd_outdated/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 55, 3, 358052) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b82uzg/reits_or_mutual_funds/'," datetime.datetime(2019, 4, 1, 12, 55, 3, 360049)",'2019-04-01',None,u'2','investing',u'REITs or Mutual Funds'," u'https://old.reddit.com/r/investing/comments/b82uzg/reits_or_mutual_funds/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 55, 3, 360049) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b82tti/stocks_real_estate_business_what_else_could_a/'," datetime.datetime(2019, 4, 1, 12, 55, 3, 361851)",'2019-04-01',None,u'1','investing',"u'Stocks, real estate, business. What else could a person invest in?'"," u'https://old.reddit.com/r/investing/comments/b82tti/stocks_real_estate_business_what_else_could_a/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 55, 3, 361851) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b828wo/i_own_some_shares_of_ecobalt_ecsif_what_does_this/'," datetime.datetime(2019, 4, 1, 12, 55, 3, 363841)",'2019-04-01',None,u'1','investing',u'I own some shares of eCobalt (ecsif) what does this news mean for me. It seems eCobalt is to be acquired by another company.'," u'https://old.reddit.com/r/investing/comments/b828wo/i_own_some_shares_of_ecobalt_ecsif_what_does_this/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 55, 3, 363841) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b81qjw/despite_the_meh_economic_signs_the_stock_market/'," datetime.datetime(2019, 4, 1, 12, 55, 3, 367722)",'2019-04-01',None,u'6','investing',"u'Despite the meh economic signs, the stock market keeps going up. How HIGH are we going once an actual trade deal is struck?'"," u'https://old.reddit.com/r/investing/comments/b81qjw/despite_the_meh_economic_signs_the_stock_market/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 55, 3, 367722) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b81mc9/buying_americanforeign_stocks_online/'," datetime.datetime(2019, 4, 1, 12, 55, 3, 369520)",'2019-04-01',None,u'0','investing',u'Buying american/foreign stocks online'," u'https://old.reddit.com/r/investing/comments/b81mc9/buying_americanforeign_stocks_online/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 55, 3, 369520) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b81bil/with_net_income_of_1111_billion_saudi_aramco/'," datetime.datetime(2019, 4, 1, 12, 55, 3, 371520)",'2019-04-01',None,u'901','investing',"u""With net income of $111.1 Billion, Saudi Aramco Confirmed as World's most Profitable company"""," u'https://old.reddit.com/r/investing/comments/b81bil/with_net_income_of_1111_billion_saudi_aramco/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 55, 3, 371520) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b8048q/looking_for_some_value_investing_business/'," datetime.datetime(2019, 4, 1, 12, 55, 3, 373831)",'2019-04-01',None,u'0','investing',u'Looking for some value investing business'," u'https://old.reddit.com/r/investing/comments/b8048q/looking_for_some_value_investing_business/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 55, 3, 373831) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b7zol6/accumulator_etf_question_how_are_dividend_taxed/'," datetime.datetime(2019, 4, 1, 12, 55, 3, 375697)",'2019-04-01',None,u'2','investing',u'Accumulator ETF Question : How are dividend taxed?'," u'https://old.reddit.com/r/investing/comments/b7zol6/accumulator_etf_question_how_are_dividend_taxed/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 55, 3, 375697) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b7y61t/why_is_price_of_brent_crude_a_little_off_on_yahoo/'," datetime.datetime(2019, 4, 1, 12, 55, 3, 377527)",'2019-04-01',u'Education',u'1','investing',u'Why is price of Brent Crude a little off on Yahoo Finance?'," u'https://old.reddit.com/r/investing/comments/b7y61t/why_is_price_of_brent_crude_a_little_off_on_yahoo/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 55, 3, 377527) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b7xjh2/roth_should_i_contribute_to_2018_before_april_15/'," datetime.datetime(2019, 4, 1, 12, 55, 3, 390031)",'2019-04-01',None,u'0','investing',"u'(Roth) Should I contribute to 2018 before April 15, or start now with 2019?'"," u'https://old.reddit.com/r/investing/comments/b7xjh2/roth_should_i_contribute_to_2018_before_april_15/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 55, 3, 390031) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b7x0m1/rakuten_says_to_book_990_mln_gain_on_lyft/'," datetime.datetime(2019, 4, 1, 12, 55, 3, 392190)",'2019-04-01',None,u'10','investing',u'Rakuten says to book $990 mln gain on Lyft investment'," u'https://old.reddit.com/r/investing/comments/b7x0m1/rakuten_says_to_book_990_mln_gain_on_lyft/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 55, 3, 392190) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b7wko4/wtf_is_up_with_insulet_podd/'," datetime.datetime(2019, 4, 1, 12, 55, 3, 393911)",'2019-04-01',None,u'2','investing',u'WTF is up with Insulet? ($PODD)'," u'https://old.reddit.com/r/investing/comments/b7wko4/wtf_is_up_with_insulet_podd/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 55, 3, 393911) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b7w9bx/contributing_to_roth_401k_if_there_is_no_match/'," datetime.datetime(2019, 4, 1, 12, 55, 3, 395629)",'2019-04-01',None,u'1','investing',u'Contributing to Roth 401k if there is no match?'," u'https://old.reddit.com/r/investing/comments/b7w9bx/contributing_to_roth_401k_if_there_is_no_match/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 55, 3, 395629) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b7vo0x/does_anyone_here_use_firstradecom/'," datetime.datetime(2019, 4, 1, 12, 55, 3, 397282)",'2019-04-01',u'Discussion',u'0','investing',u'Does Anyone Here Use Firstrade.com?'," u'https://old.reddit.com/r/investing/comments/b7vo0x/does_anyone_here_use_firstradecom/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 55, 3, 397282) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b7vke7/what_is_a_good_target_for_buying_10_year/'," datetime.datetime(2019, 4, 1, 12, 55, 3, 398908)",'2019-04-01',u'Help',u'0','investing',u'What is a good target for buying 10 year Treasuries?'," u'https://old.reddit.com/r/investing/comments/b7vke7/what_is_a_good_target_for_buying_10_year/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 55, 3, 398908) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b7vkdx/is_there_a_website_that_is_similar_to/'," datetime.datetime(2019, 4, 1, 12, 55, 3, 400502)",'2019-04-01',u'Help',u'0','investing',u'Is there a website that is similar to coinmarketcap except for equities?'," u'https://old.reddit.com/r/investing/comments/b7vkdx/is_there_a_website_that_is_similar_to/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 55, 3, 400502) is not JSON serializable
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Daytrading/new/> from <GET https://old.reddit.com/r/daytrading/new/>
2019-04-01 12:55:03 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Daytrading/new/> from <GET https://old.reddit.com/r/daytrading/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-04-01 12:55:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/thewallstreet/comments/b60bcw/daily_spx_tpos_03262019/'," datetime.datetime(2019, 4, 1, 12, 55, 4, 393374)",'2019-04-01',None,u'9','thewallstreet',"u""Daily SPX TPO's 03-26-2019"""," u'https://i.imgur.com/AahLGhw.png'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 55, 4, 393374) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/thewallstreet/comments/b5yys6/deviations_poc_and_value_area_for_wednesday_march/'," datetime.datetime(2019, 4, 1, 12, 55, 4, 395092)",'2019-04-01',None,u'9','thewallstreet',"u'Deviations, POC, and Value Area for Wednesday, March 27, 2019'"," u'https://i.imgur.com/o1BFUy0.png'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 55, 4, 395092) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/thewallstreet/comments/b5vv9j/nightly_trading_discussion_march_2627/'," datetime.datetime(2019, 4, 1, 12, 55, 4, 396871)",'2019-04-01',u'Daily',u'15','thewallstreet',u'Nightly Trading Discussion - (March 26/27)'," u'https://old.reddit.com/r/thewallstreet/comments/b5vv9j/nightly_trading_discussion_march_2627/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 55, 4, 396871) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/thewallstreet/comments/b5uhem/post_market_discussion_march_26/'," datetime.datetime(2019, 4, 1, 12, 55, 4, 398531)",'2019-04-01',u'Daily',u'13','thewallstreet',u'Post Market Discussion - (March 26)'," u'https://old.reddit.com/r/thewallstreet/comments/b5uhem/post_market_discussion_march_26/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 55, 4, 398531) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/thewallstreet/comments/b5oksj/daily_discussion_march_26/'," datetime.datetime(2019, 4, 1, 12, 55, 4, 400179)",'2019-04-01',u'Daily',u'10','thewallstreet',u'Daily Discussion - (March 26)'," u'https://old.reddit.com/r/thewallstreet/comments/b5oksj/daily_discussion_march_26/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 55, 4, 400179) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/thewallstreet/comments/b5llj0/daily_spx_tpos_03252019/'," datetime.datetime(2019, 4, 1, 12, 55, 4, 402051)",'2019-04-01',None,u'14','thewallstreet',"u""Daily SPX TPO's 03-25-2019"""," u'https://i.imgur.com/jS9xh2Y.png'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 55, 4, 402051) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/Trading/new/> (referer: None)
2019-04-01 12:55:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/Trading/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Trading/comments/azy0nx/sell_sp_500_buy_cboe_volatility_index_trade_ideas/'," datetime.datetime(2019, 4, 1, 12, 55, 4, 656029)",'2019-04-01',None,u'0','Trading',u'Sell S&P 500 Buy CBOE Volatility Index Trade Ideas'," u'https://old.reddit.com/r/Trading/comments/azy0nx/sell_sp_500_buy_cboe_volatility_index_trade_ideas/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 55, 4, 656029) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Trading/comments/azuc3q/what_do_you_guys_think_about_shopify_stock_right/'," datetime.datetime(2019, 4, 1, 12, 55, 4, 657557)",'2019-04-01',None,u'2','Trading',u'What do you guys think about Shopify stock right now? It has increased dramatically during 3 months. Is it time to sell?'," u'https://old.reddit.com/r/Trading/comments/azuc3q/what_do_you_guys_think_about_shopify_stock_right/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 55, 4, 657557) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/Stock_Picks/new/> (referer: None)
2019-04-01 12:55:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/Stock_Picks/new/> (referer: None)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/StockMarket/new/> (referer: None)
2019-04-01 12:55:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/StockMarket/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Stock_Picks/comments/awav9p/us_music_industry_posts_third_straight_year_of/'," datetime.datetime(2019, 4, 1, 12, 55, 5, 305387)",'2019-04-01',None,u'5','Stock_Picks',u'U.S. Music Industry Posts Third Straight Year of Double-Digit Growth'," u'https://variety.com/2019/biz/news/u-s-music-industry-posts-third-straight-year-of-double-digit-growth-as-streaming-soars-30-1203152036/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 55, 5, 305387) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Stock_Picks/comments/aw71n9/lyft_just_dropped_its_filing_to_go_public/'," datetime.datetime(2019, 4, 1, 12, 55, 5, 306869)",'2019-04-01',u'News',u'39','Stock_Picks',"u'Lyft just dropped its filing to go public, revealing financials for the first time'"," u'https://www.cnbc.com/2019/03/01/lyft-s-1-ipo.html'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 55, 5, 306869) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b7wndx/lyft_lyft_inc_and_uber_could_face_major_hurdle_if/'," datetime.datetime(2019, 4, 1, 12, 55, 5, 334344)",'2019-04-01',None,u'133','StockMarket',"u'$LYFT - Lyft, Inc. and Uber Could Face Major Hurdle if CA Bill Forcing Independent Contractors to Become Employees Becomes Law'"," u'https://old.reddit.com/r/StockMarket/comments/b7wndx/lyft_lyft_inc_and_uber_could_face_major_hurdle_if/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 55, 5, 334344) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b7w819/new_investment_form/'," datetime.datetime(2019, 4, 1, 12, 55, 5, 336215)",'2019-04-01',None,u'3','StockMarket',u'New Investment Form'," u'https://old.reddit.com/r/StockMarket/comments/b7w819/new_investment_form/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 55, 5, 336215) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b7swwj/the_official_rstockmarket_discord_server_live/'," datetime.datetime(2019, 4, 1, 12, 55, 5, 338009)",'2019-04-01',u'Live Chat',u'28','StockMarket',"u'The Official r/StockMarket Discord Server Live Chat, link on the right -->'"," u'https://old.reddit.com/r/StockMarket/comments/b7swwj/the_official_rstockmarket_discord_server_live/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 55, 5, 338009) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b7s2qv/question_about_think_or_swim/'," datetime.datetime(2019, 4, 1, 12, 55, 5, 339959)",'2019-04-01',None,u'0','StockMarket',u'Question about Think Or Swim'," u'https://old.reddit.com/r/StockMarket/comments/b7s2qv/question_about_think_or_swim/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 55, 5, 339959) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b7mgym/ktos_kratos_year_2000/'," datetime.datetime(2019, 4, 1, 12, 55, 5, 341825)",'2019-04-01',None,u'0','StockMarket',"u'$KTOS, Kratos year 2000?'"," u'https://old.reddit.com/r/StockMarket/comments/b7mgym/ktos_kratos_year_2000/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 55, 5, 341825) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b7kprw/biogen_buy_or_sell/'," datetime.datetime(2019, 4, 1, 12, 55, 5, 343650)",'2019-04-01',None,u'1','StockMarket',u'BIOGEN: Buy or Sell?'," u'https://old.reddit.com/r/StockMarket/comments/b7kprw/biogen_buy_or_sell/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 55, 5, 343650) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b7htdp/saudis_gained_access_to_amazon_ceo_bezos_phone/'," datetime.datetime(2019, 4, 1, 12, 55, 5, 345528)",'2019-04-01',None,u'355','StockMarket',"u""Saudis gained access to Amazon CEO Bezos' phone: Bezos' security chief"""," u'https://old.reddit.com/r/StockMarket/comments/b7htdp/saudis_gained_access_to_amazon_ceo_bezos_phone/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 55, 5, 345528) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b7gusl/5000_to_invest/'," datetime.datetime(2019, 4, 1, 12, 55, 5, 347423)",'2019-04-01',None,u'0','StockMarket',u'\xa35000 to invest.'," u'https://old.reddit.com/r/StockMarket/comments/b7gusl/5000_to_invest/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 55, 5, 347423) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b7ekt1/is_this_an_earnings_recession_if_revenues_are/'," datetime.datetime(2019, 4, 1, 12, 55, 5, 350113)",'2019-04-01',None,u'5','StockMarket',u'Is this an earnings recession if revenues are growing?'," u'https://old.reddit.com/r/StockMarket/comments/b7ekt1/is_this_an_earnings_recession_if_revenues_are/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 55, 5, 350113) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b7d4dl/wall_street_week_ahead_for_the_trading_week/'," datetime.datetime(2019, 4, 1, 12, 55, 5, 352217)",'2019-04-01',u'News',u'11','StockMarket',"u'Wall Street Week Ahead for the trading week beginning April 1st, 2019'"," u'https://old.reddit.com/r/StockMarket/comments/b7d4dl/wall_street_week_ahead_for_the_trading_week/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 55, 5, 352217) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b7b91t/what_stocks_are_you_looking_at_this_weekend/'," datetime.datetime(2019, 4, 1, 12, 55, 5, 355321)",'2019-04-01',u'Discussion',u'61','StockMarket',u'What stocks are you looking at this weekend?'," u'https://finviz.com/publish/033019/sec_all_w1_092477328.png'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 55, 5, 355321) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b7b2g8/most_anticipated_earnings_releases_for_the_week/'," datetime.datetime(2019, 4, 1, 12, 55, 5, 357594)",'2019-04-01',u'Earnings',u'148','StockMarket',"u'Most Anticipated Earnings Releases for the week beginning April 1st, 2019'"," u'https://old.reddit.com/r/StockMarket/comments/b7b2g8/most_anticipated_earnings_releases_for_the_week/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 55, 5, 357594) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b7awue/congratulations_to_the_rstockmarket_march_2019/'," datetime.datetime(2019, 4, 1, 12, 55, 5, 359716)",'2019-04-01',u'Contest',u'56','StockMarket',u'Congratulations to the r/StockMarket March 2019 Stock Picking Contest WINNER -- SirSquanchyJr!'," u'https://old.reddit.com/r/StockMarket/comments/b7awue/congratulations_to_the_rstockmarket_march_2019/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 55, 5, 359716) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b7a6dm/dcar_what_happened_to_stockprice/'," datetime.datetime(2019, 4, 1, 12, 55, 5, 361615)",'2019-04-01',None,u'5','StockMarket',u'DCAR . What happened to stockprice?'," u'https://old.reddit.com/r/StockMarket/comments/b7a6dm/dcar_what_happened_to_stockprice/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 55, 5, 361615) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b77073/benefiting_from_upcoming_usp800_compliance_law/'," datetime.datetime(2019, 4, 1, 12, 55, 5, 363371)",'2019-04-01',None,u'1','StockMarket',u'Benefiting from Upcoming USP-800 Compliance Law'," u'https://old.reddit.com/r/StockMarket/comments/b77073/benefiting_from_upcoming_usp800_compliance_law/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 55, 5, 363371) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b76tgo/tell_me_your_trading_strategies_and_how_much/'," datetime.datetime(2019, 4, 1, 12, 55, 5, 364858)",'2019-04-01',None,u'14','StockMarket',"u""Tell Me Your Trading Strategies and How Much They've Grown Your Account Over Time"""," u'https://old.reddit.com/r/StockMarket/comments/b76tgo/tell_me_your_trading_strategies_and_how_much/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 55, 5, 364858) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b75cba/quick_question_about_pe_ratios/'," datetime.datetime(2019, 4, 1, 12, 55, 5, 366323)",'2019-04-01',None,u'9','StockMarket',u'Quick Question about P/E ratios?'," u'https://old.reddit.com/r/StockMarket/comments/b75cba/quick_question_about_pe_ratios/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 55, 5, 366323) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/Daytrading/new/> (referer: None)
2019-04-01 12:55:07 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/Daytrading/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Daytrading/comments/b6oqsf/got_dumped_on_by_dcar_hyre_wins_on_hunt_clwt/'," datetime.datetime(2019, 4, 1, 12, 55, 7, 324708)",'2019-04-01',None,u'4','Daytrading',"u'Got dumped on by $DCAR $HYRE, wins on $HUNT $CLWT - Trading penny stock sympathies'"," u'https://www.youtube.com/watch?v=65ubTpOs2wk'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 55, 7, 324708) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Daytrading/comments/b6okn8/day_trading_platform/'," datetime.datetime(2019, 4, 1, 12, 55, 7, 326201)",'2019-04-01',None,u'1','Daytrading',u'Day trading platform?'," u'https://old.reddit.com/r/Daytrading/comments/b6okn8/day_trading_platform/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 55, 7, 326201) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Daytrading/comments/b6kck6/day_3_of_my_papertrading_not_the_best_today/'," datetime.datetime(2019, 4, 1, 12, 55, 7, 327758)",'2019-04-01',None,u'1','Daytrading',"u'""Day 3"" of my papertrading. Not the best today.'"," u'https://old.reddit.com/r/Daytrading/comments/b6kck6/day_3_of_my_papertrading_not_the_best_today/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 1, 12, 55, 7, 327758) is not JSON serializable
INFO:scrapy.core.engine:Closing spider (finished)
2019-04-01 12:55:07 [scrapy.core.engine] INFO: Closing spider (finished)
INFO:scrapy.statscollectors:Dumping Scrapy stats:
"
https://old.reddit.com/r/Daytrading/comments/b6qqtm/lyft_ipo_tomorrow/'," datetime.datetime(2019, 4, 2, 9, 30, 6, 808618)",'2019-04-02',None,u'1','Daytrading',u'Lyft IPO tomorrow?'," u'https://old.reddit.com/r/Daytrading/comments/b6qqtm/lyft_ipo_tomorrow/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 9, 30, 6, 808618) is not JSON serializable
INFO:scrapy.core.engine:Closing spider (finished)
2019-04-02 09:30:06 [scrapy.core.engine] INFO: Closing spider (finished)
INFO:scrapy.statscollectors:Dumping Scrapy stats:
"
https://old.reddit.com/r/wallstreetbets/comments/b8ektr/when_its_almost_midnight_and_you_still_havent/'," datetime.datetime(2019, 4, 2, 9, 35, 2, 217734)",'2019-04-02',u'Meme',u'206','wallstreetbets',u'When it\u2019s almost midnight and you still haven\u2019t been added to the autist team'," u'https://old.reddit.com/r/wallstreetbets/comments/b8ektr/when_its_almost_midnight_and_you_still_havent/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 9, 35, 2, 217734) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8hyzw/if_the_strategy_suggested_here_is_to_invest/'," datetime.datetime(2019, 4, 2, 9, 45, 2, 421818)",'2019-04-02',None,u'1','wallstreetbets',"u'If the strategy suggested here is to invest inverse of WSB, then shouldn\u2019t we be investing long WSB?'"," u'https://old.reddit.com/r/wallstreetbets/comments/b8hyzw/if_the_strategy_suggested_here_is_to_invest/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 9, 45, 2, 421818) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
2019-04-02 09:45:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8g6vb/do_strangles_count/'," datetime.datetime(2019, 4, 2, 9, 45, 2, 463182)",'2019-04-02',u'Options',u'1','wallstreetbets',u'Do strangles count?'," u'https://old.reddit.com/r/wallstreetbets/comments/b8g6vb/do_strangles_count/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 9, 45, 2, 463182) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8ce45/reminder_not_everyone_can_be_like_our_lord_and/'," datetime.datetime(2019, 4, 2, 9, 55, 2, 817868)",'2019-04-02',u'Shitpost',u'3','wallstreetbets',u'Reminder: Not everyone can be like our lord and savior Shkreli'," u'https://www.bloomberg.com/news/articles/2019-04-01/21-year-old-yale-alum-accused-of-running-hedge-fund-scam-by-sec'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 9, 55, 2, 817868) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b8fpav/can_you_short_munis/'," datetime.datetime(2019, 4, 2, 10, 0, 3, 758662)",'2019-04-02',None,u'0','investing',u'Can you short munis?'," u'https://old.reddit.com/r/investing/comments/b8fpav/can_you_short_munis/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 10, 0, 3, 758662) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b8dgii/26_yo_pe_guy_ama/'," datetime.datetime(2019, 4, 2, 10, 10, 7, 411674)",'2019-04-02',None,u'0','investing',u'26 y/o PE guy AMA'," u'https://old.reddit.com/r/investing/comments/b8dgii/26_yo_pe_guy_ama/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 10, 10, 7, 411674) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b8dexn/received_this_brainteaser_in_an_interview_today/'," datetime.datetime(2019, 4, 2, 10, 10, 7, 422688)",'2019-04-02',None,u'38','investing',"u""Received this brainteaser in an interview today. No right or wrong answer but curious for everyone's thoughts."""," u'https://old.reddit.com/r/investing/comments/b8dexn/received_this_brainteaser_in_an_interview_today/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 10, 10, 7, 422688) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-04-02 10:10:07 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b8dkih/question_about_tax_efficient_funds_in_fidelity/'," datetime.datetime(2019, 4, 2, 10, 15, 5, 64283)",'2019-04-02',None,u'1','investing',u'Question about tax efficient funds in Fidelity brokerage'," u'https://old.reddit.com/r/investing/comments/b8dkih/question_about_tax_efficient_funds_in_fidelity/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 10, 15, 5, 64283) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-04-02 10:15:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b85iah/any_suggestions_for_any_index_funds_vtiax_or_vti/'," datetime.datetime(2019, 4, 2, 10, 20, 4, 12877)",'2019-04-02',None,u'0','stocks',u'Any suggestions for any index funds VTIAX or VTI that have low expense ratio and do not give any dividends.'," u'https://old.reddit.com/r/stocks/comments/b85iah/any_suggestions_for_any_index_funds_vtiax_or_vti/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 10, 20, 4, 12877) is not JSON serializable
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
2019-04-02 10:20:04 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
2019-04-02 10:20:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b8dnaw/where_do_you_get_the_most_accurate_data_on_a_stock/'," datetime.datetime(2019, 4, 2, 10, 20, 5, 329368)",'2019-04-02',None,u'15','investing',u'Where do you get the most accurate data on a stock?'," u'https://old.reddit.com/r/investing/comments/b8dnaw/where_do_you_get_the_most_accurate_data_on_a_stock/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 10, 20, 5, 329368) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-04-02 10:20:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b81bhg/major_us_airlines_coming_back_online_after/'," datetime.datetime(2019, 4, 2, 10, 30, 6, 425013)",'2019-04-02',None,u'154','StockMarket',u'Major US airlines coming back online after system-wide outages'," u'https://old.reddit.com/r/StockMarket/comments/b81bhg/major_us_airlines_coming_back_online_after/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 10, 30, 6, 425013) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8ce7e/its_finally_starting_to_warm_up_outside/'," datetime.datetime(2019, 4, 2, 10, 40, 2, 796584)",'2019-04-02',u'Meme',u'25','wallstreetbets',"u""It's finally starting to warm up outside"""," u'https://old.reddit.com/r/wallstreetbets/comments/b8ce7e/its_finally_starting_to_warm_up_outside/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 10, 40, 2, 796584) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
2019-04-02 10:40:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
2019-04-02 10:40:03 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b8dw80/historical_average_returns_for_top_market_indexes/'," datetime.datetime(2019, 4, 2, 10, 40, 4, 217635)",'2019-04-02',u'Help',u'1','investing',u'Historical average returns for top market indexes?'," u'https://old.reddit.com/r/investing/comments/b8dw80/historical_average_returns_for_top_market_indexes/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 10, 40, 4, 217635) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b8drvv/how_long_have_you_been_holding_out_for_a_market/'," datetime.datetime(2019, 4, 2, 10, 40, 4, 220281)",'2019-04-02',None,u'72','investing',u'How long have you been holding out for a market crash?'," u'https://old.reddit.com/r/investing/comments/b8drvv/how_long_have_you_been_holding_out_for_a_market/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 10, 40, 4, 220281) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-04-02 10:40:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8cigc/official_lyft_bag_holder_here_ama/'," datetime.datetime(2019, 4, 2, 10, 50, 2, 498010)",'2019-04-02',u'Meme',u'175','wallstreetbets',"u'Official Lyft Bag Holder here, AMA'"," u'https://i.imgur.com/OoGWNru.jpg'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 10, 50, 2, 498010) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b85rhh/thoughts_on_turtle_beach_hear/'," datetime.datetime(2019, 4, 2, 11, 10, 8, 143842)",'2019-04-02',None,u'7','stocks',"u'Thoughts on Turtle Beach, HEAR'"," u'https://old.reddit.com/r/stocks/comments/b85rhh/thoughts_on_turtle_beach_hear/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 11, 10, 8, 143842) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b85n0x/wilc_has_a_current_ratio_of_18/'," datetime.datetime(2019, 4, 2, 11, 10, 8, 149612)",'2019-04-02',u'Discussion',u'0','stocks',u'WILC has a current ratio of 18?'," u'https://old.reddit.com/r/stocks/comments/b85n0x/wilc_has_a_current_ratio_of_18/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 11, 10, 8, 149612) is not JSON serializable
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
2019-04-02 11:10:08 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
2019-04-02 11:10:08 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b8e1ql/yahoo_finance_the_most_successful_people_have/'," datetime.datetime(2019, 4, 2, 11, 15, 3, 819073)",'2019-04-02',u'Discussion',u'30','investing',u'Yahoo Finance: The most successful people have parents who [talk about career and finance with kids]'," u'https://old.reddit.com/r/investing/comments/b8e1ql/yahoo_finance_the_most_successful_people_have/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 11, 15, 3, 819073) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-04-02 11:15:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b8e4rg/eli5_stock_options_and_puts/'," datetime.datetime(2019, 4, 2, 11, 30, 3, 850337)",'2019-04-02',u'Help',u'0','investing',u'ELI5: Stock Options and Puts'," u'https://old.reddit.com/r/investing/comments/b8e4rg/eli5_stock_options_and_puts/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 11, 30, 3, 850337) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-04-02 11:30:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b8ecjv/fedwire_funds_were_downalso_what_happen_if/'," datetime.datetime(2019, 4, 2, 11, 35, 3, 380714)",'2019-04-02',None,u'1','investing',"u'Fedwire Funds were down.Also, what happen if Fedwire funds stay down.'"," u'https://old.reddit.com/r/investing/comments/b8ecjv/fedwire_funds_were_downalso_what_happen_if/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 11, 35, 3, 380714) is not JSON serializable
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Daytrading/new/> from <GET https://old.reddit.com/r/daytrading/new/>
2019-04-02 11:35:03 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Daytrading/new/> from <GET https://old.reddit.com/r/daytrading/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-04-02 11:35:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8cslv/how_can_i_capitalize_on_the_streaming_service_boom/'," datetime.datetime(2019, 4, 2, 11, 40, 1, 999905)",'2019-04-02',u'Discussion',u'7','wallstreetbets',u'How can I capitalize on the Streaming Service Boom?'," u'https://old.reddit.com/r/wallstreetbets/comments/b8cslv/how_can_i_capitalize_on_the_streaming_service_boom/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 11, 40, 1, 999905) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
2019-04-02 11:40:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
2019-04-02 11:40:02 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8d67r/quick_buy_avacodo_calls/'," datetime.datetime(2019, 4, 2, 11, 45, 2, 842858)",'2019-04-02',u'Shitpost',u'130','wallstreetbets',"u'Quick, buy avacodo calls'"," u'https://old.reddit.com/r/wallstreetbets/comments/b8d67r/quick_buy_avacodo_calls/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 11, 45, 2, 842858) is not JSON serializable
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
2019-04-02 11:45:02 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b8ejvg/called_tda/'," datetime.datetime(2019, 4, 2, 11, 45, 4, 317027)",'2019-04-02',None,u'0','investing',u'Called TDA'," u'https://old.reddit.com/r/investing/comments/b8ejvg/called_tda/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 11, 45, 4, 317027) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-04-02 11:45:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8dcd5/think_lyft_selloff_is_bad_wait_till_short_sellers/'," datetime.datetime(2019, 4, 2, 11, 50, 2, 143389)",'2019-04-02',u'Discussion',u'29','wallstreetbets',u'Think Lyft Sell-off Is Bad? Wait Till Short Sellers Enter Fray'," u'https://www.bloomberg.com/news/articles/2019-04-01/think-lyft-sell-off-is-bad-wait-till-short-sellers-enter-fray?srnd=premium'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 11, 50, 2, 143389) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
2019-04-02 11:50:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Daytrading/comments/b6qrbt/best_forex_platform_for_beginner/'," datetime.datetime(2019, 4, 2, 11, 50, 6, 175788)",'2019-04-02',None,u'3','Daytrading',u'Best Forex platform for beginner?'," u'https://old.reddit.com/r/Daytrading/comments/b6qrbt/best_forex_platform_for_beginner/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 11, 50, 6, 175788) is not JSON serializable
INFO:scrapy.core.engine:Closing spider (finished)
2019-04-02 11:50:06 [scrapy.core.engine] INFO: Closing spider (finished)
INFO:scrapy.statscollectors:Dumping Scrapy stats:
"
https://old.reddit.com/r/wallstreetbets/comments/b8duxq/trump_trade/'," datetime.datetime(2019, 4, 2, 12, 0, 3, 249226)",'2019-04-02',u'Discussion',u'13','wallstreetbets',u'Trump Trade'," u'https://old.reddit.com/r/wallstreetbets/comments/b8duxq/trump_trade/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 12, 0, 3, 249226) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8efge/golden_cross_on_that_spx/'," datetime.datetime(2019, 4, 2, 12, 5, 2, 60311)",'2019-04-02',u'Technicals',u'0','wallstreetbets',u'Golden cross on that SPX'," u'https://old.reddit.com/r/wallstreetbets/comments/b8efge/golden_cross_on_that_spx/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 12, 5, 2, 60311) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8e4rd/motely_fool_tsla_450/'," datetime.datetime(2019, 4, 2, 12, 5, 2, 62280)",'2019-04-02',u'Stocks',u'4','wallstreetbets',u'Motely Fool: $TSLA $450'," u'http://www.fool.com/amp/investing/2019/04/01/tesla-stock-headed-to-450.aspx'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 12, 5, 2, 62280) is not JSON serializable
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
2019-04-02 12:05:02 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Daytrading/comments/b6tsv2/missed_out_on_100000/'," datetime.datetime(2019, 4, 2, 12, 5, 5, 904480)",'2019-04-02',None,u'2','Daytrading',"u'Missed out on $100,000'"," u'https://old.reddit.com/r/Daytrading/comments/b6tsv2/missed_out_on_100000/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 12, 5, 5, 904480) is not JSON serializable
INFO:scrapy.core.engine:Closing spider (finished)
2019-04-02 12:05:05 [scrapy.core.engine] INFO: Closing spider (finished)
INFO:scrapy.statscollectors:Dumping Scrapy stats:
"
https://old.reddit.com/r/stocks/comments/b87i04/what_are_the_best_public_avocado_companies/'," datetime.datetime(2019, 4, 2, 12, 10, 2, 444706)",'2019-04-02',u'Question',u'10','stocks',u'What are the best public avocado companies?'," u'https://old.reddit.com/r/stocks/comments/b87i04/what_are_the_best_public_avocado_companies/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 12, 10, 2, 444706) is not JSON serializable
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
2019-04-02 12:10:02 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
2019-04-02 12:10:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Daytrading/comments/b6wm5t/what_time_frame_do_you_trade_with/'," datetime.datetime(2019, 4, 2, 12, 10, 5, 884120)",'2019-04-02',None,u'14','Daytrading',u'What time frame do you trade with?'," u'https://old.reddit.com/r/Daytrading/comments/b6wm5t/what_time_frame_do_you_trade_with/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 12, 10, 5, 884120) is not JSON serializable
INFO:scrapy.core.engine:Closing spider (finished)
2019-04-02 12:10:05 [scrapy.core.engine] INFO: Closing spider (finished)
INFO:scrapy.statscollectors:Dumping Scrapy stats:
"
https://old.reddit.com/r/wallstreetbets/comments/b8eqqp/martin_shkreli_placed_in_solitary_confinement/'," datetime.datetime(2019, 4, 2, 12, 20, 2, 4566)",'2019-04-02',u'Storytime',u'53','wallstreetbets',u'Martin Shkreli Placed in Solitary Confinement After Allegedly Running Company Behind Bars: Report'," u'https://www.thedailybeast.com/martin-shkreli-thrown-in-solitary-confinement-after-running-drug-company-from-prison-cellphone-report'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 12, 20, 2, 4566) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
2019-04-02 12:20:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b87pg7/is_there_an_app_or_website_that_will_give_me_an/'," datetime.datetime(2019, 4, 2, 12, 30, 2, 432544)",'2019-04-02',None,u'1','stocks',u'Is there an app or website that will give me an average of dividend payout compared to my total investments of my portfolio'," u'https://old.reddit.com/r/stocks/comments/b87pg7/is_there_an_app_or_website_that_will_give_me_an/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 12, 30, 2, 432544) is not JSON serializable
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
2019-04-02 12:30:02 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
2019-04-02 12:30:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8f7ln/two_unrelated_events/'," datetime.datetime(2019, 4, 2, 12, 40, 2, 188059)",'2019-04-02',u'Shitpost',u'237','wallstreetbets',u'Two unrelated events.'," u'https://old.reddit.com/r/wallstreetbets/comments/b8f7ln/two_unrelated_events/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 12, 40, 2, 188059) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8gc6g/aapl_cut_price_to_iphones_yesterday_because_of/'," datetime.datetime(2019, 4, 2, 13, 0, 3, 632528)",'2019-04-02',u'Fundamentals',u'19','wallstreetbets',u'$AAPL cut price to iPhones yesterday because of long-planned reduction in VAT in China'," u'https://old.reddit.com/r/wallstreetbets/comments/b8gc6g/aapl_cut_price_to_iphones_yesterday_because_of/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 13, 0, 3, 632528) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8g7th/want_to_take_a_loan_out_to_trade_but_gf_would/'," datetime.datetime(2019, 4, 2, 13, 0, 3, 635770)",'2019-04-02',u'Shitpost',u'12','wallstreetbets',u'Want to take a loan out to trade but GF would probably murder me'," u'https://old.reddit.com/r/wallstreetbets/comments/b8g7th/want_to_take_a_loan_out_to_trade_but_gf_would/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 13, 0, 3, 635770) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b8h3or/tiomarkets_controversial_april_fools_hoax/'," datetime.datetime(2019, 4, 2, 13, 5, 15, 624792)",'2019-04-02',None,u'0','StockMarket',u'TIOmarkets\u2019 Controversial April Fools\u2019 Hoax & Industry Expos\xe9 Goes Viral'," u'https://old.reddit.com/r/StockMarket/comments/b8h3or/tiomarkets_controversial_april_fools_hoax/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 13, 5, 15, 624792) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b8elsp/vending_machines/'," datetime.datetime(2019, 4, 2, 13, 20, 4, 309991)",'2019-04-02',None,u'0','investing',u'Vending machines'," u'https://old.reddit.com/r/investing/comments/b8elsp/vending_machines/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 13, 20, 4, 309991) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-04-02 13:20:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b81r97/sensex_touches_all_time_high_crosses_39000_mark/'," datetime.datetime(2019, 4, 2, 13, 30, 6, 263605)",'2019-04-02',None,u'0','StockMarket',"u'Sensex touches all time high, crosses 39,000 mark.'"," u'https://old.reddit.com/r/StockMarket/comments/b81r97/sensex_touches_all_time_high_crosses_39000_mark/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 13, 30, 6, 263605) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8hfuw/why_otm_option_becomes_itm_does_theta_decay/'," datetime.datetime(2019, 4, 2, 13, 40, 2, 604557)",'2019-04-02',u'Discussion',u'0','wallstreetbets',"u'Why OTM option becomes ITM, does theta decay become positive?'"," u'https://old.reddit.com/r/wallstreetbets/comments/b8hfuw/why_otm_option_becomes_itm_does_theta_decay/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 13, 40, 2, 604557) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b8eqzv/are_those_sites_that_allow_you_to_buy_preipo/'," datetime.datetime(2019, 4, 2, 13, 50, 3, 909734)",'2019-04-02',None,u'1','investing',u'Are those sites that allow you to buy pre-IPO shares legit?'," u'https://old.reddit.com/r/investing/comments/b8eqzv/are_those_sites_that_allow_you_to_buy_preipo/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 13, 50, 3, 909734) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b8enn3/accumulation_and_distribution_indicator/'," datetime.datetime(2019, 4, 2, 13, 50, 3, 912640)",'2019-04-02',u'Discussion',u'0','investing',u'Accumulation and distribution indicator'," u'https://old.reddit.com/r/investing/comments/b8enn3/accumulation_and_distribution_indicator/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 13, 50, 3, 912640) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-04-02 13:50:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8hs2t/daily_discussion_thread_april_02_2019/'," datetime.datetime(2019, 4, 2, 14, 0, 3, 23819)",'2019-04-02',u'Daily Discussion',u'43','wallstreetbets',"u'Daily Discussion Thread - April 02, 2019'"," u'https://old.reddit.com/r/wallstreetbets/comments/b8hs2t/daily_discussion_thread_april_02_2019/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 14, 0, 3, 23819) is not JSON serializable
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
2019-04-02 14:00:03 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
2019-04-02 14:00:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8hw6x/best_q1_2019_sp500_performers/'," datetime.datetime(2019, 4, 2, 14, 5, 3, 394544)",'2019-04-02',u'DD',u'49','wallstreetbets',u'Best Q1 2019 S&P500 Performers'," u'https://i.imgur.com/VFPGzU0.jpg'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 14, 5, 3, 394544) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
2019-04-02 14:05:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
2019-04-02 14:05:03 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8iwwv/look_at_my_second_comma_quick_before_i_make/'," datetime.datetime(2019, 4, 2, 14, 10, 3, 235846)",'2019-04-02',u'Shitpost',u'1540','wallstreetbets',u'Look at my second comma quick before I make mistakes'," u'https://old.reddit.com/r/wallstreetbets/comments/b8iwwv/look_at_my_second_comma_quick_before_i_make/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 14, 10, 3, 235846) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8iigi/today_we_double_down_again_my_friends/'," datetime.datetime(2019, 4, 2, 14, 10, 3, 237524)",'2019-04-02',u'Shitpost',u'106','wallstreetbets',"u'Today we double down again, my friends'"," u'https://twitter.com/realDonaldTrump/status/1113048943938756609'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 14, 10, 3, 237524) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
2019-04-02 14:10:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8j1jm/aal_unusual_335c_option_activity/'," datetime.datetime(2019, 4, 2, 14, 15, 3, 156674)",'2019-04-02',u'Options',u'14','wallstreetbets',u'$AAL Unusual 33.5C Option Activity'," u'https://old.reddit.com/r/wallstreetbets/comments/b8j1jm/aal_unusual_335c_option_activity/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 14, 15, 3, 156674) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b88ntf/when_will_visa_stop/'," datetime.datetime(2019, 4, 2, 14, 15, 3, 332565)",'2019-04-02',u'Question',u'3','stocks',u'When will VISA stop??'," u'https://old.reddit.com/r/stocks/comments/b88ntf/when_will_visa_stop/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 14, 15, 3, 332565) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8j2gz/dont_forget_to_celebrate_yourselves_today/'," datetime.datetime(2019, 4, 2, 14, 20, 3, 877085)",'2019-04-02',u'Meme',u'48','wallstreetbets',"u""Don't forget to celebrate yourselves today!"""," u'https://old.reddit.com/r/wallstreetbets/comments/b8j2gz/dont_forget_to_celebrate_yourselves_today/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 14, 20, 3, 877085) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b88rg0/fundamentals_source/'," datetime.datetime(2019, 4, 2, 14, 40, 2, 899895)",'2019-04-02',u'Advice',u'5','stocks',"u""Fundamental's source"""," u'https://old.reddit.com/r/stocks/comments/b88rg0/fundamentals_source/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 14, 40, 2, 899895) is not JSON serializable
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
2019-04-02 14:40:03 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
2019-04-02 14:40:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b8fc1o/advice_on_how_i_can_get_funded_trading_stocks/'," datetime.datetime(2019, 4, 2, 14, 45, 10, 87673)",'2019-04-02',None,u'0','investing',u'Advice on how I can get funded trading stocks?'," u'https://old.reddit.com/r/investing/comments/b8fc1o/advice_on_how_i_can_get_funded_trading_stocks/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 14, 45, 10, 87673) is not JSON serializable
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Daytrading/new/> from <GET https://old.reddit.com/r/daytrading/new/>
2019-04-02 14:45:10 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Daytrading/new/> from <GET https://old.reddit.com/r/daytrading/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-04-02 14:45:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8j58t/shkreli_smiling_from_his_solitary_confinement/'," datetime.datetime(2019, 4, 2, 14, 50, 2, 967544)",'2019-04-02',u'Shitpost',u'440','wallstreetbets',u'Shkreli smiling from his solitary confinement cell as $SGMO up 45% pre-market'," u'https://old.reddit.com/r/wallstreetbets/comments/b8j58t/shkreli_smiling_from_his_solitary_confinement/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 14, 50, 2, 967544) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
2019-04-02 14:50:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
2019-04-02 14:50:03 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b8fk5e/bitcoin_up_15_in_the_last_2_hours/'," datetime.datetime(2019, 4, 2, 14, 50, 4, 304651)",'2019-04-02',None,u'578','investing',u'Bitcoin up 15% in the last 2 hours'," u'https://old.reddit.com/r/investing/comments/b8fk5e/bitcoin_up_15_in_the_last_2_hours/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 14, 50, 4, 304651) is not JSON serializable
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Daytrading/new/> from <GET https://old.reddit.com/r/daytrading/new/>
2019-04-02 14:50:04 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Daytrading/new/> from <GET https://old.reddit.com/r/daytrading/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-04-02 14:50:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8j7ka/analyst_slaps_42_target_on_lyft_42_below_its_ipo/'," datetime.datetime(2019, 4, 2, 15, 0, 3, 961348)",'2019-04-02',u'Stocks',u'91','wallstreetbets',"u""Analyst slaps $42 target on Lyft \u2014 42% below its IPO price \u2014 says buying it is a 'leap of faith'"""," u'https://www.cnbc.com/2019/04/02/lyft-shares-fall-again-as-analyst-says-buying-the-ipo-here-is-leap-of-faith.html'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 15, 0, 3, 961348) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
2019-04-02 15:00:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b8fydq/singapore_airlines_ground_2_78710_for_trent_1000/'," datetime.datetime(2019, 4, 2, 15, 0, 5, 268794)",'2019-04-02',None,u'28','investing',"u'Singapore Airlines Ground 2, 787-10 for ""Trent 1000 TEN"" engine defects'"," u'https://old.reddit.com/r/investing/comments/b8fydq/singapore_airlines_ground_2_78710_for_trent_1000/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 15, 0, 5, 268794) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-04-02 15:00:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b8fzn3/with_a_possible_recssion_looming_how_many_of_you/'," datetime.datetime(2019, 4, 2, 15, 25, 5, 557215)",'2019-04-02',None,u'0','investing',"u'With a possible recssion looming, how many of you are long on Gold?'"," u'https://old.reddit.com/r/investing/comments/b8fzn3/with_a_possible_recssion_looming_how_many_of_you/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 15, 25, 5, 557215) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-04-02 15:25:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8jfdx/if_yall_could_just_let_snap_die_already_that/'," datetime.datetime(2019, 4, 2, 15, 30, 3, 692106)",'2019-04-02',u'YOLO',u'24','wallstreetbets',"u""If ya'll could just let $SNAP die already that would be greeeeeeeat"""," u'https://old.reddit.com/r/wallstreetbets/comments/b8jfdx/if_yall_could_just_let_snap_die_already_that/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 15, 30, 3, 692106) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8jkgb/ba_jetstar_787_suffers_problems_with_both_engines/'," datetime.datetime(2019, 4, 2, 15, 35, 2, 723332)",'2019-04-02',u'Discussion',u'0','wallstreetbets',u'$BA: Jetstar 787 suffers problems with both engines on approach to Osaka'," u'https://old.reddit.com/r/wallstreetbets/comments/b8jkgb/ba_jetstar_787_suffers_problems_with_both_engines/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 15, 35, 2, 723332) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8jkg3/lyft_you_math/'," datetime.datetime(2019, 4, 2, 15, 35, 2, 725237)",'2019-04-02',u'Shitpost',u'261','wallstreetbets',u'$LYFT... you math?'," u'https://imgur.com/QAGHFZP'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 15, 35, 2, 725237) is not JSON serializable
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
2019-04-02 15:35:02 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/thewallstreet/comments/b63db7/daily_discussion_march_27/'," datetime.datetime(2019, 4, 2, 16, 0, 5, 857950)",'2019-04-02',u'Daily',u'17','thewallstreet',u'Daily Discussion - (March 27)'," u'https://old.reddit.com/r/thewallstreet/comments/b63db7/daily_discussion_march_27/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 16, 0, 5, 857950) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/Trading/new/> (referer: None)
2019-04-02 16:00:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/Trading/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b8gzs5/free_access_to_morningstar_library_doesnt_work/'," datetime.datetime(2019, 4, 2, 16, 10, 4, 746091)",'2019-04-02',None,u'29','investing',"u""Free access to Morningstar (library) doesn't work anymore? Anybody can help?"""," u'https://old.reddit.com/r/investing/comments/b8gzs5/free_access_to_morningstar_library_doesnt_work/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 16, 10, 4, 746091) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b8gged/german_equivalent_for_us_treasury_bill_yield/'," datetime.datetime(2019, 4, 2, 16, 10, 4, 749986)",'2019-04-02',u'Help',u'1','investing',u'German equivalent for U.S. Treasury Bill yield?'," u'https://old.reddit.com/r/investing/comments/b8gged/german_equivalent_for_us_treasury_bill_yield/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 16, 10, 4, 749986) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-04-02 16:10:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8k2se/69_lyft_69/'," datetime.datetime(2019, 4, 2, 16, 15, 3, 437140)",'2019-04-02',u'Meme',u'312','wallstreetbets',u'69 Lyft 69'," u'https://old.reddit.com/r/wallstreetbets/comments/b8k2se/69_lyft_69/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 16, 15, 3, 437140) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
2019-04-02 16:15:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
2019-04-02 16:15:03 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8kil0/we_are_going_to_lose_cramer_indicator/'," datetime.datetime(2019, 4, 2, 16, 20, 3, 100326)",'2019-04-02',u'Discussion',u'110','wallstreetbets',u'We are going to lose Cramer indicator...'," u'https://old.reddit.com/r/wallstreetbets/comments/b8kil0/we_are_going_to_lose_cramer_indicator/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 16, 20, 3, 100326) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8ke88/i_know_we_are_not_super_into_dd_and_are_a_yolo/'," datetime.datetime(2019, 4, 2, 16, 20, 3, 103505)",'2019-04-02',u'Discussion',u'4','wallstreetbets',"u'I know we are not super into DD and are a YOLO crowd, but if you want reliable Chinese economic data look at the Li KeQiang index and I will preemptively GFMS.'"," u'https://en.m.wikipedia.org/wiki/Li_Keqiang_index'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 16, 20, 3, 103505) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
2019-04-02 16:20:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
2019-04-02 16:20:03 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8kw6c/when_the_25_call_just_makes_sense/'," datetime.datetime(2019, 4, 2, 16, 25, 3, 735094)",'2019-04-02',u'Meme',u'2','wallstreetbets',u'When the $2.5 call just makes sense.'," u'https://old.reddit.com/r/wallstreetbets/comments/b8kw6c/when_the_25_call_just_makes_sense/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 16, 25, 3, 735094) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8kup6/time_to_buy_disney_ahead_of_endgame/'," datetime.datetime(2019, 4, 2, 16, 25, 3, 738626)",'2019-04-02',u'Discussion',u'11','wallstreetbets',u'Time to buy Disney ahead of Endgame?'," u'https://old.reddit.com/r/wallstreetbets/comments/b8kup6/time_to_buy_disney_ahead_of_endgame/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 16, 25, 3, 738626) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b8i69p/buying_zoom_ipo_uk/'," datetime.datetime(2019, 4, 2, 16, 35, 3, 973940)",'2019-04-02',None,u'0','investing',u'Buying Zoom IPO (UK)'," u'https://old.reddit.com/r/investing/comments/b8i69p/buying_zoom_ipo_uk/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 16, 35, 3, 973940) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-04-02 16:35:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Daytrading/comments/b6y735/do_candlestick_patterns_work_on_low_time_frames/'," datetime.datetime(2019, 4, 2, 16, 40, 9, 973448)",'2019-04-02',None,u'1','Daytrading',u'Do candlestick patterns work on low time frames? (some of my observations)'," u'https://old.reddit.com/r/Daytrading/comments/b6y735/do_candlestick_patterns_work_on_low_time_frames/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 16, 40, 9, 973448) is not JSON serializable
INFO:scrapy.core.engine:Closing spider (finished)
2019-04-02 16:40:09 [scrapy.core.engine] INFO: Closing spider (finished)
INFO:scrapy.statscollectors:Dumping Scrapy stats:
"
https://old.reddit.com/r/stocks/comments/b88sdt/facebook_fb_earnings_425_guesstimate_thread/'," datetime.datetime(2019, 4, 2, 16, 50, 3, 871555)",'2019-04-02',None,u'2','stocks',u'Facebook (FB) earnings 4/25 guesstimate thread'," u'https://old.reddit.com/r/stocks/comments/b88sdt/facebook_fb_earnings_425_guesstimate_thread/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 16, 50, 3, 871555) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b826uv/mvis/'," datetime.datetime(2019, 4, 2, 16, 55, 6, 357697)",'2019-04-02',None,u'1','StockMarket',u'$MVIS'," u'https://old.reddit.com/r/StockMarket/comments/b826uv/mvis/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 16, 55, 6, 357697) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b8ie5q/is_now_a_good_time_to_get_into_bond_funds/'," datetime.datetime(2019, 4, 2, 17, 5, 5, 607636)",'2019-04-02',None,u'0','investing',u'Is now a good time to get into bond funds?'," u'https://old.reddit.com/r/investing/comments/b8ie5q/is_now_a_good_time_to_get_into_bond_funds/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 17, 5, 5, 607636) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-04-02 17:05:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b8ihx3/any_reason_to_diversify_by_funds_company_with/'," datetime.datetime(2019, 4, 2, 17, 10, 4, 522156)",'2019-04-02',None,u'3','investing',u'Any reason to diversify by funds company? (with index being the same)'," u'https://old.reddit.com/r/investing/comments/b8ihx3/any_reason_to_diversify_by_funds_company_with/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 17, 10, 4, 522156) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8l52r/cant_go_tits_up/'," datetime.datetime(2019, 4, 2, 17, 15, 6, 534382)",'2019-04-02',u'Shitpost',u'742','wallstreetbets',u'Can\u2019t go tits up'," u'https://old.reddit.com/r/wallstreetbets/comments/b8l52r/cant_go_tits_up/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 17, 15, 6, 534382) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8ltr5/cheers_guys/'," datetime.datetime(2019, 4, 2, 17, 20, 3, 223526)",'2019-04-02',u'Shitpost',u'237','wallstreetbets',u'Cheers guys! \U0001f37b'," u'https://old.reddit.com/r/wallstreetbets/comments/b8ltr5/cheers_guys/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 17, 20, 3, 223526) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b8iqmv/increasing_alpha/'," datetime.datetime(2019, 4, 2, 17, 35, 5, 307286)",'2019-04-02',None,u'0','investing',u'Increasing alpha'," u'https://old.reddit.com/r/investing/comments/b8iqmv/increasing_alpha/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 17, 35, 5, 307286) is not JSON serializable
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Daytrading/new/> from <GET https://old.reddit.com/r/daytrading/new/>
2019-04-02 17:35:05 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Daytrading/new/> from <GET https://old.reddit.com/r/daytrading/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-04-02 17:35:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Stock_Picks/comments/awkur7/gap_jcpenney_victorias_secret_foot_locker_465/'," datetime.datetime(2019, 4, 2, 17, 35, 7, 375620)",'2019-04-02',u'News',u'13','Stock_Picks',"u""Gap, JCPenney, Victoria's Secret, Foot Locker: 465 stores closures in 48 hours"""," u'https://www.foxbusiness.com/retail/gap-jcpenney-victorias-secret-foot-locker-465-stores-closures-in-48-hours'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 17, 35, 7, 375620) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8lvkz/finally_decided_to_post_this_pulled_40k_out/'," datetime.datetime(2019, 4, 2, 17, 45, 4, 1682)",'2019-04-02',u'Gain',u'102','wallstreetbets',u'Finally decided to post this. Pulled 40k out'," u'https://old.reddit.com/r/wallstreetbets/comments/b8lvkz/finally_decided_to_post_this_pulled_40k_out/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 17, 45, 4, 1682) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8lup3/walgreens_helping_my_account/'," datetime.datetime(2019, 4, 2, 17, 45, 4, 4519)",'2019-04-02',u'Gain',u'22','wallstreetbets',u'Walgreens Helping my Account'," u'https://imgur.com/a/Ie0wzJD'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 17, 45, 4, 4519) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
2019-04-02 17:45:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8lzv8/the_stupid_idiots_guide_to_the_future_of_uber_and/'," datetime.datetime(2019, 4, 2, 17, 55, 4, 804820)",'2019-04-02',u'Stocks',u'5','wallstreetbets',"u""The Stupid Idiot's Guide to the Future of Uber and Lyft"""," u'https://splinternews.com/the-stupid-idiots-guide-to-the-future-of-uber-and-lyft-1833741006'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 17, 55, 4, 804820) is not JSON serializable
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
2019-04-02 17:55:04 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/thewallstreet/comments/b68yn5/post_market_discussion_march_27/'," datetime.datetime(2019, 4, 2, 17, 55, 7, 390862)",'2019-04-02',u'Daily',u'7','thewallstreet',u'Post Market Discussion - (March 27)'," u'https://old.reddit.com/r/thewallstreet/comments/b68yn5/post_market_discussion_march_27/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 17, 55, 7, 390862) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8o06e/cramer_my_sources_at_amazon_indicate_to_me_theyll/'," datetime.datetime(2019, 4, 2, 18, 0, 2, 591656)",'2019-04-02',u'Discussion',u'8','wallstreetbets',"u""Cramer: My sources at Amazon indicate to me they'll do whatever it takes to undercut Walmart"""," u'https://www.cnbc.com/2019/04/02/cramer-amazon-will-do-whatever-it-takes-to-undercut-walmart.html'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 18, 0, 2, 591656) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8nx5e/here_we_go_again/'," datetime.datetime(2019, 4, 2, 18, 0, 2, 596780)",'2019-04-02',u'Shitpost',u'80','wallstreetbets',u'Here we go again'," u'https://old.reddit.com/r/wallstreetbets/comments/b8nx5e/here_we_go_again/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 18, 0, 2, 596780) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8nw6l/my_first_big_boy_yolo_on_options/'," datetime.datetime(2019, 4, 2, 18, 0, 2, 598664)",'2019-04-02',u'Loss',u'2','wallstreetbets',u'My first big boy yolo on options'," u'https://gyazo.com/fac3e62fd043eac39579d544ba3e92c4'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 18, 0, 2, 598664) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8nkdo/hunting_season/'," datetime.datetime(2019, 4, 2, 18, 0, 2, 600398)",'2019-04-02',u'Shitpost',u'7','wallstreetbets',u'Hunting season'," u'https://old.reddit.com/r/wallstreetbets/comments/b8nkdo/hunting_season/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 18, 0, 2, 600398) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8n5z7/blackberry_stock_is_bound_to_head_much_higher_in/'," datetime.datetime(2019, 4, 2, 18, 0, 2, 602078)",'2019-04-02',u'Stocks',u'4','wallstreetbets',u'BlackBerry Stock Is Bound to Head Much Higher in the Long Run'," u'https://investorplace.com/2019/04/blackberry-stock-is-bound-to-head-much-higher-in-the-long-run/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 18, 0, 2, 602078) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8n3ra/mother_drone_ship_long_amzn/'," datetime.datetime(2019, 4, 2, 18, 0, 2, 603791)",'2019-04-02',u'Stocks',u'210','wallstreetbets',"u'Mother Drone Ship, Long $AMZN'"," u'https://old.reddit.com/r/wallstreetbets/comments/b8n3ra/mother_drone_ship_long_amzn/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 18, 0, 2, 603791) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8mw6t/he_just_lost_70_million_to_get_his_hands_on_a/'," datetime.datetime(2019, 4, 2, 18, 0, 2, 605607)",'2019-04-02',u'Shitpost',u'21','wallstreetbets',"u'He just lost $70 million to get his hands on a half-baked gambling app, I hereby nominate Tom Dundon as head mod of WSB'"," u'https://i.imgur.com/ddZvH40.jpg'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 18, 0, 2, 605607) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8mqvd/enough_dd_for_yolo_expecting_a_massive_subscriber/'," datetime.datetime(2019, 4, 2, 18, 0, 2, 607580)",'2019-04-02',u'DD',u'12','wallstreetbets',u'Enough DD for YOLO. Expecting a massive subscriber beat for Netflix. Want to see if there is a consenting group to play games with this'," u'https://old.reddit.com/r/wallstreetbets/comments/b8mqvd/enough_dd_for_yolo_expecting_a_massive_subscriber/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 18, 0, 2, 607580) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8mefb/my_boys_over_at_wharton_have_this_offshore_cayman/'," datetime.datetime(2019, 4, 2, 18, 0, 2, 610410)",'2019-04-02',u'Satire',u'39','wallstreetbets',u'My boys over at Wharton have this offshore cayman account for us! We will deliver more than this shitty endowment.... it literally cannot go tits up!'," u'https://old.reddit.com/r/wallstreetbets/comments/b8mefb/my_boys_over_at_wharton_have_this_offshore_cayman/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 18, 0, 2, 610410) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8md5b/i_wrote_a_python_script_for_portfolio_optimization/'," datetime.datetime(2019, 4, 2, 18, 0, 2, 612663)",'2019-04-02',u'Stocks',u'46','wallstreetbets',u'I wrote a Python script for Portfolio Optimization'," u'https://old.reddit.com/r/wallstreetbets/comments/b8md5b/i_wrote_a_python_script_for_portfolio_optimization/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 18, 0, 2, 612663) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8m3qp/50_cent_is_one_of_us/'," datetime.datetime(2019, 4, 2, 18, 0, 2, 614322)",'2019-04-02',u'Shitpost',u'67','wallstreetbets',u'50 Cent is one of us!'," u'https://old.reddit.com/r/wallstreetbets/comments/b8m3qp/50_cent_is_one_of_us/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 18, 0, 2, 614322) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
2019-04-02 18:00:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
2019-04-02 18:00:02 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b8ib9h/walgreens_shares_slide_as_drugstore_chain_misses/'," datetime.datetime(2019, 4, 2, 18, 0, 2, 821435)",'2019-04-02',None,u'21','stocks',"u'Walgreens shares slide as drugstore chain misses earnings estimates, lowers 2019 forecast'"," u'https://old.reddit.com/r/stocks/comments/b8ib9h/walgreens_shares_slide_as_drugstore_chain_misses/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 18, 0, 2, 821435) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b8hes9/does_how_you_personally_feel_about_a_company/'," datetime.datetime(2019, 4, 2, 18, 0, 2, 823999)",'2019-04-02',None,u'0','stocks',"u""Does how you personally feel about a company affect whether or not you'll invest in it even though you know it's fundamentally a good company to invest in."""," u'https://old.reddit.com/r/stocks/comments/b8hes9/does_how_you_personally_feel_about_a_company/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 18, 0, 2, 823999) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b8gv9f/nsf_takeover_provident/'," datetime.datetime(2019, 4, 2, 18, 0, 2, 827552)",'2019-04-02',None,u'1','stocks',u'NSF takeover Provident'," u'https://old.reddit.com/r/stocks/comments/b8gv9f/nsf_takeover_provident/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 18, 0, 2, 827552) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b8e3ld/help_solve_this_mystery/'," datetime.datetime(2019, 4, 2, 18, 0, 2, 830501)",'2019-04-02',None,u'1','stocks',u'Help solve this mystery'," u'https://old.reddit.com/r/stocks/comments/b8e3ld/help_solve_this_mystery/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 18, 0, 2, 830501) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b8dqb9/marketwatch_game/'," datetime.datetime(2019, 4, 2, 18, 0, 2, 832909)",'2019-04-02',None,u'1','stocks',u'MarketWatch Game'," u'https://old.reddit.com/r/stocks/comments/b8dqb9/marketwatch_game/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 18, 0, 2, 832909) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b8dhp8/blackberry_thoughts/'," datetime.datetime(2019, 4, 2, 18, 0, 2, 835881)",'2019-04-02',None,u'2','stocks',u'Blackberry thoughts ??'," u'https://old.reddit.com/r/stocks/comments/b8dhp8/blackberry_thoughts/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 18, 0, 2, 835881) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b8cz7o/at_what_price_would_you_consider_horizon_pharma_a/'," datetime.datetime(2019, 4, 2, 18, 0, 2, 839031)",'2019-04-02',None,u'1','stocks',u'At what price would you consider horizon pharma a buy?'," u'https://old.reddit.com/r/stocks/comments/b8cz7o/at_what_price_would_you_consider_horizon_pharma_a/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 18, 0, 2, 839031) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b8ce4y/can_anyone_who_is_proficient_in_stocksfilings_let/'," datetime.datetime(2019, 4, 2, 18, 0, 2, 841320)",'2019-04-02',u'Advice Request',u'2','stocks',u'Can anyone who is proficient in stocks/filings let me know what this means?'," u'https://old.reddit.com/r/stocks/comments/b8ce4y/can_anyone_who_is_proficient_in_stocksfilings_let/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 18, 0, 2, 841320) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b8bmdj/inpixon/'," datetime.datetime(2019, 4, 2, 18, 0, 2, 843527)",'2019-04-02',None,u'1','stocks',u'Inpixon'," u'https://old.reddit.com/r/stocks/comments/b8bmdj/inpixon/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 18, 0, 2, 843527) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b8be2k/what_price_do_you_predict_lyft_to_bottom_out_at/'," datetime.datetime(2019, 4, 2, 18, 0, 2, 846307)",'2019-04-02',None,u'1','stocks',u'What price do you predict LYFT to bottom out at?'," u'https://old.reddit.com/r/stocks/comments/b8be2k/what_price_do_you_predict_lyft_to_bottom_out_at/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 18, 0, 2, 846307) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b89wch/is_it_time_to_invest_in_boeing_ba/'," datetime.datetime(2019, 4, 2, 18, 0, 2, 848499)",'2019-04-02',u'Ticker Question',u'1','stocks',u'Is it time to invest in Boeing? ($BA)'," u'https://old.reddit.com/r/stocks/comments/b89wch/is_it_time_to_invest_in_boeing_ba/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 18, 0, 2, 848499) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b89t1b/whole_foods_will_slash_prices_on_hundreds_of/'," datetime.datetime(2019, 4, 2, 18, 0, 2, 850736)",'2019-04-02',None,u'486','stocks',u'Whole Foods will slash prices on hundreds of items starting Wednesday'," u'https://old.reddit.com/r/stocks/comments/b89t1b/whole_foods_will_slash_prices_on_hundreds_of/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 18, 0, 2, 850736) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b89r3f/what_would_you_rather_have/'," datetime.datetime(2019, 4, 2, 18, 0, 2, 852393)",'2019-04-02',None,u'2','stocks',u'What would you rather have?'," u'https://old.reddit.com/r/stocks/comments/b89r3f/what_would_you_rather_have/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 18, 0, 2, 852393) is not JSON serializable
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
2019-04-02 18:00:03 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
2019-04-02 18:00:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b8noi0/when_do_analysts_update_their_price_targets/'," datetime.datetime(2019, 4, 2, 18, 0, 3, 969482)",'2019-04-02',None,u'0','investing',u'When do analysts update their price targets?'," u'https://old.reddit.com/r/investing/comments/b8noi0/when_do_analysts_update_their_price_targets/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 18, 0, 3, 969482) is not JSON serializable
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Daytrading/new/> from <GET https://old.reddit.com/r/daytrading/new/>
2019-04-02 18:00:03 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Daytrading/new/> from <GET https://old.reddit.com/r/daytrading/new/>
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b8ndo0/taxefficient_perhaps_swapbased_bond_etf/'," datetime.datetime(2019, 4, 2, 18, 0, 3, 974899)",'2019-04-02',None,u'2','investing',"u'Tax-efficient, perhaps swap-based Bond ETF?'"," u'https://old.reddit.com/r/investing/comments/b8ndo0/taxefficient_perhaps_swapbased_bond_etf/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 18, 0, 3, 974899) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b8n92u/why_does_cramer_says_investing_in_bank_stocks_is/'," datetime.datetime(2019, 4, 2, 18, 0, 3, 978630)",'2019-04-02',None,u'2','investing',u'Why does Cramer says investing in bank stocks is not a good idea?'," u'https://old.reddit.com/r/investing/comments/b8n92u/why_does_cramer_says_investing_in_bank_stocks_is/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 18, 0, 3, 978630) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b8n8ji/do_limited_partnerships_not_have_retained/'," datetime.datetime(2019, 4, 2, 18, 0, 3, 981995)",'2019-04-02',None,u'4','investing',u'Do limited partnerships not have retained earnings on their balance sheets or are there no retained earnings for a limited partnership? Is there an equivalent for them?'," u'https://old.reddit.com/r/investing/comments/b8n8ji/do_limited_partnerships_not_have_retained/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 18, 0, 3, 981995) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b8mig5/what_is_a_realistic_market_cap_for_revenue/'," datetime.datetime(2019, 4, 2, 18, 0, 3, 986610)",'2019-04-02',u'Discussion',u'0','investing',u'What is a realistic market cap for revenue?'," u'https://old.reddit.com/r/investing/comments/b8mig5/what_is_a_realistic_market_cap_for_revenue/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 18, 0, 3, 986610) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b8mhqm/how_do_you_find_a_tech_startup_to_invest_in/'," datetime.datetime(2019, 4, 2, 18, 0, 3, 989735)",'2019-04-02',None,u'0','investing',u'How do you find a tech startup to invest in?'," u'https://old.reddit.com/r/investing/comments/b8mhqm/how_do_you_find_a_tech_startup_to_invest_in/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 18, 0, 3, 989735) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b8m400/snap_android_update_release_gap_fill_back_to_5_50/'," datetime.datetime(2019, 4, 2, 18, 0, 3, 992710)",'2019-04-02',u'Discussion',u'0','investing',u'$SNAP android update release = gap fill back to $5 (50%+ downside risk)'," u'https://old.reddit.com/r/investing/comments/b8m400/snap_android_update_release_gap_fill_back_to_5_50/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 18, 0, 3, 992710) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b8lkc1/single_family_to_2_family_conversion/'," datetime.datetime(2019, 4, 2, 18, 0, 3, 996004)",'2019-04-02',None,u'0','investing',u'single family to 2 family conversion'," u'https://old.reddit.com/r/investing/comments/b8lkc1/single_family_to_2_family_conversion/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 18, 0, 3, 996004) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b8kxb5/whats_up_and_down_with_vanadium/'," datetime.datetime(2019, 4, 2, 18, 0, 3, 999323)",'2019-04-02',None,u'1','investing',u'What\u2019s up and down with Vanadium?'," u'https://old.reddit.com/r/investing/comments/b8kxb5/whats_up_and_down_with_vanadium/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 18, 0, 3, 999323) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b8kpw4/tax_question_on_3000_cap/'," datetime.datetime(2019, 4, 2, 18, 0, 4, 2778)",'2019-04-02',u'Help',u'0','investing',"u'Tax Question on $3,000 cap!'"," u'https://old.reddit.com/r/investing/comments/b8kpw4/tax_question_on_3000_cap/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 18, 0, 4, 2778) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b8kjqq/how_do_you_account_for_the_problem_of/'," datetime.datetime(2019, 4, 2, 18, 0, 4, 5461)",'2019-04-02',None,u'1','investing',"u""How do you account for the problem of 'subjectivity' in Fundamental Analysis?"""," u'https://old.reddit.com/r/investing/comments/b8kjqq/how_do_you_account_for_the_problem_of/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 18, 0, 4, 5461) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b8k3ex/how_long_before_i_can_transfer_funds_out_of_etrade/'," datetime.datetime(2019, 4, 2, 18, 0, 4, 7087)",'2019-04-02',u'Help',u'0','investing',u'How long before I can transfer funds out of etrade?'," u'https://old.reddit.com/r/investing/comments/b8k3ex/how_long_before_i_can_transfer_funds_out_of_etrade/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 18, 0, 4, 7087) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b8k2zj/what_is_the_best_method_to_hold_real_estate/'," datetime.datetime(2019, 4, 2, 18, 0, 4, 9013)",'2019-04-02',u'Help',u'0','investing',u'What is the best method to hold Real Estate Rental Properties?'," u'https://old.reddit.com/r/investing/comments/b8k2zj/what_is_the_best_method_to_hold_real_estate/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 18, 0, 4, 9013) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b8juka/best_stocks_to_buy/'," datetime.datetime(2019, 4, 2, 18, 0, 4, 11964)",'2019-04-02',None,u'0','investing',u'Best stocks to buy?'," u'https://old.reddit.com/r/investing/comments/b8juka/best_stocks_to_buy/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 18, 0, 4, 11964) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b8jskf/qualcomm/'," datetime.datetime(2019, 4, 2, 18, 0, 4, 13939)",'2019-04-02',u'Discussion',u'0','investing',u'Qualcomm'," u'https://old.reddit.com/r/investing/comments/b8jskf/qualcomm/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 18, 0, 4, 13939) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b8jr80/nio_tops_expectations_for_first_quarter_deliveries/'," datetime.datetime(2019, 4, 2, 18, 0, 4, 15800)",'2019-04-02',u'News',u'0','investing',u'$NIO tops expectations for first quarter deliveries.'," u'https://old.reddit.com/r/investing/comments/b8jr80/nio_tops_expectations_for_first_quarter_deliveries/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 18, 0, 4, 15800) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b8jqu5/eli5_how_is_investing_in_a_stock_different_than/'," datetime.datetime(2019, 4, 2, 18, 0, 4, 17406)",'2019-04-02',u'Education',u'0','investing',u'ELI5: how is investing in a stock different than buying it and hoping someone else buys it later at a higher price?'," u'https://old.reddit.com/r/investing/comments/b8jqu5/eli5_how_is_investing_in_a_stock_different_than/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 18, 0, 4, 17406) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b8j1ti/whats_undervalued_at_the_moment_crashed_from/'," datetime.datetime(2019, 4, 2, 18, 0, 4, 18977)",'2019-04-02',None,u'1','investing',u'What\u2019s undervalued at the moment (crashed from brexit/ trade war)?'," u'https://old.reddit.com/r/investing/comments/b8j1ti/whats_undervalued_at_the_moment_crashed_from/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 18, 0, 4, 18977) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-04-02 18:00:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/thewallstreet/comments/b6epnd/daily_spx_tpos_03272019/'," datetime.datetime(2019, 4, 2, 18, 0, 5, 66874)",'2019-04-02',None,u'12','thewallstreet',"u""Daily SPX TPO's 03-27-2019"""," u'https://i.imgur.com/P3qPhcF.png'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 18, 0, 5, 66874) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/thewallstreet/comments/b6d846/deviations_for_thursday_march_28_2019/'," datetime.datetime(2019, 4, 2, 18, 0, 5, 68607)",'2019-04-02',None,u'13','thewallstreet',"u'Deviations for Thursday, March 28, 2019'"," u'https://i.imgur.com/G17d9nJ.png'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 18, 0, 5, 68607) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/thewallstreet/comments/b6ads7/nightly_trading_discussion_march_2728/'," datetime.datetime(2019, 4, 2, 18, 0, 5, 70295)",'2019-04-02',u'Daily',u'11','thewallstreet',u'Nightly Trading Discussion - (March 27/28)'," u'https://old.reddit.com/r/thewallstreet/comments/b6ads7/nightly_trading_discussion_march_2728/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 18, 0, 5, 70295) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/Trading/new/> (referer: None)
2019-04-02 18:00:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/Trading/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b87am9/lyft_is_a_sinking_ship/'," datetime.datetime(2019, 4, 2, 18, 0, 6, 76208)",'2019-04-02',None,u'6','StockMarket',u'Lyft is a sinking ship.'," u'https://old.reddit.com/r/StockMarket/comments/b87am9/lyft_is_a_sinking_ship/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 18, 0, 6, 76208) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b876u9/your_favorite_stocks_for_q2_2019_and_why/'," datetime.datetime(2019, 4, 2, 18, 0, 6, 78503)",'2019-04-02',None,u'7','StockMarket',u'Your favorite stocks for Q2 2019 and why?'," u'https://old.reddit.com/r/StockMarket/comments/b876u9/your_favorite_stocks_for_q2_2019_and_why/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 18, 0, 6, 78503) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b84fsz/short_gamestop_on_tuesday/'," datetime.datetime(2019, 4, 2, 18, 0, 6, 80391)",'2019-04-02',None,u'21','StockMarket',u'Short GameStop on Tuesday?'," u'https://old.reddit.com/r/StockMarket/comments/b84fsz/short_gamestop_on_tuesday/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 18, 0, 6, 80391) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b83h36/pulmatrix_inc_nasdaq_pulm_enters_into_binding/'," datetime.datetime(2019, 4, 2, 18, 0, 6, 82240)",'2019-04-02',None,u'0','StockMarket',"u'Pulmatrix, Inc. (NASDAQ: PULM) enters into binding term sheet with Cipla Technologies, LLC'"," u'https://old.reddit.com/r/StockMarket/comments/b83h36/pulmatrix_inc_nasdaq_pulm_enters_into_binding/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 18, 0, 6, 82240) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b82saj/rstockmarket_april_2019_stock_picking_contest_is/'," datetime.datetime(2019, 4, 2, 18, 0, 6, 83958)",'2019-04-02',u'Contest',u'2','StockMarket',u'r/StockMarket April 2019 Stock Picking Contest is now LIVE!'," u'https://docs.google.com/spreadsheets/d/1nl-AaV0ymsKLLCbgAhOvbpmdWnE1WhCIXZbFrWIB25A/edit?usp=sharing'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 18, 0, 6, 83958) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b829p1/morning_news_20190401/'," datetime.datetime(2019, 4, 2, 18, 0, 6, 85508)",'2019-04-02',None,u'9','StockMarket',u'Morning News (2019-04-01)'," u'https://old.reddit.com/r/StockMarket/comments/b829p1/morning_news_20190401/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 18, 0, 6, 85508) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b827o9/bemg_news_beta_music_group_inc_otcbemg_completes/'," datetime.datetime(2019, 4, 2, 18, 0, 6, 87785)",'2019-04-02',None,u'2','StockMarket',"u'$BEMG News ""Beta Music Group Inc (OTC:BEMG) Completes Share Reduction Of 75% Of Its Common Shares; Lyft Deal Complete.""'"," u'https://old.reddit.com/r/StockMarket/comments/b827o9/bemg_news_beta_music_group_inc_otcbemg_completes/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 18, 0, 6, 87785) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/Daytrading/new/> (referer: None)
2019-04-02 18:00:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/Daytrading/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Daytrading/comments/b6yia6/what_are_the_indications_of_a_choppy_market/'," datetime.datetime(2019, 4, 2, 18, 0, 6, 355417)",'2019-04-02',None,u'2','Daytrading',u'What are the Indications of a Choppy Market?'," u'https://old.reddit.com/r/Daytrading/comments/b6yia6/what_are_the_indications_of_a_choppy_market/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 2, 18, 0, 6, 355417) is not JSON serializable
INFO:scrapy.core.engine:Closing spider (finished)
2019-04-02 18:00:06 [scrapy.core.engine] INFO: Closing spider (finished)
INFO:scrapy.statscollectors:Dumping Scrapy stats:
"
https://old.reddit.com/r/stocks/comments/b8ic4c/lyft_shares_fall_again_as_analyst_says_buying_the/'," datetime.datetime(2019, 4, 3, 9, 30, 3, 80177)",'2019-04-03',None,u'171','stocks',u'Lyft shares fall again as analyst says buying the IPO here is \u2018leap of faith\u2019'," u'https://old.reddit.com/r/stocks/comments/b8ic4c/lyft_shares_fall_again_as_analyst_says_buying_the/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 9, 30, 3, 80177) is not JSON serializable
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
2019-04-03 09:30:03 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
2019-04-03 09:30:03 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
2019-04-03 09:30:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b8srzc/how_do_i_profit_from_canadian_home_prices/'," datetime.datetime(2019, 4, 3, 9, 30, 4, 455491)",'2019-04-03',None,u'11','investing',u'How do I profit from Canadian home prices plummeting in the near future?'," u'https://old.reddit.com/r/investing/comments/b8srzc/how_do_i_profit_from_canadian_home_prices/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 9, 30, 4, 455491) is not JSON serializable
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Daytrading/new/> from <GET https://old.reddit.com/r/daytrading/new/>
2019-04-03 09:30:04 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Daytrading/new/> from <GET https://old.reddit.com/r/daytrading/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-04-03 09:30:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b8sxto/what_stockbond_allocation_to_sleep_better/'," datetime.datetime(2019, 4, 3, 9, 40, 3, 815885)",'2019-04-03',None,u'4','investing',u'What stock/bond allocation to sleep better?'," u'https://old.reddit.com/r/investing/comments/b8sxto/what_stockbond_allocation_to_sleep_better/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 9, 40, 3, 815885) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-04-03 09:40:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b8t5kr/utg/'," datetime.datetime(2019, 4, 3, 9, 45, 4, 225786)",'2019-04-03',None,u'1','investing',u'Utg'," u'https://old.reddit.com/r/investing/comments/b8t5kr/utg/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 9, 45, 4, 225786) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b8t0dm/gamestop_lowest_since_march_2005/'," datetime.datetime(2019, 4, 3, 9, 45, 4, 227663)",'2019-04-03',None,u'0','investing',u'GameStop: Lowest since March 2005'," u'https://old.reddit.com/r/investing/comments/b8t0dm/gamestop_lowest_since_march_2005/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 9, 45, 4, 227663) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-04-03 09:45:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b8icjy/rstocks_daily_discussion_tuesday_apr_02_2019/'," datetime.datetime(2019, 4, 3, 9, 55, 2, 399931)",'2019-04-03',None,u'0','stocks',"u'r/Stocks Daily Discussion Tuesday - Apr 02, 2019'"," u'https://old.reddit.com/r/stocks/comments/b8icjy/rstocks_daily_discussion_tuesday_apr_02_2019/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 9, 55, 2, 399931) is not JSON serializable
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
2019-04-03 09:55:02 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
2019-04-03 09:55:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b8t62i/help_with_international_stock_adrads_or_ordinary/'," datetime.datetime(2019, 4, 3, 9, 55, 3, 574192)",'2019-04-03',None,u'2','investing',u'Help with International Stock! ADR/ADS or Ordinary Shares?'," u'https://old.reddit.com/r/investing/comments/b8t62i/help_with_international_stock_adrads_or_ordinary/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 9, 55, 3, 574192) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-04-03 09:55:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b8uhy5/what_are_the_benefits_of_investing_in_the_stock/'," datetime.datetime(2019, 4, 3, 10, 0, 5, 153533)",'2019-04-03',None,u'0','StockMarket',u'What are the Benefits of Investing in the Stock Market?'," u'https://old.reddit.com/r/StockMarket/comments/b8uhy5/what_are_the_benefits_of_investing_in_the_stock/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 10, 0, 5, 153533) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8peqy/why_cant_i_short_patagonia/'," datetime.datetime(2019, 4, 3, 10, 5, 3, 278477)",'2019-04-03',u'Satire',u'5','wallstreetbets',u'Why can\u2019t I short Patagonia?'," u'https://www.buzzfeednews.com/article/katienotopoulos/patagonia-power-vest-policy-change'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 10, 5, 3, 278477) is not JSON serializable
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
2019-04-03 10:05:03 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8pown/online_shopping_has_officially_overtaken/'," datetime.datetime(2019, 4, 3, 10, 10, 2, 891969)",'2019-04-03',u'Discussion',u'42','wallstreetbets',u'Online shopping has officially overtaken brick-and-mortar retail for the first time ever'," u'https://www.cnbc.com/2019/04/02/online-shopping-officially-overtakes-brick-and-mortar-retail-for-the-first-time-ever.html?__source=twitter%7Cmain'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 10, 10, 2, 891969) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
2019-04-03 10:10:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b8jul2/swing_trade_on_ba/'," datetime.datetime(2019, 4, 3, 10, 10, 3, 136697)",'2019-04-03',u'Trades',u'1','stocks',u'Swing Trade On BA'," u'https://old.reddit.com/r/stocks/comments/b8jul2/swing_trade_on_ba/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 10, 10, 3, 136697) is not JSON serializable
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
2019-04-03 10:10:03 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
2019-04-03 10:10:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b8t6jl/vtsax_vs_sphd/'," datetime.datetime(2019, 4, 3, 10, 15, 4, 686525)",'2019-04-03',u'Help',u'2','investing',u'VTSAX Vs. SPHD'," u'https://old.reddit.com/r/investing/comments/b8t6jl/vtsax_vs_sphd/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 10, 15, 4, 686525) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-04-03 10:15:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8uwls/seeking_alpha_murder/'," datetime.datetime(2019, 4, 3, 10, 20, 2, 386545)",'2019-04-03',u'Shitpost',u'0','wallstreetbets',u'Seeking Alpha MUrder'," u'https://old.reddit.com/r/wallstreetbets/comments/b8uwls/seeking_alpha_murder/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 10, 20, 2, 386545) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8pvif/seriously_who_talks_like_this_much_less_a_fox/'," datetime.datetime(2019, 4, 3, 10, 20, 2, 425660)",'2019-04-03',u'Shitpost',u'39','wallstreetbets',u'Seriously who talks like this? Much less a Fox Business professional talking head? This guy is out of control on Fannie Mae and the GSEs... wtf and I watching? This guy is on TV:'," u'https://old.reddit.com/r/wallstreetbets/comments/b8pvif/seriously_who_talks_like_this_much_less_a_fox/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 10, 20, 2, 425660) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8pukc/altaba_plans_to_liquidate_100_of_its_alibaba/'," datetime.datetime(2019, 4, 3, 10, 20, 2, 427200)",'2019-04-03',u'Stocks',u'3','wallstreetbets',u'Altaba plans to liquidate 100% of its Alibaba shares...'," u'https://apnews.com/Business%20Wire/31a53dbe58c74bddb42854b406352137'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 10, 20, 2, 427200) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
2019-04-03 10:20:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b8t9en/gamestop_gme_now_9_17_dividend_worth_it/'," datetime.datetime(2019, 4, 3, 10, 20, 3, 828241)",'2019-04-03',u'Education',u'12','investing',u'Gamestop (GME) now $9. 17% dividend. Worth it?'," u'https://old.reddit.com/r/investing/comments/b8t9en/gamestop_gme_now_9_17_dividend_worth_it/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 10, 20, 3, 828241) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-04-03 10:20:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8pw3w/thanks_cramer/'," datetime.datetime(2019, 4, 3, 10, 25, 4, 555391)",'2019-04-03',u'Satire',u'37','wallstreetbets',u'Thanks Cramer...\u200b'," u'https://old.reddit.com/r/wallstreetbets/comments/b8pw3w/thanks_cramer/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 10, 25, 4, 555391) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
2019-04-03 10:25:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8setc/uschina_trade_agreement_reportedly_90_done_lol/'," datetime.datetime(2019, 4, 3, 10, 30, 3, 44375)",'2019-04-03',u'Futures',u'39','wallstreetbets',u'U.S.-China trade agreement reportedly 90% done LOL BYE BYE BEARS SPY 300 EOW'," u'https://www.marketwatch.com/story/us-china-trade-agreement-reportedly-90-done-2019-04-02?mod=mw_latestnews'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 10, 30, 3, 44375) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
2019-04-03 10:30:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b8khmn/what_is_the_play_on_wfc/'," datetime.datetime(2019, 4, 3, 10, 35, 3, 300547)",'2019-04-03',None,u'2','stocks',u'What is the play on WFC?'," u'https://old.reddit.com/r/stocks/comments/b8khmn/what_is_the_play_on_wfc/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 10, 35, 3, 300547) is not JSON serializable
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
2019-04-03 10:35:03 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
2019-04-03 10:35:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b8tzio/is_there_anyway_to_know_when_a_stock_is_going_to/'," datetime.datetime(2019, 4, 3, 10, 35, 4, 543100)",'2019-04-03',None,u'2','investing',u'Is there anyway to know when a stock is going to split? Specifically AMT.'," u'https://old.reddit.com/r/investing/comments/b8tzio/is_there_anyway_to_know_when_a_stock_is_going_to/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 10, 35, 4, 543100) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b8tboh/why_do_fixed_income_investors_use_duration_and/'," datetime.datetime(2019, 4, 3, 10, 35, 4, 544726)",'2019-04-03',None,u'2','investing',u'Why do fixed income investors use duration and convexity instead of absolute measures in measuring interest rate risk?'," u'https://old.reddit.com/r/investing/comments/b8tboh/why_do_fixed_income_investors_use_duration_and/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 10, 35, 4, 544726) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-04-03 10:35:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8t1oe/quit_playin_gme_with_my_heart/'," datetime.datetime(2019, 4, 3, 10, 45, 3, 77338)",'2019-04-03',u'Meme',u'3048','wallstreetbets',u'Quit playin GME with my heart'," u'https://old.reddit.com/r/wallstreetbets/comments/b8t1oe/quit_playin_gme_with_my_heart/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 10, 45, 3, 77338) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
2019-04-03 10:45:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8t852/ethiopian_airlines_pilots_initially_followed/'," datetime.datetime(2019, 4, 3, 10, 55, 2, 759993)",'2019-04-03',u'Stocks',u'47','wallstreetbets',u'Ethiopian Airlines Pilots Initially Followed Boeing\u2019s Required Emergency Steps To Disable 737 MAX System'," u'https://old.reddit.com/r/wallstreetbets/comments/b8t852/ethiopian_airlines_pilots_initially_followed/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 10, 55, 2, 759993) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
2019-04-03 10:55:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
2019-04-03 10:55:02 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8ta1p/lyft_bagholders_unite/'," datetime.datetime(2019, 4, 3, 11, 0, 3, 69576)",'2019-04-03',u'Shitpost',u'8','wallstreetbets',u'LYFT BAGHOLDERS UNITE'," u'https://old.reddit.com/r/wallstreetbets/comments/b8ta1p/lyft_bagholders_unite/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 11, 0, 3, 69576) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
2019-04-03 11:00:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b8khmo/looking_for_some_good_canadian_cannabis_stocks/'," datetime.datetime(2019, 4, 3, 11, 0, 3, 302548)",'2019-04-03',None,u'1','stocks',u'Looking for some good Canadian cannabis stocks. I\u2019ve been following pure global cannabis and FSD pharm. Any insights?'," u'https://old.reddit.com/r/stocks/comments/b8khmo/looking_for_some_good_canadian_cannabis_stocks/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 11, 0, 3, 302548) is not JSON serializable
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
2019-04-03 11:00:03 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
2019-04-03 11:00:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8tan2/when_you_see_the_april_fools_banwave_as_its/'," datetime.datetime(2019, 4, 3, 11, 10, 2, 846486)",'2019-04-03',u'Meme',u'43','wallstreetbets',"u'When you see the April Fools banwave as its happening so you resign as a moderator, then make a shitty meme about it and get banned for even longer than you would have otherwise.'"," u'https://old.reddit.com/r/wallstreetbets/comments/b8tan2/when_you_see_the_april_fools_banwave_as_its/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 11, 10, 2, 846486) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
2019-04-03 11:10:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
2019-04-03 11:10:02 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Daytrading/comments/b6yygo/day_4_of_paper_trading/'," datetime.datetime(2019, 4, 3, 11, 10, 6, 187112)",'2019-04-03',None,u'1','Daytrading',u'Day 4 of paper trading'," u'https://old.reddit.com/r/Daytrading/comments/b6yygo/day_4_of_paper_trading/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 11, 10, 6, 187112) is not JSON serializable
INFO:scrapy.core.engine:Closing spider (finished)
2019-04-03 11:10:06 [scrapy.core.engine] INFO: Closing spider (finished)
INFO:scrapy.statscollectors:Dumping Scrapy stats:
"
https://old.reddit.com/r/investing/comments/b8u2r2/how_to_destroy_spy_using_leverage_and_top_global/'," datetime.datetime(2019, 4, 3, 11, 15, 4, 531593)",'2019-04-03',None,u'0','investing',u'How to destroy SPY using leverage and top global consumer brands'," u'https://old.reddit.com/r/investing/comments/b8u2r2/how_to_destroy_spy_using_leverage_and_top_global/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 11, 15, 4, 531593) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-04-03 11:15:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b8ub91/is_it_still_a_good_time_to_go_back_to_crypto/'," datetime.datetime(2019, 4, 3, 11, 25, 3, 609986)",'2019-04-03',None,u'0','investing',u'Is it still a good time to go back to crypto?'," u'https://old.reddit.com/r/investing/comments/b8ub91/is_it_still_a_good_time_to_go_back_to_crypto/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 11, 25, 3, 609986) is not JSON serializable
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Daytrading/new/> from <GET https://old.reddit.com/r/daytrading/new/>
2019-04-03 11:25:03 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Daytrading/new/> from <GET https://old.reddit.com/r/daytrading/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-04-03 11:25:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8uhuf/leave_our_boy_alone/'," datetime.datetime(2019, 4, 3, 11, 40, 2, 702936)",'2019-04-03',u'Meme',u'30','wallstreetbets',u'Leave our boy alone'," u'https://old.reddit.com/r/wallstreetbets/comments/b8uhuf/leave_our_boy_alone/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 11, 40, 2, 702936) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8u28y/buy_ea_sure_they_are_a_shitty_gaming_company_to/'," datetime.datetime(2019, 4, 3, 11, 40, 2, 704606)",'2019-04-03',u'Discussion',u'0','wallstreetbets',"u""Buy EA. Sure they are a shitty gaming company to it's players but its all about the shareholders. With Google announcing Stadia, i highly recommend to buy EA and sell during the E3 event when the stock is hot. Cloud computing is the future."""," u'https://old.reddit.com/r/wallstreetbets/comments/b8u28y/buy_ea_sure_they_are_a_shitty_gaming_company_to/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 11, 40, 2, 704606) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8u1kj/warren_buffet_bet_recap_for_all_of_you_hardos/'," datetime.datetime(2019, 4, 3, 11, 40, 2, 707702)",'2019-04-03',u'Storytime',u'1','wallstreetbets',u'WARREN BUFFET BET RECAP for all of YOU HARDOS'," u'https://old.reddit.com/r/wallstreetbets/comments/b8u1kj/warren_buffet_bet_recap_for_all_of_you_hardos/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 11, 40, 2, 707702) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8v4ec/lost_redditor/'," datetime.datetime(2019, 4, 3, 11, 45, 3, 434716)",'2019-04-03',u'Shitpost',u'90','wallstreetbets',u'Lost redditor??'," u'https://i.imgur.com/Cj3gChZ.jpg'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 11, 45, 3, 434716) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8v2h0/tesla_to_host_autonomy_investor_day_tesla_inc/'," datetime.datetime(2019, 4, 3, 11, 45, 3, 436392)",'2019-04-03',u'Stocks',u'15','wallstreetbets',"u'Tesla To Host Autonomy Investor Day | Tesla, Inc.'"," u'http://ir.tesla.com/news-releases/news-release-details/tesla-host-autonomy-investor-day'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 11, 45, 3, 436392) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8uszo/tsla_fudging_production_numbers_or_just_a_weird/'," datetime.datetime(2019, 4, 3, 11, 45, 3, 438126)",'2019-04-03',u'Discussion',u'0','wallstreetbets',u'$TSLA Fudging Production Numbers or just a weird storage solution? Random Teslas found on the 5th Floor of a mall parking garage'," u'https://twitter.com/gupamf/status/1113283728640368646?s=21'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 11, 45, 3, 438126) is not JSON serializable
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
2019-04-03 11:45:03 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b8l7lr/has_there_ever_been_an_ipo_that_didnt_initially/'," datetime.datetime(2019, 4, 3, 11, 45, 3, 558748)",'2019-04-03',None,u'1','stocks',"u""Has there ever been an IPO that didn't initially drop?"""," u'https://old.reddit.com/r/stocks/comments/b8l7lr/has_there_ever_been_an_ipo_that_didnt_initially/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 11, 45, 3, 558748) is not JSON serializable
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
2019-04-03 11:45:03 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
2019-04-03 11:45:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b8lhz0/ladybaybee_picks_for_20190402/'," datetime.datetime(2019, 4, 3, 11, 50, 3, 20375)",'2019-04-03',None,u'8','stocks',u'Ladybaybee picks for 2019-04-02'," u'https://old.reddit.com/r/stocks/comments/b8lhz0/ladybaybee_picks_for_20190402/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 11, 50, 3, 20375) is not JSON serializable
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
2019-04-03 11:50:03 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
2019-04-03 11:50:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b8lxn9/rstocks_technicals_tuesday_apr_02_2019/'," datetime.datetime(2019, 4, 3, 11, 55, 2, 828720)",'2019-04-03',None,u'3','stocks',"u'r/Stocks Technicals Tuesday - Apr 02, 2019'"," u'https://old.reddit.com/r/stocks/comments/b8lxn9/rstocks_technicals_tuesday_apr_02_2019/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 11, 55, 2, 828720) is not JSON serializable
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
2019-04-03 11:55:03 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
2019-04-03 11:55:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b87gll/significant_insider_trading_activity_last_7_days/'," datetime.datetime(2019, 4, 3, 11, 55, 6, 141950)",'2019-04-03',None,u'11','StockMarket',u'Significant Insider Trading Activity (Last 7 Days)'," u'https://old.reddit.com/r/StockMarket/comments/b87gll/significant_insider_trading_activity_last_7_days/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 11, 55, 6, 141950) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/Daytrading/new/> (referer: None)
2019-04-03 11:55:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/Daytrading/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b8mtd9/ppa_vs_ita/'," datetime.datetime(2019, 4, 3, 12, 0, 4, 18664)",'2019-04-03',None,u'3','stocks',u'PPA vs ITA'," u'https://old.reddit.com/r/stocks/comments/b8mtd9/ppa_vs_ita/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 12, 0, 4, 18664) is not JSON serializable
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
2019-04-03 12:00:04 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
2019-04-03 12:00:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Daytrading/comments/b71m25/can_someone_please_give_me_some_advice_on/'," datetime.datetime(2019, 4, 3, 12, 5, 7, 888726)",'2019-04-03',None,u'4','Daytrading',u'Can someone please give me some advice on screening for stock to day trade.'," u'https://old.reddit.com/r/Daytrading/comments/b71m25/can_someone_please_give_me_some_advice_on/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 12, 5, 7, 888726) is not JSON serializable
INFO:scrapy.core.engine:Closing spider (finished)
2019-04-03 12:05:07 [scrapy.core.engine] INFO: Closing spider (finished)
INFO:scrapy.statscollectors:Dumping Scrapy stats:
"
https://old.reddit.com/r/wallstreetbets/comments/b8y0my/absolute_unit/'," datetime.datetime(2019, 4, 3, 12, 15, 3, 153735)",'2019-04-03',u'Shitpost',u'118','wallstreetbets',u'Absolute UNIT'," u'https://old.reddit.com/r/wallstreetbets/comments/b8y0my/absolute_unit/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 12, 15, 3, 153735) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8v5g4/saw_this_in_a_private_group_he_was_serious_elon/'," datetime.datetime(2019, 4, 3, 12, 20, 2, 217252)",'2019-04-03',u'Shitpost',u'52','wallstreetbets',"u'Saw this in a Private Group, He Was Serious. Elon Musk Twitter Has Real Life Consequences'"," u'https://old.reddit.com/r/wallstreetbets/comments/b8v5g4/saw_this_in_a_private_group_he_was_serious_elon/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 12, 20, 2, 217252) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
2019-04-03 12:20:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b8uf0i/value_investing_is_failing_me/'," datetime.datetime(2019, 4, 3, 12, 20, 3, 648155)",'2019-04-03',u'Discussion',u'5','investing',u'Value investing is failing me'," u'https://old.reddit.com/r/investing/comments/b8uf0i/value_investing_is_failing_me/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 12, 20, 3, 648155) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b8ubtd/high_dividends/'," datetime.datetime(2019, 4, 3, 12, 20, 3, 649806)",'2019-04-03',None,u'0','investing',u'High dividends'," u'https://old.reddit.com/r/investing/comments/b8ubtd/high_dividends/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 12, 20, 3, 649806) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-04-03 12:20:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b8mtqy/similar_site_to_stocktwits/'," datetime.datetime(2019, 4, 3, 12, 25, 2, 741757)",'2019-04-03',u'Resources',u'4','stocks',u'Similar site to StockTwits?'," u'https://old.reddit.com/r/stocks/comments/b8mtqy/similar_site_to_stocktwits/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 12, 25, 2, 741757) is not JSON serializable
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
2019-04-03 12:25:02 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
2019-04-03 12:25:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b8uoqe/american_toweramt_or_crown_castlecci/'," datetime.datetime(2019, 4, 3, 12, 25, 3, 616334)",'2019-04-03',u'Discussion',u'3','investing',u'American Tower(AMT) or Crown Castle(CCI)'," u'https://old.reddit.com/r/investing/comments/b8uoqe/american_toweramt_or_crown_castlecci/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 12, 25, 3, 616334) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-04-03 12:25:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b87gv0/significant_activist_hedge_fund_activity_last_7/'," datetime.datetime(2019, 4, 3, 12, 30, 6, 69433)",'2019-04-03',None,u'2','StockMarket',u'Significant Activist Hedge Fund Activity (Last 7 Days)'," u'https://old.reddit.com/r/StockMarket/comments/b87gv0/significant_activist_hedge_fund_activity_last_7/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 12, 30, 6, 69433) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/Daytrading/new/> (referer: None)
2019-04-03 12:30:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/Daytrading/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b8uv0x/visa_stock/'," datetime.datetime(2019, 4, 3, 12, 40, 4, 858128)",'2019-04-03',None,u'0','investing',u'visa stock'," u'https://old.reddit.com/r/investing/comments/b8uv0x/visa_stock/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 12, 40, 4, 858128) is not JSON serializable
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Daytrading/new/> from <GET https://old.reddit.com/r/daytrading/new/>
2019-04-03 12:40:04 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Daytrading/new/> from <GET https://old.reddit.com/r/daytrading/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-04-03 12:40:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Trading/comments/b0n2wu/best_trading_broker_for_nonus_citizen/'," datetime.datetime(2019, 4, 3, 12, 50, 6, 298541)",'2019-04-03',None,u'1','Trading',u'Best trading broker for Non-Us citizen'," u'https://old.reddit.com/r/Trading/comments/b0n2wu/best_trading_broker_for_nonus_citizen/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 12, 50, 6, 298541) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Trading/comments/b09cre/whats_the_difference_between_market_wizards_and/'," datetime.datetime(2019, 4, 3, 12, 50, 6, 302100)",'2019-04-03',None,u'6','Trading',u'What\u2019s the difference between Market Wizards and The New Market Wizards?'," u'https://old.reddit.com/r/Trading/comments/b09cre/whats_the_difference_between_market_wizards_and/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 12, 50, 6, 302100) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Trading/comments/b02qm1/how_to_know_if_a_security_is_optionable/'," datetime.datetime(2019, 4, 3, 12, 50, 6, 305882)",'2019-04-03',None,u'0','Trading',u'How to know if a security is optionable?'," u'https://old.reddit.com/r/Trading/comments/b02qm1/how_to_know_if_a_security_is_optionable/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 12, 50, 6, 305882) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b88eje/robinhood_for_passive/'," datetime.datetime(2019, 4, 3, 12, 50, 6, 752787)",'2019-04-03',None,u'6','StockMarket',u'Robinhood for Passive'," u'https://old.reddit.com/r/StockMarket/comments/b88eje/robinhood_for_passive/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 12, 50, 6, 752787) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b87up5/lyfts_tumbling_stock_is_a_worrying_sign_for_other/'," datetime.datetime(2019, 4, 3, 12, 50, 6, 757302)",'2019-04-03',None,u'174','StockMarket',u'Lyft\u2019s Tumbling Stock Is a Worrying Sign for Other Unicorns'," u'https://old.reddit.com/r/StockMarket/comments/b87up5/lyfts_tumbling_stock_is_a_worrying_sign_for_other/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 12, 50, 6, 757302) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8yvrs/cvs_and_aetna_insight_on_the_business_moving/'," datetime.datetime(2019, 4, 3, 12, 55, 2, 615059)",'2019-04-03',u'DD',u'6','wallstreetbets',u'CVS and Aetna - Insight on the business moving forward.'," u'https://old.reddit.com/r/wallstreetbets/comments/b8yvrs/cvs_and_aetna_insight_on_the_business_moving/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 12, 55, 2, 615059) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8yk2a/secret_when_a_meme_stock_amd_goes_up_10_it_will/'," datetime.datetime(2019, 4, 3, 12, 55, 2, 747267)",'2019-04-03',u'Shitpost',u'12','wallstreetbets',"u'Secret: When a meme stock (AMD) goes up 10%, it will go higher. Happens every time.'"," u'https://old.reddit.com/r/wallstreetbets/comments/b8yk2a/secret_when_a_meme_stock_amd_goes_up_10_it_will/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 12, 55, 2, 747267) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b8n4x3/selling_and_rebuy/'," datetime.datetime(2019, 4, 3, 13, 0, 3, 5499)",'2019-04-03',None,u'1','stocks',u'Selling and Rebuy?'," u'https://old.reddit.com/r/stocks/comments/b8n4x3/selling_and_rebuy/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 13, 0, 3, 5499) is not JSON serializable
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
2019-04-03 13:00:03 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
2019-04-03 13:00:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8vt3b/gangs_of_wallstreetbets/'," datetime.datetime(2019, 4, 3, 13, 5, 3, 609501)",'2019-04-03',u'Meme',u'268','wallstreetbets',u'Gangs of Wallstreetbets'," u'https://i.imgur.com/tG4NiSt.jpg'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 13, 5, 3, 609501) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8vewz/ntrp_alzheimers_disease_co_is_starting_to_have/'," datetime.datetime(2019, 4, 3, 13, 5, 3, 612117)",'2019-04-03',u'Stocks',u'1','wallstreetbets',"u""NTRP, Alzheimer's disease co is starting to have momentum"""," u'https://old.reddit.com/r/wallstreetbets/comments/b8vewz/ntrp_alzheimers_disease_co_is_starting_to_have/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 13, 5, 3, 612117) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b8oqga/amazons_giant_dystopian_deliverydrone_blimp_isnt/'," datetime.datetime(2019, 4, 3, 13, 5, 3, 613798)",'2019-04-03',None,u'332','stocks',"u""Amazon's giant 'dystopian' delivery-drone blimp isn't real yet, but it's something the tech giant has explored"""," u'https://old.reddit.com/r/stocks/comments/b8oqga/amazons_giant_dystopian_deliverydrone_blimp_isnt/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 13, 5, 3, 613798) is not JSON serializable
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
2019-04-03 13:05:03 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
2019-04-03 13:05:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8wj7z/daily_discussion_thread_april_03_2019/'," datetime.datetime(2019, 4, 3, 13, 10, 2, 676065)",'2019-04-03',u'Daily Discussion',u'53','wallstreetbets',"u'Daily Discussion Thread - April 03, 2019'"," u'https://old.reddit.com/r/wallstreetbets/comments/b8wj7z/daily_discussion_thread_april_03_2019/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 13, 10, 2, 676065) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
2019-04-03 13:10:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8xgif/so_youre_saying_i_have_a_chance/'," datetime.datetime(2019, 4, 3, 13, 15, 3, 162435)",'2019-04-03',u'Shitpost',u'530','wallstreetbets',"u""So you're saying I have a chance..."""," u'https://old.reddit.com/r/wallstreetbets/comments/b8xgif/so_youre_saying_i_have_a_chance/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 13, 15, 3, 162435) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8x0ug/when_you_get_a_cash_infusion_from_your_investors/'," datetime.datetime(2019, 4, 3, 13, 15, 3, 164524)",'2019-04-03',u'Meme',u'0','wallstreetbets',u'When you get a cash infusion from your investors'," u'https://old.reddit.com/r/wallstreetbets/comments/b8x0ug/when_you_get_a_cash_infusion_from_your_investors/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 13, 15, 3, 164524) is not JSON serializable
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
2019-04-03 13:15:03 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
2019-04-03 13:15:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8xri7/a_company_lyft_partners_with_to_just_use_their/'," datetime.datetime(2019, 4, 3, 13, 25, 3, 73895)",'2019-04-03',u'Stocks',u'0','wallstreetbets',"u'A Company $LYFT partners with to JUST use their software, now does better revenues than $LYFT lolwut'"," u'https://old.reddit.com/r/wallstreetbets/comments/b8xri7/a_company_lyft_partners_with_to_just_use_their/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 13, 25, 3, 73895) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
2019-04-03 13:25:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b8uxtc/investing_with_equal_pay_in_mind_may_be_more/'," datetime.datetime(2019, 4, 3, 13, 30, 3, 738814)",'2019-04-03',None,u'0','investing',u'Investing with equal pay in mind may be more difficult than you think'," u'https://old.reddit.com/r/investing/comments/b8uxtc/investing_with_equal_pay_in_mind_may_be_more/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 13, 30, 3, 738814) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-04-03 13:30:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8yh9q/amd_up_first_options_tradeso_i_can_quit_my_job/'," datetime.datetime(2019, 4, 3, 13, 35, 2, 699161)",'2019-04-03',None,u'0','wallstreetbets',u'AMD up. first options tradeSo I can quit my job now right?'," u'https://old.reddit.com/r/wallstreetbets/comments/b8yh9q/amd_up_first_options_tradeso_i_can_quit_my_job/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 13, 35, 2, 699161) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
2019-04-03 13:35:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8yfwc/cramer_straight_going_manic_on_twitter/'," datetime.datetime(2019, 4, 3, 13, 35, 2, 706432)",'2019-04-03',u'Shitpost',u'41','wallstreetbets',u'Cramer straight going manic on Twitter'," u'https://twitter.com/jimcramer/status/1113431704302231553?s=20'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 13, 35, 2, 706432) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b8v7dy/how_did_invesco_sp_500_etf_triple_rather_than_x25/'," datetime.datetime(2019, 4, 3, 13, 55, 4, 575132)",'2019-04-03',None,u'0','investing',u'How did Invesco S&P 500 ETF triple rather than x2.5 in the past eight years?'," u'https://old.reddit.com/r/investing/comments/b8v7dy/how_did_invesco_sp_500_etf_triple_rather_than_x25/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 13, 55, 4, 575132) is not JSON serializable
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Daytrading/new/> from <GET https://old.reddit.com/r/daytrading/new/>
2019-04-03 13:55:04 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Daytrading/new/> from <GET https://old.reddit.com/r/daytrading/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-04-03 13:55:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8yn46/the_perfect_hedge_shop/'," datetime.datetime(2019, 4, 3, 14, 0, 2, 791805)",'2019-04-03',u'Shitpost',u'213','wallstreetbets',u'The Perfect Hedge $SHOP'," u'https://old.reddit.com/r/wallstreetbets/comments/b8yn46/the_perfect_hedge_shop/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 14, 0, 2, 791805) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8ykdj/why_is_tqqq_or_soxl_considered_an_intraday/'," datetime.datetime(2019, 4, 3, 14, 0, 2, 793641)",'2019-04-03',u'Discussion',u'7','wallstreetbets',u'Why is TQQQ or SOXL considered an intraday trading instrument if you can get these kind of gains?'," u'https://i.imgur.com/fYMkQe8.jpg'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 14, 0, 2, 793641) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
2019-04-03 14:00:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8ypwz/100k_of_tsla_stock_too_scared_to_buy_options/'," datetime.datetime(2019, 4, 3, 14, 10, 2, 708008)",'2019-04-03',u'Stocks',u'21','wallstreetbets',u'$100k of $tsla stock - too scared to buy options'," u'https://old.reddit.com/r/wallstreetbets/comments/b8ypwz/100k_of_tsla_stock_too_scared_to_buy_options/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 14, 10, 2, 708008) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b8vs60/where_to_go_from_here_in_order_to_grow_in_real/'," datetime.datetime(2019, 4, 3, 14, 15, 5, 53001)",'2019-04-03',u'Help',u'0','investing',u'Where to go from here in order to grow in real estate'," u'https://old.reddit.com/r/investing/comments/b8vs60/where_to_go_from_here_in_order_to_grow_in_real/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 14, 15, 5, 53001) is not JSON serializable
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Daytrading/new/> from <GET https://old.reddit.com/r/daytrading/new/>
2019-04-03 14:15:05 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Daytrading/new/> from <GET https://old.reddit.com/r/daytrading/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-04-03 14:15:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b88hwh/tech_analysis_chart/'," datetime.datetime(2019, 4, 3, 14, 15, 7, 77870)",'2019-04-03',None,u'1','StockMarket',u'Tech Analysis Chart'," u'https://old.reddit.com/r/StockMarket/comments/b88hwh/tech_analysis_chart/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 14, 15, 7, 77870) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/Daytrading/new/> (referer: None)
2019-04-03 14:15:07 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/Daytrading/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8z2cw/promised_id_update_never_bet_against_amazon_roll/'," datetime.datetime(2019, 4, 3, 14, 20, 2, 981328)",'2019-04-03',u'Gain',u'166','wallstreetbets',"u""Promised I'd update. Never bet against Amazon - roll losers into winners by sizing up into positions. Don't blow up my inbox please, that's why I shut this account down last time!"""," u'https://old.reddit.com/r/wallstreetbets/comments/b8z2cw/promised_id_update_never_bet_against_amazon_roll/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 14, 20, 2, 981328) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b8w0ej/why_do_academic_type_investors_lose_in_the_real/'," datetime.datetime(2019, 4, 3, 14, 30, 3, 761451)",'2019-04-03',None,u'0','investing',u'Why do academic type investors lose in the real world?'," u'https://old.reddit.com/r/investing/comments/b8w0ej/why_do_academic_type_investors_lose_in_the_real/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 14, 30, 3, 761451) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b8vty8/hedge_fund_liquidated_who_pays_costs/'," datetime.datetime(2019, 4, 3, 14, 30, 3, 764321)",'2019-04-03',None,u'4','investing',"u'Hedge fund liquidated, who pays costs?'"," u'https://old.reddit.com/r/investing/comments/b8vty8/hedge_fund_liquidated_who_pays_costs/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 14, 30, 3, 764321) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-04-03 14:30:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b8x2gc/warren_buffett_banks_will_be_worth_more_money_10/'," datetime.datetime(2019, 4, 3, 14, 35, 10, 62605)",'2019-04-03',None,u'376','investing',u'Warren Buffett: Banks will be worth more money 10 years from now'," u'https://old.reddit.com/r/investing/comments/b8x2gc/warren_buffett_banks_will_be_worth_more_money_10/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 14, 35, 10, 62605) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b8wv4n/do_you_ever_sell_some_long_term_position_for_a/'," datetime.datetime(2019, 4, 3, 14, 35, 10, 68935)",'2019-04-03',None,u'0','investing',u'Do you ever sell some long term position for a profit?'," u'https://old.reddit.com/r/investing/comments/b8wv4n/do_you_ever_sell_some_long_term_position_for_a/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 14, 35, 10, 68935) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-04-03 14:35:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b8ppgc/who_else_is_holding_insys_therapeutics/'," datetime.datetime(2019, 4, 3, 14, 40, 6, 644935)",'2019-04-03',None,u'0','stocks',u'Who else is holding Insys Therapeutics?'," u'https://old.reddit.com/r/stocks/comments/b8ppgc/who_else_is_holding_insys_therapeutics/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 14, 40, 6, 644935) is not JSON serializable
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
2019-04-03 14:40:06 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
2019-04-03 14:40:07 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b8xled/learning_to_invest_what_are_the_reasons_for_ever/'," datetime.datetime(2019, 4, 3, 14, 45, 39, 927563)",'2019-04-03',u'Discussion',u'21','investing',u'Learning to Invest: What are the reasons for ever NOT using the cheapest online broker?'," u'https://old.reddit.com/r/investing/comments/b8xled/learning_to_invest_what_are_the_reasons_for_ever/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 14, 45, 39, 927563) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-04-03 14:45:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b88mcy/what_is_short_vix/'," datetime.datetime(2019, 4, 3, 14, 50, 5, 888823)",'2019-04-03',None,u'0','StockMarket',u'What is short VIX?'," u'https://old.reddit.com/r/StockMarket/comments/b88mcy/what_is_short_vix/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 14, 50, 5, 888823) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/Daytrading/new/> (referer: None)
2019-04-03 14:50:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/Daytrading/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8z6xo/wild_ride/'," datetime.datetime(2019, 4, 3, 14, 55, 4, 737168)",'2019-04-03',u'Storytime',u'0','wallstreetbets',u'Wild Ride'," u'https://old.reddit.com/r/wallstreetbets/comments/b8z6xo/wild_ride/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 14, 55, 4, 737168) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8zl14/its_been_a_rough_q1_2019_for_the_bears/'," datetime.datetime(2019, 4, 3, 15, 0, 6, 312194)",'2019-04-03',u'Shitpost',u'212','wallstreetbets',"u""It's been a rough Q1 2019 for the Bears"""," u'https://old.reddit.com/r/wallstreetbets/comments/b8zl14/its_been_a_rough_q1_2019_for_the_bears/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 15, 0, 6, 312194) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b8xmlv/backdoor_roth_ira_question/'," datetime.datetime(2019, 4, 3, 15, 0, 7, 521991)",'2019-04-03',None,u'0','investing',u'Backdoor Roth IRA Question'," u'https://old.reddit.com/r/investing/comments/b8xmlv/backdoor_roth_ira_question/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 15, 0, 7, 521991) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-04-03 15:00:07 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8znug/once_again_charles_gasparino_a_fox_news/'," datetime.datetime(2019, 4, 3, 15, 10, 2, 991554)",'2019-04-03',None,u'0','wallstreetbets',"u'Once again Charles Gasparino, a Fox News contributor, continues to shit post his followers day in and day out. Never seen anything like this before... how does this \u201cprofessional\u201d still have a job?'"," u'https://old.reddit.com/r/wallstreetbets/comments/b8znug/once_again_charles_gasparino_a_fox_news/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 15, 10, 2, 991554) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
2019-04-03 15:10:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
2019-04-03 15:10:03 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b8pqn9/disney_stocks/'," datetime.datetime(2019, 4, 3, 15, 15, 3, 16423)",'2019-04-03',u'Advice',u'2','stocks',u'Disney stocks'," u'https://old.reddit.com/r/stocks/comments/b8pqn9/disney_stocks/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 15, 15, 3, 16423) is not JSON serializable
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
2019-04-03 15:15:03 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
2019-04-03 15:15:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b8y31q/understanding_dwdp/'," datetime.datetime(2019, 4, 3, 15, 30, 4, 413838)",'2019-04-03',None,u'2','investing',u'Understanding DWDP'," u'https://old.reddit.com/r/investing/comments/b8y31q/understanding_dwdp/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 15, 30, 4, 413838) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b8xyov/vtsax_alternatives/'," datetime.datetime(2019, 4, 3, 15, 30, 4, 424653)",'2019-04-03',None,u'1','investing',u'VTSAX alternatives'," u'https://old.reddit.com/r/investing/comments/b8xyov/vtsax_alternatives/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 15, 30, 4, 424653) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-04-03 15:30:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8zqh8/sen_ron_wyden_dore_has_proposed_a_socalled/'," datetime.datetime(2019, 4, 3, 15, 35, 4, 66197)",'2019-04-03',u'Discussion',u'6','wallstreetbets',"u'Sen. Ron Wyden, D-Ore., has proposed a so-called mark-to-market version of the capital gains tax, which would tax unrealized gains.'"," u'https://www.cnbc.com/2019/04/03/top-democrats-proposed-capital-gains-tax-would-be-devastating-for-markets.html'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 15, 35, 4, 66197) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8zoa0/pomp_eet/'," datetime.datetime(2019, 4, 3, 15, 35, 4, 68411)",'2019-04-03',u'Meme',u'42','wallstreetbets',u'Pomp eet'," u'https://imgur.com/NwjytA0'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 15, 35, 4, 68411) is not JSON serializable
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
2019-04-03 15:35:04 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b902ja/i_had_a_dream/'," datetime.datetime(2019, 4, 3, 15, 40, 2, 161555)",'2019-04-03',None,u'1','wallstreetbets',u'I had a dream'," u'https://old.reddit.com/r/wallstreetbets/comments/b902ja/i_had_a_dream/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 15, 40, 2, 161555) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b8zyg8/how_are_we_playing_the_inevitable_mu_pullback_to/'," datetime.datetime(2019, 4, 3, 15, 40, 2, 163190)",'2019-04-03',u'Discussion',u'7','wallstreetbets',u'How are we playing the inevitable MU pullback to $40 by EOW?'," u'https://old.reddit.com/r/wallstreetbets/comments/b8zyg8/how_are_we_playing_the_inevitable_mu_pullback_to/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 15, 40, 2, 163190) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
2019-04-03 15:40:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
2019-04-03 15:40:02 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b8y7eb/ft_says_the_view_of_a_firm_yield_curve_nonlong/'," datetime.datetime(2019, 4, 3, 15, 50, 4, 843928)",'2019-04-03',u'News',u'9','investing',"u'FT says the view of a firm ""Yield curve nonlong related to economic growth""'"," u'https://old.reddit.com/r/investing/comments/b8y7eb/ft_says_the_view_of_a_firm_yield_curve_nonlong/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 15, 50, 4, 843928) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b8y4x6/aapl_streaming_service/'," datetime.datetime(2019, 4, 3, 15, 50, 4, 845547)",'2019-04-03',None,u'0','investing',u'AAPL Streaming Service'," u'https://old.reddit.com/r/investing/comments/b8y4x6/aapl_streaming_service/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 15, 50, 4, 845547) is not JSON serializable
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Daytrading/new/> from <GET https://old.reddit.com/r/daytrading/new/>
2019-04-03 15:50:04 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Daytrading/new/> from <GET https://old.reddit.com/r/daytrading/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-04-03 15:50:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b905oj/mia_khalifa_getting_pumped_and_dumped_in_new_ways/'," datetime.datetime(2019, 4, 3, 15, 55, 4, 516696)",'2019-04-03',u'Shitpost',u'1212','wallstreetbets',u'Mia Khalifa getting pumped and dumped in new ways'," u'https://old.reddit.com/r/wallstreetbets/comments/b905oj/mia_khalifa_getting_pumped_and_dumped_in_new_ways/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 15, 55, 4, 516696) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b9059y/i_never_believed_in_fds_but_all_your_sceenshots/'," datetime.datetime(2019, 4, 3, 15, 55, 4, 519377)",'2019-04-03',u'Options',u'19','wallstreetbets',"u'I never believed in FDs, but all your sceenshots had persuaded me to test the waters. Being a noob to FDs, I need your expert opinion. Do I sell at this point, or hold till expiration?'"," u'https://old.reddit.com/r/wallstreetbets/comments/b9059y/i_never_believed_in_fds_but_all_your_sceenshots/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 15, 55, 4, 519377) is not JSON serializable
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
2019-04-03 15:55:04 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b9035j/not_a_yolo_just_a_big_week_for_a_lil_portfolio/'," datetime.datetime(2019, 4, 3, 15, 55, 4, 524430)",'2019-04-03',u'Gain',u'26','wallstreetbets',"u'not a YOLO, just a big week for a lil portfolio'"," u'https://i.imgur.com/qNvXwnY.png'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 15, 55, 4, 524430) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/thewallstreet/comments/b6ho45/daily_discussion_march_28/'," datetime.datetime(2019, 4, 3, 16, 0, 5, 316477)",'2019-04-03',u'Daily',u'13','thewallstreet',u'Daily Discussion - (March 28)'," u'https://old.reddit.com/r/thewallstreet/comments/b6ho45/daily_discussion_march_28/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 16, 0, 5, 316477) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b90nh7/hedge_funds_are_loading_up_to_bet_against_lyft/'," datetime.datetime(2019, 4, 3, 16, 10, 2, 726425)",'2019-04-03',None,u'1','stocks',u'Hedge funds are loading up to bet against Lyft'," u'https://old.reddit.com/r/stocks/comments/b90nh7/hedge_funds_are_loading_up_to_bet_against_lyft/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 16, 10, 2, 726425) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b90kpy/apples_first_new_subscription_service_is_unlikely/'," datetime.datetime(2019, 4, 3, 16, 15, 2, 870398)",'2019-04-03',None,u'1','stocks',"u""Apple's first new subscription service is unlikely to move the needle towards the company's lofty goals for services"""," u'https://old.reddit.com/r/stocks/comments/b90kpy/apples_first_new_subscription_service_is_unlikely/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 16, 15, 2, 870398) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b8a5bl/stocks_to_watch_after_a_dovish_surprise/'," datetime.datetime(2019, 4, 3, 16, 15, 6, 173359)",'2019-04-03',None,u'18','StockMarket',u'Stocks to Watch After a Dovish Surprise'," u'https://old.reddit.com/r/StockMarket/comments/b8a5bl/stocks_to_watch_after_a_dovish_surprise/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 16, 15, 6, 173359) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/Daytrading/new/> (referer: None)
2019-04-03 16:15:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/Daytrading/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b8cdr0/hd_621_215c_when_to_sell/'," datetime.datetime(2019, 4, 3, 16, 25, 6, 942981)",'2019-04-03',None,u'3','StockMarket',u'HD 6/21 215c. When to sell?'," u'https://old.reddit.com/r/StockMarket/comments/b8cdr0/hd_621_215c_when_to_sell/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 16, 25, 6, 942981) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b8qk27/if_a_companys_stocks_are_increasing_does_that/'," datetime.datetime(2019, 4, 3, 16, 30, 3, 225276)",'2019-04-03',u'Question',u'1','stocks',"u""If a company's stocks are increasing, does that mean they are obtaining more profit and are successful?"""," u'https://old.reddit.com/r/stocks/comments/b8qk27/if_a_companys_stocks_are_increasing_does_that/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 16, 30, 3, 225276) is not JSON serializable
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
2019-04-03 16:30:03 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
2019-04-03 16:30:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b90h6n/think_im_done_here_guys/'," datetime.datetime(2019, 4, 3, 16, 35, 2, 788723)",'2019-04-03',None,u'42','wallstreetbets',u'Think im done here guys'," u'https://old.reddit.com/r/wallstreetbets/comments/b90h6n/think_im_done_here_guys/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 16, 35, 2, 788723) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b8yams/intellectually_investing_is_simple_but/'," datetime.datetime(2019, 4, 3, 16, 45, 4, 453679)",'2019-04-03',u'Discussion',u'0','investing',"u'Intellectually, Investing is simple, but emotionally, Investing is difficult.'"," u'https://old.reddit.com/r/investing/comments/b8yams/intellectually_investing_is_simple_but/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 16, 45, 4, 453679) is not JSON serializable
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Daytrading/new/> from <GET https://old.reddit.com/r/daytrading/new/>
2019-04-03 16:45:04 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Daytrading/new/> from <GET https://old.reddit.com/r/daytrading/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-04-03 16:45:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b90noh/ge_q1_coming_up_11_call/'," datetime.datetime(2019, 4, 3, 16, 50, 3, 824415)",'2019-04-03',u'Shitpost',u'0','wallstreetbets',u'GE Q1 coming up $11 call?'," u'https://old.reddit.com/r/wallstreetbets/comments/b90noh/ge_q1_coming_up_11_call/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 16, 50, 3, 824415) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b90qhm/new_to_robinhood_i_dont_know_how_but_apparently_i/'," datetime.datetime(2019, 4, 3, 16, 55, 2, 782221)",'2019-04-03',u'Gain',u'9','wallstreetbets',"u""New to robinhood. I don't know how but apparently I made some money."""," u'https://imgur.com/6ZB2g1n'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 16, 55, 2, 782221) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
2019-04-03 16:55:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
2019-04-03 16:55:02 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b90tbv/missiondriven_companies_that_prioritize_the_planet/'," datetime.datetime(2019, 4, 3, 17, 0, 3, 108247)",'2019-04-03',u'Discussion',u'1','wallstreetbets',"u'\u201cmission-driven companies that prioritize the planet,\u201d'"," u'https://www.bloomberg.com/news/articles/2019-04-03/sorry-wall-streeters-you-now-need-to-earn-your-patagonia-vests'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 17, 0, 3, 108247) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
2019-04-03 17:00:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b90xgo/question_for_anyone_with_little_historical/'," datetime.datetime(2019, 4, 3, 17, 5, 2, 705459)",'2019-04-03',u'Shitpost',u'32','wallstreetbets',"u'Question for anyone with little historical perspective: Has that ever worked out well, when the Germans are in economic decline and the rest of Europe is thriving? Seems like there was something...'"," u'https://old.reddit.com/r/wallstreetbets/comments/b90xgo/question_for_anyone_with_little_historical/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 17, 5, 2, 705459) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b8yjc7/the_decade_of_deleveraging_didnt_quite_turn_out/'," datetime.datetime(2019, 4, 3, 17, 5, 3, 844938)",'2019-04-03',u'Discussion',u'1','investing',u'The Decade of Deleveraging Didn\u2019t Quite Turn Out That Way'," u'https://old.reddit.com/r/investing/comments/b8yjc7/the_decade_of_deleveraging_didnt_quite_turn_out/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 17, 5, 3, 844938) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-04-03 17:05:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b912hm/here_is_an_iron_condor_and_a_good_bet_amd_iron/'," datetime.datetime(2019, 4, 3, 17, 20, 2, 365958)",'2019-04-03',u'Options',u'3','wallstreetbets',u'Here is an Iron Condor and a good bet. AMD Iron Condor. My last post I promise. $27/$31'," u'https://old.reddit.com/r/wallstreetbets/comments/b912hm/here_is_an_iron_condor_and_a_good_bet_amd_iron/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 17, 20, 2, 365958) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b910v4/anyone_know_how_to_short_the_city_of_chicago/'," datetime.datetime(2019, 4, 3, 17, 20, 2, 367849)",'2019-04-03',u'Discussion',u'12','wallstreetbets',u'Anyone know how to short the city of Chicago?'," u'https://old.reddit.com/r/wallstreetbets/comments/b910v4/anyone_know_how_to_short_the_city_of_chicago/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 17, 20, 2, 367849) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
2019-04-03 17:20:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b8yruk/oil_prices_fall_as_us_crude_stockpiles_surge_by/'," datetime.datetime(2019, 4, 3, 17, 25, 3, 470037)",'2019-04-03',u'News',u'30','investing',u'Oil prices fall as US crude stockpiles surge by 7.2 million barrels'," u'https://old.reddit.com/r/investing/comments/b8yruk/oil_prices_fall_as_us_crude_stockpiles_surge_by/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 17, 25, 3, 470037) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b8yl1c/momentum_strategy_diversification_of_signals/'," datetime.datetime(2019, 4, 3, 17, 25, 3, 472155)",'2019-04-03',u'Discussion',u'0','investing',u'Momentum strategy: Diversification of signals?'," u'https://old.reddit.com/r/investing/comments/b8yl1c/momentum_strategy_diversification_of_signals/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 17, 25, 3, 472155) is not JSON serializable
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Daytrading/new/> from <GET https://old.reddit.com/r/daytrading/new/>
2019-04-03 17:25:03 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Daytrading/new/> from <GET https://old.reddit.com/r/daytrading/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-04-03 17:25:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b91miz/help_me_with_mnx_mini_nasdaq_500/'," datetime.datetime(2019, 4, 3, 17, 30, 2, 380931)",'2019-04-03',None,u'3','wallstreetbets',u'Help me with MNX (mini nasdaq 500)'," u'https://old.reddit.com/r/wallstreetbets/comments/b91miz/help_me_with_mnx_mini_nasdaq_500/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 17, 30, 2, 380931) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b8ysff/will_the_health_sector_still_be_considered/'," datetime.datetime(2019, 4, 3, 17, 30, 3, 629003)",'2019-04-03',u'Discussion',u'0','investing',u'Will the health sector still be considered defensive during the upcoming election season? I have doubts.'," u'https://old.reddit.com/r/investing/comments/b8ysff/will_the_health_sector_still_be_considered/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 17, 30, 3, 629003) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-04-03 17:30:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b8dfzq/tmobile_merger/'," datetime.datetime(2019, 4, 3, 17, 30, 5, 501578)",'2019-04-03',None,u'4','StockMarket',u'T-Mobile merger?'," u'https://old.reddit.com/r/StockMarket/comments/b8dfzq/tmobile_merger/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 17, 30, 5, 501578) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b91mm3/extremely_hot_analyst_has_a_4000_pt_for_tsla_long/'," datetime.datetime(2019, 4, 3, 17, 35, 2, 105733)",'2019-04-03',None,u'52','wallstreetbets',u'Extremely hot analyst has a $4000 PT for TSLA. Long TSLA'," u'https://www.youtube.com/watch?v=VP6MVdM80mU'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 17, 35, 2, 105733) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
2019-04-03 17:35:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
2019-04-03 17:35:02 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b92o59/carl_icahn_sold_lyft_stake_ahead_of_ipo/'," datetime.datetime(2019, 4, 3, 17, 45, 2, 180846)",'2019-04-03',u'Discussion',u'20','wallstreetbets',u'Carl Icahn sold Lyft stake ahead of IPO'," u'https://www.cnbc.com/2019/04/03/carl-icahn-sold-lyft-stake-ahead-of-ipo.html'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 17, 45, 2, 180846) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b92mob/everyone_on_this_sub_who_has_a_big_win_on_the_1/'," datetime.datetime(2019, 4, 3, 17, 45, 2, 187117)",'2019-04-03',u'Shitpost',u'62','wallstreetbets',"u'Everyone on this Sub who has a ""Big Win"" on the 1 Day Chart vs. All Time'"," u'https://old.reddit.com/r/wallstreetbets/comments/b92mob/everyone_on_this_sub_who_has_a_big_win_on_the_1/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 17, 45, 2, 187117) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b92li7/earnings_play_on_stz_anyone/'," datetime.datetime(2019, 4, 3, 17, 45, 2, 190307)",'2019-04-03',u'Options',u'4','wallstreetbets',u'Earnings play on $STZ anyone?'," u'https://old.reddit.com/r/wallstreetbets/comments/b92li7/earnings_play_on_stz_anyone/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 17, 45, 2, 190307) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b92i78/uniti_follow_up/'," datetime.datetime(2019, 4, 3, 17, 45, 2, 193671)",'2019-04-03',u'Gain',u'4','wallstreetbets',u'UNITI Follow Up'," u'https://old.reddit.com/r/wallstreetbets/comments/b92i78/uniti_follow_up/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 17, 45, 2, 193671) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b92hge/chinese_ipo_of_the_day_ruhn/'," datetime.datetime(2019, 4, 3, 17, 45, 2, 196946)",'2019-04-03',u'Stocks',u'2','wallstreetbets',u'Chinese ipo of the day RUHN'," u'https://old.reddit.com/r/wallstreetbets/comments/b92hge/chinese_ipo_of_the_day_ruhn/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 17, 45, 2, 196946) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b92h1r/amazon_amzn_reportedly_asked_advertisers_to/'," datetime.datetime(2019, 4, 3, 17, 45, 2, 200306)",'2019-04-03',u'DD',u'4','wallstreetbets',u'Amazon (AMZN) reportedly asked advertisers to pledge millions for a Roku (ROKU) competitor'," u'https://old.reddit.com/r/wallstreetbets/comments/b92h1r/amazon_amzn_reportedly_asked_advertisers_to/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 17, 45, 2, 200306) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b92a4s/yirv/'," datetime.datetime(2019, 4, 3, 17, 45, 2, 203380)",'2019-04-03',None,u'4','wallstreetbets',u'YIRV'," u'https://old.reddit.com/r/wallstreetbets/comments/b92a4s/yirv/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 17, 45, 2, 203380) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b921wt/all_i_care_about_is_carl_icahn_holding_his/'," datetime.datetime(2019, 4, 3, 17, 45, 2, 206454)",'2019-04-03',u'Shitpost',u'15','wallstreetbets',u'All I care about is Carl Icahn holding his martini like the fucking king he is'," u'https://www.wsj.com/articles/carl-icahn-sold-lyft-stake-prior-to-initial-public-offering-11554316497?emailToken=6aba984619ddbc50bc717c1f11b6a963ST+k069XMtXpNLgrTmHjWsCGTvfvATplpU7GHgNT8vz0KT0FCv8lDmz28ri9YU0f6Gp+7Qvd1qkZ5VDFV1ifplIUfRN5kz9oqZPR7ujpEPT90fwlKt0MRzjwrqIBWymA&reflink=article_copyURL_share'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 17, 45, 2, 206454) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b91vsp/calling_teva_anyone_else/'," datetime.datetime(2019, 4, 3, 17, 45, 2, 209542)",'2019-04-03',u'Options',u'4','wallstreetbets',"u'Calling TEVA, anyone else?'"," u'https://old.reddit.com/r/wallstreetbets/comments/b91vsp/calling_teva_anyone_else/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 17, 45, 2, 209542) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b91vid/some_thoughts_about_ray_dalios_big_debt_crises/'," datetime.datetime(2019, 4, 3, 17, 45, 2, 212639)",'2019-04-03',u'DD',u'22','wallstreetbets',"u'Some thoughts about Ray Dalio\'s ""Big Debt Crises""'"," u'https://old.reddit.com/r/wallstreetbets/comments/b91vid/some_thoughts_about_ray_dalios_big_debt_crises/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 17, 45, 2, 212639) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
2019-04-03 17:45:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b8zn5r/traders_are_already_mounting_a_455_million_bet/'," datetime.datetime(2019, 4, 3, 17, 45, 2, 458704)",'2019-04-03',None,u'1','stocks',"u'Traders are already mounting a $455 million bet against Lyft, now the most expensive US stock to short (LYFT)'"," u'https://old.reddit.com/r/stocks/comments/b8zn5r/traders_are_already_mounting_a_455_million_bet/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 17, 45, 2, 458704) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b8z2gu/is_it_just_me_or_is_mckesson_wildly_undervalued/'," datetime.datetime(2019, 4, 3, 17, 45, 2, 461740)",'2019-04-03',None,u'0','stocks',u'Is it just me or is McKesson wildly undervalued?'," u'https://old.reddit.com/r/stocks/comments/b8z2gu/is_it_just_me_or_is_mckesson_wildly_undervalued/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 17, 45, 2, 461740) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b8yse7/what_i_learned_this_winter/'," datetime.datetime(2019, 4, 3, 17, 45, 2, 463792)",'2019-04-03',u'Discussion',u'157','stocks',u'What I learned this winter...'," u'https://old.reddit.com/r/stocks/comments/b8yse7/what_i_learned_this_winter/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 17, 45, 2, 463792) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b8yh8u/what_do_you_think_of_amds_rally_today/'," datetime.datetime(2019, 4, 3, 17, 45, 2, 465408)",'2019-04-03',None,u'8','stocks',"u""What do you think of AMD's rally today?"""," u'https://old.reddit.com/r/stocks/comments/b8yh8u/what_do_you_think_of_amds_rally_today/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 17, 45, 2, 465408) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b8yaw6/750000_to_invest/'," datetime.datetime(2019, 4, 3, 17, 45, 2, 467446)",'2019-04-03',None,u'232','stocks',"u'$750,000 to invest'"," u'https://old.reddit.com/r/stocks/comments/b8yaw6/750000_to_invest/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 17, 45, 2, 467446) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b8xz08/whats_the_rule_on_brokers_being_able_to_lend_out/'," datetime.datetime(2019, 4, 3, 17, 45, 2, 470053)",'2019-04-03',None,u'0','stocks',"u""what's the rule on brokers being able to lend out your shares to shorts?"""," u'https://old.reddit.com/r/stocks/comments/b8xz08/whats_the_rule_on_brokers_being_able_to_lend_out/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 17, 45, 2, 470053) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b8xo76/why_is_sbux_so_high_atm_dont_they_grow_at_like_1/'," datetime.datetime(2019, 4, 3, 17, 45, 2, 472838)",'2019-04-03',None,u'0','stocks',"u""why is SBUX so high atm? Don't they grow at like 1 or 2% lol?"""," u'https://old.reddit.com/r/stocks/comments/b8xo76/why_is_sbux_so_high_atm_dont_they_grow_at_like_1/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 17, 45, 2, 472838) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b8xjak/todays_premarket_news_wednesday_april_3rd_2019/'," datetime.datetime(2019, 4, 3, 17, 45, 2, 475456)",'2019-04-03',u'News',u'13','stocks',"u""Today's Pre-Market News [Wednesday, April 3rd, 2019]"""," u'https://old.reddit.com/r/stocks/comments/b8xjak/todays_premarket_news_wednesday_april_3rd_2019/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 17, 45, 2, 475456) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b8x3s8/rstocks_daily_discussion_wednesday_apr_03_2019/'," datetime.datetime(2019, 4, 3, 17, 45, 2, 479464)",'2019-04-03',None,u'1','stocks',"u'r/Stocks Daily Discussion Wednesday - Apr 03, 2019'"," u'https://old.reddit.com/r/stocks/comments/b8x3s8/rstocks_daily_discussion_wednesday_apr_03_2019/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 17, 45, 2, 479464) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b8wzv5/global_stocks_rock_on_today_continuing_global/'," datetime.datetime(2019, 4, 3, 17, 45, 2, 483358)",'2019-04-03',u'News',u'18','stocks',u'Global stocks rock on today continuing global risk-on as positive data from China and Germany keep the party going'," u'https://old.reddit.com/r/stocks/comments/b8wzv5/global_stocks_rock_on_today_continuing_global/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 17, 45, 2, 483358) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b8ufao/how_much_is_matson_shipping_worth/'," datetime.datetime(2019, 4, 3, 17, 45, 2, 486709)",'2019-04-03',u'Question',u'0','stocks',u'How much is Matson Shipping worth?'," u'https://old.reddit.com/r/stocks/comments/b8ufao/how_much_is_matson_shipping_worth/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 17, 45, 2, 486709) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b8tu4v/what_is_your_daily_job/'," datetime.datetime(2019, 4, 3, 17, 45, 2, 489877)",'2019-04-03',u'Off-Topic',u'2','stocks',u'What is your daily job?'," u'https://old.reddit.com/r/stocks/comments/b8tu4v/what_is_your_daily_job/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 17, 45, 2, 489877) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b8slpx/play/'," datetime.datetime(2019, 4, 3, 17, 45, 2, 493090)",'2019-04-03',None,u'1','stocks',u'PLAY'," u'https://old.reddit.com/r/stocks/comments/b8slpx/play/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 17, 45, 2, 493090) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b8sle2/vicl_70_upside_value_from_current_price/'," datetime.datetime(2019, 4, 3, 17, 45, 2, 496239)",'2019-04-03',u'Discussion',u'1','stocks',u'VICL: 70% Upside Value from Current Price'," u'https://old.reddit.com/r/stocks/comments/b8sle2/vicl_70_upside_value_from_current_price/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 17, 45, 2, 496239) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b8rvvc/corbus_pharmaceuticals_good_buy/'," datetime.datetime(2019, 4, 3, 17, 45, 2, 499037)",'2019-04-03',None,u'3','stocks',u'Corbus Pharmaceuticals. Good buy?'," u'https://old.reddit.com/r/stocks/comments/b8rvvc/corbus_pharmaceuticals_good_buy/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 17, 45, 2, 499037) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b8ql8y/when_we_pay_taxes_on_our_sold_stocks_do_we_pay/'," datetime.datetime(2019, 4, 3, 17, 45, 2, 501362)",'2019-04-03',None,u'3','stocks',u'When we pay taxes on our sold stocks do we pay out of pocket or will our broker deduct it automatically after we file taxes?'," u'https://old.reddit.com/r/stocks/comments/b8ql8y/when_we_pay_taxes_on_our_sold_stocks_do_we_pay/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 17, 45, 2, 501362) is not JSON serializable
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
2019-04-03 17:45:02 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
2019-04-03 17:45:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b92io7/company_stock_in_your_employerbased_401k_good_or/'," datetime.datetime(2019, 4, 3, 17, 45, 3, 741091)",'2019-04-03',None,u'0','investing',"u'Company stock in your employer-based 401k: Good or bad idea, or totally dependent on the amount and the company?'"," u'https://old.reddit.com/r/investing/comments/b92io7/company_stock_in_your_employerbased_401k_good_or/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 17, 45, 3, 741091) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b92i0y/what_do_you_think_about_the_ishares_stoxx_global/'," datetime.datetime(2019, 4, 3, 17, 45, 3, 744201)",'2019-04-03',None,u'0','investing',u'What do you think about the iShares Stoxx global select dividend 100 etf?'," u'https://old.reddit.com/r/investing/comments/b92i0y/what_do_you_think_about_the_ishares_stoxx_global/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 17, 45, 3, 744201) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b923o0/how_does_td_ameritrades_margin_rates_work/'," datetime.datetime(2019, 4, 3, 17, 45, 3, 745965)",'2019-04-03',None,u'0','investing',u'How does TD Ameritrades Margin rates work?'," u'https://old.reddit.com/r/investing/comments/b923o0/how_does_td_ameritrades_margin_rates_work/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 17, 45, 3, 745965) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b91ysy/carl_icahn_sold_lyft_stake_prior_to_initial/'," datetime.datetime(2019, 4, 3, 17, 45, 3, 747625)",'2019-04-03',u'News',u'8','investing',u'Carl Icahn Sold Lyft Stake Prior to Initial Public Offering'," u'https://old.reddit.com/r/investing/comments/b91ysy/carl_icahn_sold_lyft_stake_prior_to_initial/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 17, 45, 3, 747625) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b91ttr/hedge_funds_are_loading_up_to_bet_against_lyft/'," datetime.datetime(2019, 4, 3, 17, 45, 3, 750120)",'2019-04-03',None,u'3','investing',u'Hedge Funds Are Loading up to Bet Against Lyft'," u'https://old.reddit.com/r/investing/comments/b91ttr/hedge_funds_are_loading_up_to_bet_against_lyft/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 17, 45, 3, 750120) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b91t3u/goldman_sachs_wants_to_pay_students_100000_to/'," datetime.datetime(2019, 4, 3, 17, 45, 3, 753996)",'2019-04-03',None,u'3','investing',"u'Goldman Sachs Wants to Pay Students $100,000 to Tackle Wall Street\u2019s Technology Challenges'"," u'https://old.reddit.com/r/investing/comments/b91t3u/goldman_sachs_wants_to_pay_students_100000_to/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 17, 45, 3, 753996) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b91sbv/is_rite_aid_a_buy/'," datetime.datetime(2019, 4, 3, 17, 45, 3, 767711)",'2019-04-03',None,u'0','investing',u'Is Rite Aid a buy?'," u'https://old.reddit.com/r/investing/comments/b91sbv/is_rite_aid_a_buy/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 17, 45, 3, 767711) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b91qgz/xom_will_oil_prices_rise_again/'," datetime.datetime(2019, 4, 3, 17, 45, 3, 769430)",'2019-04-03',None,u'0','investing',u'$XOM Will oil prices rise again?'," u'https://old.reddit.com/r/investing/comments/b91qgz/xom_will_oil_prices_rise_again/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 17, 45, 3, 769430) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b91pst/f_is_in_hold_pattern_until_headwinds_are_cleared/'," datetime.datetime(2019, 4, 3, 17, 45, 3, 771115)",'2019-04-03',None,u'0','investing',u'$F is in hold pattern until headwinds are cleared'," u'https://old.reddit.com/r/investing/comments/b91pst/f_is_in_hold_pattern_until_headwinds_are_cleared/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 17, 45, 3, 771115) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b91jc9/international_airlines_etf/'," datetime.datetime(2019, 4, 3, 17, 45, 3, 773392)",'2019-04-03',None,u'0','investing',u'International airlines etf'," u'https://old.reddit.com/r/investing/comments/b91jc9/international_airlines_etf/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 17, 45, 3, 773392) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b91bo0/fidelity_cma/'," datetime.datetime(2019, 4, 3, 17, 45, 3, 775415)",'2019-04-03',u'Help',u'0','investing',u'Fidelity CMA'," u'https://old.reddit.com/r/investing/comments/b91bo0/fidelity_cma/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 17, 45, 3, 775415) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b90zxr/cash_flow_of_chinas_11566_lgfvs_can_only_cover_40/'," datetime.datetime(2019, 4, 3, 17, 45, 3, 778502)",'2019-04-03',None,u'8','investing',"u'Cash flow of China\u2019s 11,566 LGFVs can only cover 40% of their repayments and interest this year, says CICC'"," u'https://old.reddit.com/r/investing/comments/b90zxr/cash_flow_of_chinas_11566_lgfvs_can_only_cover_40/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 17, 45, 3, 778502) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b90qa1/than_merrills_free_real_estate_investing_seminar/'," datetime.datetime(2019, 4, 3, 17, 45, 3, 781552)",'2019-04-03',None,u'0','investing',u'Than Merrill\u2019s free real estate investing seminar'," u'https://old.reddit.com/r/investing/comments/b90qa1/than_merrills_free_real_estate_investing_seminar/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 17, 45, 3, 781552) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b90cqb/do_we_know_what_caused_the_december_2018_crash/'," datetime.datetime(2019, 4, 3, 17, 45, 3, 786408)",'2019-04-03',None,u'0','investing',u'Do we know what caused the December 2018 crash?'," u'https://old.reddit.com/r/investing/comments/b90cqb/do_we_know_what_caused_the_december_2018_crash/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 17, 45, 3, 786408) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b9050j/which_stocks_will_benefit_most_from_a_chinaus_deal/'," datetime.datetime(2019, 4, 3, 17, 45, 3, 789666)",'2019-04-03',None,u'0','investing',u'Which stocks will benefit most from a China/US deal?'," u'https://old.reddit.com/r/investing/comments/b9050j/which_stocks_will_benefit_most_from_a_chinaus_deal/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 17, 45, 3, 789666) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b903i2/us_vs_canadian_etf/'," datetime.datetime(2019, 4, 3, 17, 45, 3, 792014)",'2019-04-03',None,u'3','investing',u'U.S vs Canadian ETF'," u'https://old.reddit.com/r/investing/comments/b903i2/us_vs_canadian_etf/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 17, 45, 3, 792014) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b902bw/money_left_etrade_but_has_yet_to_enter_my_bank/'," datetime.datetime(2019, 4, 3, 17, 45, 3, 793925)",'2019-04-03',u'Help',u'0','investing',"u'Money left etrade, but has yet to enter my bank account?'"," u'https://old.reddit.com/r/investing/comments/b902bw/money_left_etrade_but_has_yet_to_enter_my_bank/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 17, 45, 3, 793925) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b8zdd0/why_does_airbus_have_3_different_ticker_symbols/'," datetime.datetime(2019, 4, 3, 17, 45, 3, 795791)",'2019-04-03',None,u'2','investing',u'Why does Airbus have 3 different ticker symbols?'," u'https://old.reddit.com/r/investing/comments/b8zdd0/why_does_airbus_have_3_different_ticker_symbols/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 17, 45, 3, 795791) is not JSON serializable
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Daytrading/new/> from <GET https://old.reddit.com/r/daytrading/new/>
2019-04-03 17:45:03 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Daytrading/new/> from <GET https://old.reddit.com/r/daytrading/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-04-03 17:45:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/ValueInvesting/comments/ay2spj/question_about_when_to_buy_into_a_position/'," datetime.datetime(2019, 4, 3, 17, 45, 4, 800791)",'2019-04-03',None,u'1','ValueInvesting',u'Question about when to buy into a position.'," u'https://old.reddit.com/r/ValueInvesting/comments/ay2spj/question_about_when_to_buy_into_a_position/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 17, 45, 4, 800791) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/thewallstreet/new/> (referer: None)
2019-04-03 17:45:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/thewallstreet/new/> (referer: None)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/Trading/new/> (referer: None)
2019-04-03 17:45:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/Trading/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/thewallstreet/comments/b6u7zp/my_rough_attempt_at_a_fuzzy_logic_based/'," datetime.datetime(2019, 4, 3, 17, 45, 5, 176828)",'2019-04-03',u'thinkscript',u'12','thewallstreet',u'My rough attempt at a fuzzy logic based system/indicator in thinkscript'," u'https://old.reddit.com/r/thewallstreet/comments/b6u7zp/my_rough_attempt_at_a_fuzzy_logic_based/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 17, 45, 5, 176828) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/thewallstreet/comments/b6t8hw/daily_spx_tpos_03282019/'," datetime.datetime(2019, 4, 3, 17, 45, 5, 182637)",'2019-04-03',None,u'6','thewallstreet',"u""Daily SPX TPO's 03-28-2019"""," u'https://i.imgur.com/1RGgUap.png'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 17, 45, 5, 182637) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/thewallstreet/comments/b6scwq/deviations_poc_and_value_area_for_friday_march_29/'," datetime.datetime(2019, 4, 3, 17, 45, 5, 211996)",'2019-04-03',None,u'12','thewallstreet',"u'Deviations, POC, and Value Area for Friday, March 29, 2019'"," u'https://i.imgur.com/qGbMcK0.png'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 17, 45, 5, 211996) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/thewallstreet/comments/b6owip/nightly_trading_discussion_march_2829/'," datetime.datetime(2019, 4, 3, 17, 45, 5, 216092)",'2019-04-03',u'Daily',u'9','thewallstreet',u'Nightly Trading Discussion - (March 28/29)'," u'https://old.reddit.com/r/thewallstreet/comments/b6owip/nightly_trading_discussion_march_2829/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 17, 45, 5, 216092) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/thewallstreet/comments/b6njwc/post_market_discussion_march_28/'," datetime.datetime(2019, 4, 3, 17, 45, 5, 220834)",'2019-04-03',u'Daily',u'5','thewallstreet',u'Post Market Discussion - (March 28)'," u'https://old.reddit.com/r/thewallstreet/comments/b6njwc/post_market_discussion_march_28/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 17, 45, 5, 220834) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Trading/comments/b0wzp5/how_much_to_start_investing/'," datetime.datetime(2019, 4, 3, 17, 45, 5, 248670)",'2019-04-03',None,u'2','Trading',u'How much to start investing?'," u'https://old.reddit.com/r/Trading/comments/b0wzp5/how_much_to_start_investing/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 17, 45, 5, 248670) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/StockMarket/new/> (referer: None)
2019-04-03 17:45:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/StockMarket/new/> (referer: None)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/Stock_Picks/new/> (referer: None)
2019-04-03 17:45:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/Stock_Picks/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b8jym2/tmobile_stock_sprint_could_be_volatile_on_us/'," datetime.datetime(2019, 4, 3, 17, 45, 5, 691272)",'2019-04-03',None,u'2','StockMarket',"u'T-Mobile Stock, Sprint Could Be Volatile On U.S., State Merger Decisions'"," u'https://old.reddit.com/r/StockMarket/comments/b8jym2/tmobile_stock_sprint_could_be_volatile_on_us/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 17, 45, 5, 691272) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b8itge/bemg_reports_record_breaking_annual_revenues_of/'," datetime.datetime(2019, 4, 3, 17, 45, 5, 694601)",'2019-04-03',None,u'0','StockMarket',"u'BEMG Reports Record Breaking Annual Revenues of $1,057,837 for 2018 in Comparison to $962,997 for 2017 and $764,342 for 2016 and is Poised for Rapid Expansion in 2019'"," u'https://old.reddit.com/r/StockMarket/comments/b8itge/bemg_reports_record_breaking_annual_revenues_of/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 17, 45, 5, 694601) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b8iozt/morning_market_news_20190402/'," datetime.datetime(2019, 4, 3, 17, 45, 5, 697454)",'2019-04-03',None,u'10','StockMarket',u'Morning Market News (2019-04-02)'," u'https://old.reddit.com/r/StockMarket/comments/b8iozt/morning_market_news_20190402/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 17, 45, 5, 697454) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b8ikg7/is_technical_analysis_spam_or_real/'," datetime.datetime(2019, 4, 3, 17, 45, 5, 700398)",'2019-04-03',None,u'1','StockMarket',u'Is technical analysis spam or real???'," u'https://old.reddit.com/r/StockMarket/comments/b8ikg7/is_technical_analysis_spam_or_real/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 17, 45, 5, 700398) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b8hysw/how_does_the_stock_market_work/'," datetime.datetime(2019, 4, 3, 17, 45, 5, 705523)",'2019-04-03',None,u'0','StockMarket',u'How does the stock market work?'," u'https://old.reddit.com/r/StockMarket/comments/b8hysw/how_does_the_stock_market_work/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 17, 45, 5, 705523) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Stock_Picks/comments/awl9te/february_stock_pick_contest_over_march_stock_pick/'," datetime.datetime(2019, 4, 3, 17, 45, 5, 800778)",'2019-04-03',u'Contest',u'30','Stock_Picks',"u'February Stock Pick Contest over, March Stock Pick Contest now open!'"," u'https://old.reddit.com/r/Stock_Picks/comments/awl9te/february_stock_pick_contest_over_march_stock_pick/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 17, 45, 5, 800778) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/Daytrading/new/> (referer: None)
2019-04-03 17:45:07 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/Daytrading/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Daytrading/comments/b74qfy/anybody_can_explain_what_is_quantitative_trading/'," datetime.datetime(2019, 4, 3, 17, 45, 7, 570904)",'2019-04-03',None,u'4','Daytrading',u'Anybody can explain what is quantitative trading? And is there any good sites to learn about this and be able to provide a simple quantitative trading strategy within a month or so.'," u'https://old.reddit.com/r/Daytrading/comments/b74qfy/anybody_can_explain_what_is_quantitative_trading/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 17, 45, 7, 570904) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Daytrading/comments/b73tc7/looking_for_a_discord_group_chat_focused_on_price/'," datetime.datetime(2019, 4, 3, 17, 45, 7, 574337)",'2019-04-03',None,u'10','Daytrading',u'Looking for a Discord group chat focused on Price and Volume'," u'https://old.reddit.com/r/Daytrading/comments/b73tc7/looking_for_a_discord_group_chat_focused_on_price/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 17, 45, 7, 574337) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Daytrading/comments/b72s46/forex_competition_tiomarkets_guys_a_new_fx_firm/'," datetime.datetime(2019, 4, 3, 17, 45, 7, 582617)",'2019-04-03',None,u'1','Daytrading',"u'forex competition @TIOmarkets Guys a new FX firm TIOmarkets is opening its doors for pre-registration on April 1st. The company is giving away 100,000 free, top of the range headsets to anybody who signs up for pre-registration on April 1st.'"," u'https://www.youtube.com/watch?v=2E8ullQJzNg&feature=youtu.b'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 3, 17, 45, 7, 582617) is not JSON serializable
INFO:scrapy.core.engine:Closing spider (finished)
2019-04-03 17:45:07 [scrapy.core.engine] INFO: Closing spider (finished)
INFO:scrapy.statscollectors:Dumping Scrapy stats:
"
https://old.reddit.com/r/investing/comments/b9511w/researchers_have_found_the_records_of_millions_of/'," datetime.datetime(2019, 4, 4, 9, 20, 4, 26533)",'2019-04-04',None,u'1','investing',u'Researchers have found the records of millions of Facebook users in publicly-accessible places on Amazon\u2019s cloud servers.'," u'https://old.reddit.com/r/investing/comments/b9511w/researchers_have_found_the_records_of_millions_of/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 9, 20, 4, 26533) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-04-04 09:20:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b95kok/mega_backdoor_roth/'," datetime.datetime(2019, 4, 4, 9, 35, 4, 307079)",'2019-04-04',None,u'0','investing',u'Mega Backdoor Roth'," u'https://old.reddit.com/r/investing/comments/b95kok/mega_backdoor_roth/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 9, 35, 4, 307079) is not JSON serializable
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Daytrading/new/> from <GET https://old.reddit.com/r/daytrading/new/>
2019-04-04 09:35:04 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Daytrading/new/> from <GET https://old.reddit.com/r/daytrading/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-04-04 09:35:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b9a2z0/just_started_investing_and_yolod_everything_on/'," datetime.datetime(2019, 4, 4, 9, 45, 2, 112074)",'2019-04-04',u'Gain',u'8','wallstreetbets',u'Just started investing and yolod everything on weedstocks already.'," u'https://imgur.com/LvOadSe'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 9, 45, 2, 112074) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
2019-04-04 09:45:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b8zpnj/market_maker_executing_tiny_orders/'," datetime.datetime(2019, 4, 4, 9, 55, 2, 943783)",'2019-04-04',None,u'2','stocks',u'Market Maker executing tiny orders'," u'https://old.reddit.com/r/stocks/comments/b8zpnj/market_maker_executing_tiny_orders/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 9, 55, 2, 943783) is not JSON serializable
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
2019-04-04 09:55:03 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
2019-04-04 09:55:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b95rch/need_help_understanding_etrade_margin_calculator/'," datetime.datetime(2019, 4, 4, 9, 55, 4, 73264)",'2019-04-04',None,u'4','investing',u'Need help understanding etrade margin calculator'," u'https://old.reddit.com/r/investing/comments/b95rch/need_help_understanding_etrade_margin_calculator/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 9, 55, 4, 73264) is not JSON serializable
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Daytrading/new/> from <GET https://old.reddit.com/r/daytrading/new/>
2019-04-04 09:55:04 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Daytrading/new/> from <GET https://old.reddit.com/r/daytrading/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-04-04 09:55:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b8zsgp/day_trading_can_i_make_4_trades_in_each_account/'," datetime.datetime(2019, 4, 4, 10, 0, 3, 77302)",'2019-04-04',None,u'2','stocks',"u'Day trading: Can I make 4 trades in each account (Brokerage, Roth, IRA) totaling 12 trades, in a 5day period, without being considered a day trader? Additionally, if I want to make more than 4 trades, do I need $25k in each account ($75k+ total), or do I only need $25k totaling ALL my accounts?'"," u'https://old.reddit.com/r/stocks/comments/b8zsgp/day_trading_can_i_make_4_trades_in_each_account/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 0, 3, 77302) is not JSON serializable
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
2019-04-04 10:00:03 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
2019-04-04 10:00:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b8k1s7/bank_of_america_says_it_will_make_5_billion_in/'," datetime.datetime(2019, 4, 4, 10, 0, 6, 402053)",'2019-04-04',None,u'367','StockMarket',u'Bank of America says it will make $5 billion in mortgages to low and moderate income borrowers'," u'https://old.reddit.com/r/StockMarket/comments/b8k1s7/bank_of_america_says_it_will_make_5_billion_in/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 0, 6, 402053) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/Daytrading/new/> (referer: None)
2019-04-04 10:00:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/Daytrading/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b9a5ak/something_weird_with_futures/'," datetime.datetime(2019, 4, 4, 10, 5, 2, 674066)",'2019-04-04',u'Shitpost',u'4','wallstreetbets',u'Something weird with futures'," u'https://old.reddit.com/r/wallstreetbets/comments/b9a5ak/something_weird_with_futures/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 5, 2, 674066) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b8zteu/oesx_what_to_do_now/'," datetime.datetime(2019, 4, 4, 10, 5, 2, 680676)",'2019-04-04',None,u'2','stocks',u'OESX - what to do now?'," u'https://old.reddit.com/r/stocks/comments/b8zteu/oesx_what_to_do_now/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 5, 2, 680676) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
2019-04-04 10:05:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Daytrading/new/> from <GET https://old.reddit.com/r/daytrading/new/>
2019-04-04 10:05:03 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Daytrading/new/> from <GET https://old.reddit.com/r/daytrading/new/>
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b9aau8/i_did_it_boys_my_journey_out_of_the_pit/'," datetime.datetime(2019, 4, 4, 10, 10, 3, 887138)",'2019-04-04',None,u'89','wallstreetbets',u'I did it boys. My journey out of the pit.'," u'https://i.imgur.com/3avrz1A.jpg'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 10, 3, 887138) is not JSON serializable
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
2019-04-04 10:10:03 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b95u9r/creative_planning_wealth_management_and_moreany/'," datetime.datetime(2019, 4, 4, 10, 15, 3, 235468)",'2019-04-04',None,u'0','investing',u'Creative Planning - Wealth Management and More...any experience with this?'," u'https://old.reddit.com/r/investing/comments/b95u9r/creative_planning_wealth_management_and_moreany/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 15, 3, 235468) is not JSON serializable
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Daytrading/new/> from <GET https://old.reddit.com/r/daytrading/new/>
2019-04-04 10:15:03 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Daytrading/new/> from <GET https://old.reddit.com/r/daytrading/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-04-04 10:15:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b9au1y/of_course_carl_icahn_cashed_out_of_lyft_before/'," datetime.datetime(2019, 4, 4, 10, 20, 2, 622490)",'2019-04-04',u'Storytime',u'7','wallstreetbets',u'Of Course Carl Icahn Cashed Out Of Lyft Before The IPO Imploded'," u'https://dealbreaker.com/2019/04/carl-icahn-not-sweating-lyft-of-course?'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 20, 2, 622490) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b9am55/bbc_ethiopian_crash_jet_crew_followed_procedure/'," datetime.datetime(2019, 4, 4, 10, 20, 2, 624066)",'2019-04-04',u'Discussion',u'22','wallstreetbets',"u""BBC: Ethiopian crash jet crew 'followed procedure'"""," u'https://www.bbc.com/news/business-47812225'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 20, 2, 624066) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b9akow/dogecoin_up_big_after_elons_tweets_sec_securitys/'," datetime.datetime(2019, 4, 4, 10, 20, 2, 627129)",'2019-04-04',u'Shitpost',u'8','wallstreetbets',u'Dogecoin up big after Elon\u2019s Tweets SEC Security\u2019s Fraud Investigation incoming'," u'https://old.reddit.com/r/wallstreetbets/comments/b9akow/dogecoin_up_big_after_elons_tweets_sec_securitys/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 20, 2, 627129) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
2019-04-04 10:20:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
2019-04-04 10:20:02 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b905vg/ladybaybee_recommended_picks_for_20190403/'," datetime.datetime(2019, 4, 4, 10, 20, 2, 933070)",'2019-04-04',None,u'7','stocks',u'Ladybaybee recommended picks for 2019-04-03'," u'https://old.reddit.com/r/stocks/comments/b905vg/ladybaybee_recommended_picks_for_20190403/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 20, 2, 933070) is not JSON serializable
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
2019-04-04 10:20:03 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
2019-04-04 10:20:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b966is/iheartmedia_tunes_into_ipo_as_it_nears_bankruptcy/'," datetime.datetime(2019, 4, 4, 10, 20, 4, 1846)",'2019-04-04',None,u'3','investing',u'iHeartMedia tunes into IPO as it nears bankruptcy exit (REUTERS)'," u'https://old.reddit.com/r/investing/comments/b966is/iheartmedia_tunes_into_ipo_as_it_nears_bankruptcy/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 20, 4, 1846) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-04-04 10:20:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b9b5f5/looking_forward_to_loosing_all_my_money_with_you/'," datetime.datetime(2019, 4, 4, 10, 25, 2, 732973)",'2019-04-04',None,u'13','wallstreetbets',u'Looking forward to loosing all my money with you lads across the pond'," u'https://old.reddit.com/r/wallstreetbets/comments/b9b5f5/looking_forward_to_loosing_all_my_money_with_you/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 25, 2, 732973) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b9b0o4/this_picture_says_it_all/'," datetime.datetime(2019, 4, 4, 10, 25, 2, 735126)",'2019-04-04',u'YOLO',u'38','wallstreetbets',u'This picture says it all'," u'https://old.reddit.com/r/wallstreetbets/comments/b9b0o4/this_picture_says_it_all/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 25, 2, 735126) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b91vs9/lyft_and_the_doubters/'," datetime.datetime(2019, 4, 4, 10, 25, 2, 807534)",'2019-04-04',None,u'1','stocks',u'LYFT and the doubters'," u'https://old.reddit.com/r/stocks/comments/b91vs9/lyft_and_the_doubters/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 25, 2, 807534) is not JSON serializable
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
2019-04-04 10:25:02 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
2019-04-04 10:25:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b9682g/constellation_brands_q4_2019_earnings_call/'," datetime.datetime(2019, 4, 4, 10, 25, 3, 901216)",'2019-04-04',None,u'1','investing',u'Constellation Brands Q4 2019 Earnings Call Webcast 10:30 AM ET'," u'https://old.reddit.com/r/investing/comments/b9682g/constellation_brands_q4_2019_earnings_call/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 25, 3, 901216) is not JSON serializable
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Daytrading/new/> from <GET https://old.reddit.com/r/daytrading/new/>
2019-04-04 10:25:03 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Daytrading/new/> from <GET https://old.reddit.com/r/daytrading/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-04-04 10:25:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Daytrading/comments/b7apt7/day_trading_strategies/'," datetime.datetime(2019, 4, 4, 10, 25, 5, 892302)",'2019-04-04',None,u'0','Daytrading',u'Day trading strategies'," u'https://old.reddit.com/r/Daytrading/comments/b7apt7/day_trading_strategies/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 25, 5, 892302) is not JSON serializable
INFO:scrapy.core.engine:Closing spider (finished)
2019-04-04 10:25:05 [scrapy.core.engine] INFO: Closing spider (finished)
INFO:scrapy.statscollectors:Dumping Scrapy stats:
"
https://old.reddit.com/r/wallstreetbets/comments/b9bbat/omed/'," datetime.datetime(2019, 4, 4, 10, 30, 2, 344578)",'2019-04-04',None,u'1','wallstreetbets',u'OMED'," u'https://old.reddit.com/r/wallstreetbets/comments/b9bbat/omed/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 30, 2, 344578) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
2019-04-04 10:30:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b92c41/ssti_thoughts/'," datetime.datetime(2019, 4, 4, 10, 30, 2, 589200)",'2019-04-04',u'Ticker Question',u'1','stocks',u'$SSTI Thoughts?'," u'https://old.reddit.com/r/stocks/comments/b92c41/ssti_thoughts/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 30, 2, 589200) is not JSON serializable
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
2019-04-04 10:30:02 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
2019-04-04 10:30:02 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
2019-04-04 10:30:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b9bzzt/fb_longliterally_cant_go_tits_up_dd/'," datetime.datetime(2019, 4, 4, 10, 35, 5, 932373)",'2019-04-04',u'DD',u'2','wallstreetbets',"u""$FB Long-literally can't go tits up (DD)"""," u'https://old.reddit.com/r/wallstreetbets/comments/b9bzzt/fb_longliterally_cant_go_tits_up_dd/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 5, 932373) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b9byz9/gentle_reminder_if_tsla_doesnt_rally_11_by_2pm/'," datetime.datetime(2019, 4, 4, 10, 35, 5, 943679)",'2019-04-04',None,u'385','wallstreetbets',"u""Gentle reminder: If TSLA doesn't rally 11% by 2pm, u/fredd369 gets perm banned"""," u'https://old.reddit.com/r/wallstreetbets/comments/b9byz9/gentle_reminder_if_tsla_doesnt_rally_11_by_2pm/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 5, 943679) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b9byp3/tsla_today/'," datetime.datetime(2019, 4, 4, 10, 35, 5, 947098)",'2019-04-04',u'Shitpost',u'2','wallstreetbets',u'$TSLA today'," u'https://old.reddit.com/r/wallstreetbets/comments/b9byp3/tsla_today/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 5, 947098) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b9btpb/bear_gang_returns/'," datetime.datetime(2019, 4, 4, 10, 35, 5, 950342)",'2019-04-04',u'Shitpost',u'50','wallstreetbets',u'BEAR GANG RETURNS'," u'https://old.reddit.com/r/wallstreetbets/comments/b9btpb/bear_gang_returns/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 5, 950342) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b9bjt2/levis_levi_earnings_are_next_week_49/'," datetime.datetime(2019, 4, 4, 10, 35, 5, 953911)",'2019-04-04',None,u'1','wallstreetbets',"u""Levi's ($LEVI) earnings are next week, 4/9"""," u'https://old.reddit.com/r/wallstreetbets/comments/b9bjt2/levis_levi_earnings_are_next_week_49/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 5, 953911) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b9bdv6/when_people_tell_you_that_tsla_isnt_growing_show/'," datetime.datetime(2019, 4, 4, 10, 35, 5, 958603)",'2019-04-04',u'Fundamentals',u'2','wallstreetbets',"u""When people tell you that $TSLA isn't growing show them these delivery numbers"""," u'https://old.reddit.com/r/wallstreetbets/comments/b9bdv6/when_people_tell_you_that_tsla_isnt_growing_show/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 5, 958603) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b9bc46/look_at_the_amd_chart_which_way_is_it_going_to_go/'," datetime.datetime(2019, 4, 4, 10, 35, 5, 962209)",'2019-04-04',u'Discussion',u'0','wallstreetbets',u'Look at the AMD chart. Which way is it going to go? (IMGUR)(Strawpoll)'," u'https://old.reddit.com/r/wallstreetbets/comments/b9bc46/look_at_the_amd_chart_which_way_is_it_going_to_go/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 5, 962209) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/wallstreetbets/comments/b9bbe8/daily_discussion_thread_april_04_2019/'," datetime.datetime(2019, 4, 4, 10, 35, 5, 965566)",'2019-04-04',u'Daily Discussion',u'22','wallstreetbets',"u'Daily Discussion Thread - April 04, 2019'"," u'https://old.reddit.com/r/wallstreetbets/comments/b9bbe8/daily_discussion_thread_april_04_2019/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 5, 965566) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
2019-04-04 10:35:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stocks/new/> (referer: None)
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
2019-04-04 10:35:06 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Stock_Picks/new/> from <GET https://old.reddit.com/r/stock_picks/new/>
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b9bwa2/jp_morgan_says_tesla_just_undermined_elon_musks/'," datetime.datetime(2019, 4, 4, 10, 35, 6, 186991)",'2019-04-04',None,u'1','stocks',"u""JP Morgan says Tesla just undermined Elon Musk's defense against the SEC"""," u'https://old.reddit.com/r/stocks/comments/b9bwa2/jp_morgan_says_tesla_just_undermined_elon_musks/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 6, 186991) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b9bvhh/rstocks_daily_discussion_thursday_apr_04_2019/'," datetime.datetime(2019, 4, 4, 10, 35, 6, 191002)",'2019-04-04',None,u'1','stocks',"u'r/Stocks Daily Discussion Thursday - Apr 04, 2019'"," u'https://old.reddit.com/r/stocks/comments/b9bvhh/rstocks_daily_discussion_thursday_apr_04_2019/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 6, 191002) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b9bu39/target_raises_its_minimum_wage_to_13_an_hour_with/'," datetime.datetime(2019, 4, 4, 10, 35, 6, 199213)",'2019-04-04',None,u'122','stocks',"u'Target raises its minimum wage to $13 an hour, with goal of reaching $15 by end of 2020'"," u'https://old.reddit.com/r/stocks/comments/b9bu39/target_raises_its_minimum_wage_to_13_an_hour_with/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 6, 199213) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b9br6z/global_stocks_pause_seeking_additional_uschina/'," datetime.datetime(2019, 4, 4, 10, 35, 6, 202596)",'2019-04-04',u'News',u'2','stocks',"u'Global stocks pause, seeking additional US-China trade progress for next leg up, ahead of US jobs report tomorrow'"," u'https://old.reddit.com/r/stocks/comments/b9br6z/global_stocks_pause_seeking_additional_uschina/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 6, 202596) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b9bmk9/options_value_question/'," datetime.datetime(2019, 4, 4, 10, 35, 6, 207198)",'2019-04-04',None,u'1','stocks',u'Options Value Question'," u'https://old.reddit.com/r/stocks/comments/b9bmk9/options_value_question/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 6, 207198) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b9b6n5/what_does_the_rule_of_risking_1_of_your_account/'," datetime.datetime(2019, 4, 4, 10, 35, 6, 213300)",'2019-04-04',None,u'0','stocks',"u'What does the ""rule"" of risking 1% of your account per trade really mean?'"," u'https://old.reddit.com/r/stocks/comments/b9b6n5/what_does_the_rule_of_risking_1_of_your_account/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 6, 213300) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b98wp5/options_bidask_spread/'," datetime.datetime(2019, 4, 4, 10, 35, 6, 216791)",'2019-04-04',u'Question',u'1','stocks',u'Options bid/ask spread'," u'https://old.reddit.com/r/stocks/comments/b98wp5/options_bidask_spread/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 6, 216791) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b978ye/apple/'," datetime.datetime(2019, 4, 4, 10, 35, 6, 221624)",'2019-04-04',None,u'3','stocks',u'Apple'," u'https://old.reddit.com/r/stocks/comments/b978ye/apple/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 6, 221624) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b95qjm/should_i_sell_some_to_get_long_term_loss_to/'," datetime.datetime(2019, 4, 4, 10, 35, 6, 231453)",'2019-04-04',u'Advice',u'1','stocks',u'Should I sell some to get long term loss to offset my gains?'," u'https://old.reddit.com/r/stocks/comments/b95qjm/should_i_sell_some_to_get_long_term_loss_to/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 6, 231453) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b95q15/the_high_school_etf/'," datetime.datetime(2019, 4, 4, 10, 35, 6, 234773)",'2019-04-04',u'ETFs',u'5','stocks',u'The High School ETF:'," u'https://old.reddit.com/r/stocks/comments/b95q15/the_high_school_etf/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 6, 234773) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b95od7/help_need_guidance/'," datetime.datetime(2019, 4, 4, 10, 35, 6, 238517)",'2019-04-04',None,u'0','stocks',u'Help! Need guidance'," u'https://old.reddit.com/r/stocks/comments/b95od7/help_need_guidance/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 6, 238517) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b9591f/aig_warrant/'," datetime.datetime(2019, 4, 4, 10, 35, 6, 242139)",'2019-04-04',None,u'3','stocks',u'AIG Warrant'," u'https://old.reddit.com/r/stocks/comments/b9591f/aig_warrant/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 6, 242139) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b9558x/are_there_any_mobile_apps_that_will_be_able_to/'," datetime.datetime(2019, 4, 4, 10, 35, 6, 248723)",'2019-04-04',None,u'2','stocks',u'Are there any mobile apps that will be able to manage my assets?'," u'https://old.reddit.com/r/stocks/comments/b9558x/are_there_any_mobile_apps_that_will_be_able_to/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 6, 248723) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b94nph/what_prevents_traders_from_guaranteeing_profits/'," datetime.datetime(2019, 4, 4, 10, 35, 6, 251918)",'2019-04-04',u'Question',u'0','stocks',u'What prevents traders from guaranteeing profits using bracket orders?'," u'https://old.reddit.com/r/stocks/comments/b94nph/what_prevents_traders_from_guaranteeing_profits/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 6, 251918) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b94bn3/constellation_brands_stz/'," datetime.datetime(2019, 4, 4, 10, 35, 6, 255324)",'2019-04-04',None,u'10','stocks',u'Constellation Brands (STZ)'," u'https://old.reddit.com/r/stocks/comments/b94bn3/constellation_brands_stz/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 6, 255324) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b93amj/did_phun_register_those_restricted_shares_yet/'," datetime.datetime(2019, 4, 4, 10, 35, 6, 259095)",'2019-04-04',None,u'0','stocks',"u'Did PHUN register those restricted shares yet? They filed an S-1 Feb.05 for millions of warrants, down 50% today.'"," u'https://old.reddit.com/r/stocks/comments/b93amj/did_phun_register_those_restricted_shares_yet/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 6, 259095) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b933uk/how_is_everyone_feeling_about_aapl_and_gs/'," datetime.datetime(2019, 4, 4, 10, 35, 6, 264707)",'2019-04-04',u'Ticker Question',u'4','stocks',u'How is everyone feeling about AAPL and GS?'," u'https://old.reddit.com/r/stocks/comments/b933uk/how_is_everyone_feeling_about_aapl_and_gs/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 6, 264707) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stocks/comments/b92ot8/atvi/'," datetime.datetime(2019, 4, 4, 10, 35, 6, 267726)",'2019-04-04',None,u'5','stocks',u'ATVI'," u'https://old.reddit.com/r/stocks/comments/b92ot8/atvi/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 6, 267726) is not JSON serializable
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
2019-04-04 10:35:06 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/StockMarket/new/> from <GET https://old.reddit.com/r/stockmarket/new/>
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
2019-04-04 10:35:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/stockaday/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stockaday/comments/77enp3/diving_into_seaworld_seas/'," datetime.datetime(2019, 4, 4, 10, 35, 6, 945229)",'2019-04-04',None,u'24','stockaday',u'Diving into Seaworld? $SEAS'," u'https://old.reddit.com/r/stockaday/comments/77enp3/diving_into_seaworld_seas/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 6, 945229) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stockaday/comments/7723u6/is_2u_inc_for_you_twou/'," datetime.datetime(2019, 4, 4, 10, 35, 6, 948597)",'2019-04-04',None,u'11','stockaday',u'Is 2U Inc for you? $TWOU'," u'https://old.reddit.com/r/stockaday/comments/7723u6/is_2u_inc_for_you_twou/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 6, 948597) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stockaday/comments/76ri7w/has_versum_materials_turned_vsm/'," datetime.datetime(2019, 4, 4, 10, 35, 6, 951800)",'2019-04-04',None,u'8','stockaday',u'Has Versum Materials turned? $VSM'," u'https://old.reddit.com/r/stockaday/comments/76ri7w/has_versum_materials_turned_vsm/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 6, 951800) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stockaday/comments/7644ky/how_about_the_home_of_the_faangs_yes_nasdaq_inc/'," datetime.datetime(2019, 4, 4, 10, 35, 6, 956127)",'2019-04-04',None,u'11','stockaday',"u'How about the home of the FAANGs? Yes, Nasdaq Inc. $NDAQ'"," u'https://old.reddit.com/r/stockaday/comments/7644ky/how_about_the_home_of_the_faangs_yes_nasdaq_inc/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 6, 956127) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stockaday/comments/75poz4/i_wont_av_a_go_at_broadcom_avgo/'," datetime.datetime(2019, 4, 4, 10, 35, 6, 960665)",'2019-04-04',None,u'12','stockaday',"u""I won't 'av' a go at Broadcom. $AVGO"""," u'https://old.reddit.com/r/stockaday/comments/75poz4/i_wont_av_a_go_at_broadcom_avgo/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 6, 960665) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stockaday/comments/75gi3w/hows_stockaday_done_this_year_looking_back_to/'," datetime.datetime(2019, 4, 4, 10, 35, 6, 965359)",'2019-04-04',None,u'36','stockaday',"u""How's stockAday done this year? Looking back to early 2017"""," u'https://old.reddit.com/r/stockaday/comments/75gi3w/hows_stockaday_done_this_year_looking_back_to/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 6, 965359) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stockaday/comments/759irn/a_taste_for_dennys_denn/'," datetime.datetime(2019, 4, 4, 10, 35, 6, 968786)",'2019-04-04',None,u'14','stockaday',"u""A taste for Denny's? $DENN"""," u'https://old.reddit.com/r/stockaday/comments/759irn/a_taste_for_dennys_denn/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 6, 968786) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stockaday/comments/74munz/exel_to_the_moon_exelixis/'," datetime.datetime(2019, 4, 4, 10, 35, 6, 992394)",'2019-04-04',None,u'13','stockaday',u'$EXEL to the moon? Exelixis'," u'https://old.reddit.com/r/stockaday/comments/74munz/exel_to_the_moon_exelixis/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 6, 992394) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stockaday/comments/74ga04/how_about_a_voyage_of_discovery_disck/'," datetime.datetime(2019, 4, 4, 10, 35, 6, 995837)",'2019-04-04',None,u'5','stockaday',u'How about a voyage of Discovery? $DISCK'," u'https://old.reddit.com/r/stockaday/comments/74ga04/how_about_a_voyage_of_discovery_disck/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 6, 995837) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stockaday/comments/748vtt/can_amd_overcome_the_apathy_amd/'," datetime.datetime(2019, 4, 4, 10, 35, 6, 999435)",'2019-04-04',None,u'18','stockaday',u'Can AMD overcome the apathy? $AMD'," u'https://old.reddit.com/r/stockaday/comments/748vtt/can_amd_overcome_the_apathy_amd/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 6, 999435) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stockaday/comments/73z8tl/can_you_wait_for_ges_ceo_to_talk_ge/'," datetime.datetime(2019, 4, 4, 10, 35, 7, 3274)",'2019-04-04',None,u'15','stockaday',"u""Can you wait for GE's CEO to talk? $GE"""," u'https://old.reddit.com/r/stockaday/comments/73z8tl/can_you_wait_for_ges_ceo_to_talk_ge/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 7, 3274) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stockaday/comments/73so1r/is_purestorage_flash_gordon_or_a_flash_in_the_pan/'," datetime.datetime(2019, 4, 4, 10, 35, 7, 8085)",'2019-04-04',None,u'9','stockaday',u'Is PureStorage Flash Gordon or a flash in the pan? $PSTG'," u'https://old.reddit.com/r/stockaday/comments/73so1r/is_purestorage_flash_gordon_or_a_flash_in_the_pan/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 7, 8085) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stockaday/comments/738vm9/getting_exercised_by_planet_fitness_plnt/'," datetime.datetime(2019, 4, 4, 10, 35, 7, 11727)",'2019-04-04',None,u'11','stockaday',u'Getting exercised by Planet Fitness $PLNT'," u'https://old.reddit.com/r/stockaday/comments/738vm9/getting_exercised_by_planet_fitness_plnt/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 7, 11727) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stockaday/comments/7301o8/under_armoured_uaa/'," datetime.datetime(2019, 4, 4, 10, 35, 7, 15598)",'2019-04-04',None,u'16','stockaday',u'Under Armoured $UAA'," u'https://old.reddit.com/r/stockaday/comments/7301o8/under_armoured_uaa/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 7, 15598) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stockaday/comments/72tjp5/applied_optoelectronics_isnt_boring_aaoi/'," datetime.datetime(2019, 4, 4, 10, 35, 7, 20183)",'2019-04-04',None,u'8','stockaday',"u""Applied Optoelectronics isn't boring! $AAOI"""," u'https://old.reddit.com/r/stockaday/comments/72tjp5/applied_optoelectronics_isnt_boring_aaoi/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 7, 20183) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stockaday/comments/72k2f0/irobot_scifi_or_high_fly_irbt/'," datetime.datetime(2019, 4, 4, 10, 35, 7, 27259)",'2019-04-04',None,u'5','stockaday',u'iRobot... SciFi or high fly? $IRBT'," u'https://old.reddit.com/r/stockaday/comments/72k2f0/irobot_scifi_or_high_fly_irbt/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 7, 27259) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stockaday/comments/72c0gt/any_requests_for_stockaday_this_week_25th_sept/'," datetime.datetime(2019, 4, 4, 10, 35, 7, 31160)",'2019-04-04',None,u'9','stockaday',u'any requests for stockAday this week? (25th Sept)'," u'https://old.reddit.com/r/stockaday/comments/72c0gt/any_requests_for_stockaday_this_week_25th_sept/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 7, 31160) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stockaday/comments/72c065/can_gileads_kite_soar_gild/'," datetime.datetime(2019, 4, 4, 10, 35, 7, 34545)",'2019-04-04',None,u'2','stockaday',"u""Can Gilead's Kite soar? $GILD"""," u'https://old.reddit.com/r/stockaday/comments/72c065/can_gileads_kite_soar_gild/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 7, 34545) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stockaday/comments/6l2fws/applied_material_matters_amat/'," datetime.datetime(2019, 4, 4, 10, 35, 7, 40093)",'2019-04-04',None,u'9','stockaday',u'Applied Material matters! $AMAT'," u'https://old.reddit.com/r/stockaday/comments/6l2fws/applied_material_matters_amat/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 7, 40093) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stockaday/comments/6kylcn/requests_for_stockaday_week_of_3rd_july/'," datetime.datetime(2019, 4, 4, 10, 35, 7, 43788)",'2019-04-04',None,u'8','stockaday',u'Requests for stockAday? (Week of 3rd July)'," u'https://old.reddit.com/r/stockaday/comments/6kylcn/requests_for_stockaday_week_of_3rd_july/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 7, 43788) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stockaday/comments/6kg3gi/can_fedex_deliver_fdx/'," datetime.datetime(2019, 4, 4, 10, 35, 7, 47197)",'2019-04-04',None,u'17','stockaday',u'Can FedEx deliver? $FDX'," u'https://old.reddit.com/r/stockaday/comments/6kg3gi/can_fedex_deliver_fdx/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 7, 47197) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stockaday/comments/6k9nta/is_amex_platinum_axp/'," datetime.datetime(2019, 4, 4, 10, 35, 7, 50700)",'2019-04-04',None,u'17','stockaday',u'Is Amex Platinum? $AXP'," u'https://old.reddit.com/r/stockaday/comments/6k9nta/is_amex_platinum_axp/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 7, 50700) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stockaday/comments/6jv10n/is_magna_electric_mga/'," datetime.datetime(2019, 4, 4, 10, 35, 7, 58945)",'2019-04-04',None,u'13','stockaday',u'Is Magna electric? $MGA'," u'https://old.reddit.com/r/stockaday/comments/6jv10n/is_magna_electric_mga/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 7, 58945) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stockaday/comments/6jjusc/requests_for_stockaday_june_26th/'," datetime.datetime(2019, 4, 4, 10, 35, 7, 63078)",'2019-04-04',None,u'6','stockaday',u'Requests for stockAday? June 26th'," u'https://old.reddit.com/r/stockaday/comments/6jjusc/requests_for_stockaday_june_26th/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 7, 63078) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/stockaday/comments/6iwvh5/eating_at_the_cheesecake_factory_cake/'," datetime.datetime(2019, 4, 4, 10, 35, 7, 66933)",'2019-04-04',None,u'19','stockaday',u'Eating at the Cheesecake Factory. $CAKE'," u'https://old.reddit.com/r/stockaday/comments/6iwvh5/eating_at_the_cheesecake_factory_cake/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 7, 66933) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/investing/new/> (referer: None)
2019-04-04 10:35:07 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/investing/new/> (referer: None)
DEBUG:scrapy.downloadermiddlewares.redirect:Redirecting (301) to <GET https://old.reddit.com/r/Daytrading/new/> from <GET https://old.reddit.com/r/daytrading/new/>
2019-04-04 10:35:07 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://old.reddit.com/r/Daytrading/new/> from <GET https://old.reddit.com/r/daytrading/new/>
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b9byrt/daily_advice_thread_all_questions_about_your/'," datetime.datetime(2019, 4, 4, 10, 35, 7, 334661)",'2019-04-04',None,u'6','investing',u'Daily advice thread. All questions about your personal situation should be asked here'," u'https://old.reddit.com/r/investing/comments/b9byrt/daily_advice_thread_all_questions_about_your/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 7, 334661) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b9btgu/how_to_invest_in_mental_illness/'," datetime.datetime(2019, 4, 4, 10, 35, 7, 339643)",'2019-04-04',u'Discussion',u'0','investing',u'How to invest in mental illness'," u'https://old.reddit.com/r/investing/comments/b9btgu/how_to_invest_in_mental_illness/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 7, 339643) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b9a96u/ethiopia_report_calls_on_boeing_to_confirm_that/'," datetime.datetime(2019, 4, 4, 10, 35, 7, 344146)",'2019-04-04',u'News',u'2','investing',u'Ethiopia report calls on Boeing to confirm that control issues are fixed before 737 Max 8 flies again'," u'https://old.reddit.com/r/investing/comments/b9a96u/ethiopia_report_calls_on_boeing_to_confirm_that/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 7, 344146) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b99m7w/what_do_you_think_of_andreessen_horowitz/'," datetime.datetime(2019, 4, 4, 10, 35, 7, 347407)",'2019-04-04',u'News',u'0','investing',"u'What do you think of Andreessen Horowitz renouncing its status as a venture-capital firm to be able to invest in riskier assets, including raising 2 Billion (Yes 2B!!!) USD venture fund for crypto/blockchain investments.'"," u'https://old.reddit.com/r/investing/comments/b99m7w/what_do_you_think_of_andreessen_horowitz/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 7, 347407) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b99avo/best_hedge_to_everything_going_completely_south/'," datetime.datetime(2019, 4, 4, 10, 35, 7, 350788)",'2019-04-04',None,u'10','investing',u'Best hedge to everything going completely south.'," u'https://old.reddit.com/r/investing/comments/b99avo/best_hedge_to_everything_going_completely_south/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 7, 350788) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b98rkf/30year_mortgage_rates_tomorrow_and_friday/'," datetime.datetime(2019, 4, 4, 10, 35, 7, 355842)",'2019-04-04',None,u'0','investing',u'30-year mortgage rates tomorrow and Friday'," u'https://old.reddit.com/r/investing/comments/b98rkf/30year_mortgage_rates_tomorrow_and_friday/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 7, 355842) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b98ocb/best_way_to_get_long_exposure_to_ai_automation/'," datetime.datetime(2019, 4, 4, 10, 35, 7, 359656)",'2019-04-04',None,u'19','investing',u'Best way to get long exposure to AI / automation?'," u'https://old.reddit.com/r/investing/comments/b98ocb/best_way_to_get_long_exposure_to_ai_automation/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 7, 359656) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b98hhq/global_investors_havent_bought_enough_asian/'," datetime.datetime(2019, 4, 4, 10, 35, 7, 362969)",'2019-04-04',None,u'1','investing',"u'Global investors haven\u2019t bought enough Asian stocks, Fidelity says'"," u'https://old.reddit.com/r/investing/comments/b98hhq/global_investors_havent_bought_enough_asian/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 7, 362969) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b97nzg/russian_government_bonds/'," datetime.datetime(2019, 4, 4, 10, 35, 7, 366441)",'2019-04-04',None,u'0','investing',u'Russian Government Bonds'," u'https://old.reddit.com/r/investing/comments/b97nzg/russian_government_bonds/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 7, 366441) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b97jil/chap_chaparral_energy/'," datetime.datetime(2019, 4, 4, 10, 35, 7, 370347)",'2019-04-04',None,u'1','investing',u'(CHAP) Chaparral Energy'," u'https://old.reddit.com/r/investing/comments/b97jil/chap_chaparral_energy/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 7, 370347) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b97bjq/can_anyone_explain_why_vmrgx_suddenly_crashed_10/'," datetime.datetime(2019, 4, 4, 10, 35, 7, 377665)",'2019-04-04',None,u'0','investing',u'Can anyone explain why VMRGX suddenly crashed 10% today?'," u'https://old.reddit.com/r/investing/comments/b97bjq/can_anyone_explain_why_vmrgx_suddenly_crashed_10/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 7, 377665) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b97b4i/buy_microsoft_msft_not_faang_stocks_for_stable/'," datetime.datetime(2019, 4, 4, 10, 35, 7, 381275)",'2019-04-04',None,u'0','investing',"u'Buy Microsoft (MSFT), Not FAANG Stocks for Stable Growth & Income - April 3, 2019 - Zacks.com'"," u'https://old.reddit.com/r/investing/comments/b97b4i/buy_microsoft_msft_not_faang_stocks_for_stable/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 7, 381275) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b979qg/negative_index/'," datetime.datetime(2019, 4, 4, 10, 35, 7, 384431)",'2019-04-04',None,u'1','investing',u'Negative Index?'," u'https://old.reddit.com/r/investing/comments/b979qg/negative_index/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 7, 384431) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b971v9/vanguard_admiral_shares_vs_etf_index_funds/'," datetime.datetime(2019, 4, 4, 10, 35, 7, 389900)",'2019-04-04',None,u'6','investing',u'Vanguard Admiral Shares vs ETF index funds'," u'https://old.reddit.com/r/investing/comments/b971v9/vanguard_admiral_shares_vs_etf_index_funds/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 7, 389900) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b96z2q/does_robinhood_not_pay_dividends_for_kenon/'," datetime.datetime(2019, 4, 4, 10, 35, 7, 393629)",'2019-04-04',None,u'0','investing',u'Does robinhood not pay dividends for kenon holdings?'," u'https://old.reddit.com/r/investing/comments/b96z2q/does_robinhood_not_pay_dividends_for_kenon/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 7, 393629) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/investing/comments/b96c5m/tesla_q1_2019_vehicle_production_and_deliveries/'," datetime.datetime(2019, 4, 4, 10, 35, 7, 396902)",'2019-04-04',None,u'45','investing',u'Tesla Q1 2019 Vehicle Production and Deliveries'," u'https://old.reddit.com/r/investing/comments/b96c5m/tesla_q1_2019_vehicle_production_and_deliveries/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 7, 396902) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
2019-04-04 10:35:07 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/ValueInvesting/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/ValueInvesting/comments/b94ocr/what_would_be_key_items_to_research_and_note_for/'," datetime.datetime(2019, 4, 4, 10, 35, 7, 904802)",'2019-04-04',None,u'3','ValueInvesting',u'What would be key items to research and note for companies (im making google docs for information on different businesses )'," u'https://old.reddit.com/r/ValueInvesting/comments/b94ocr/what_would_be_key_items_to_research_and_note_for/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 7, 904802) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/ValueInvesting/comments/b7s7xs/list_of_all_durable_competitive_advantages/'," datetime.datetime(2019, 4, 4, 10, 35, 7, 909156)",'2019-04-04',None,u'12','ValueInvesting',u'List of all durable competitive advantages'," u'https://old.reddit.com/r/ValueInvesting/comments/b7s7xs/list_of_all_durable_competitive_advantages/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 7, 909156) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/ValueInvesting/comments/b5ik75/lets_talk_about_apple/'," datetime.datetime(2019, 4, 4, 10, 35, 7, 912753)",'2019-04-04',None,u'3','ValueInvesting',"u""Let's talk about Apple"""," u'https://old.reddit.com/r/ValueInvesting/comments/b5ik75/lets_talk_about_apple/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 7, 912753) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/ValueInvesting/comments/b5gxjr/this_is_a_really_helpful_site_in_terms_of/'," datetime.datetime(2019, 4, 4, 10, 35, 7, 915844)",'2019-04-04',None,u'0','ValueInvesting',"u""This is a really helpful site in terms of understanding a business's financial under the scope of a value investor."""," u'https://www.ruleoneinvesting.com/toolbox/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 7, 915844) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/ValueInvesting/comments/b4zzs1/doing_better_research/'," datetime.datetime(2019, 4, 4, 10, 35, 7, 921046)",'2019-04-04',None,u'8','ValueInvesting',u'Doing Better Research'," u'https://old.reddit.com/r/ValueInvesting/comments/b4zzs1/doing_better_research/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 7, 921046) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/ValueInvesting/comments/b4ymdr/adobe_inc_buy_in_the_next_recession/'," datetime.datetime(2019, 4, 4, 10, 35, 7, 924861)",'2019-04-04',u'stocks',u'1','ValueInvesting',u'Adobe Inc... Buy in the next recession'," u'https://old.reddit.com/r/ValueInvesting/comments/b4ymdr/adobe_inc_buy_in_the_next_recession/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 7, 924861) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/ValueInvesting/comments/b40x2q/hidden_gem_of_the_otc_or_dog_with_fleas/'," datetime.datetime(2019, 4, 4, 10, 35, 7, 928555)",'2019-04-04',None,u'0','ValueInvesting',u'Hidden Gem of the OTC or Dog With Fleas?'," u'https://old.reddit.com/r/ValueInvesting/comments/b40x2q/hidden_gem_of_the_otc_or_dog_with_fleas/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 7, 928555) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/ValueInvesting/comments/b3z2ub/grahams_growth_stock_method/'," datetime.datetime(2019, 4, 4, 10, 35, 7, 934440)",'2019-04-04',None,u'2','ValueInvesting',u'Graham\u2019s growth stock method'," u'https://old.reddit.com/r/ValueInvesting/comments/b3z2ub/grahams_growth_stock_method/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 7, 934440) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/ValueInvesting/comments/b3uoyw/my_buddy_left_the_family_business_started_a_value/'," datetime.datetime(2019, 4, 4, 10, 35, 7, 939617)",'2019-04-04',None,u'15','ValueInvesting',"u""My buddy left the family business, started a Value Investing Podcast. Here's some stock analysis of Newell Brands! NWL"""," u'https://youtu.be/4MzRcWtWlHw'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 7, 939617) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/ValueInvesting/comments/b3sw40/reading_about_john_maynard_keynes_as_a_value/'," datetime.datetime(2019, 4, 4, 10, 35, 7, 944482)",'2019-04-04',None,u'4','ValueInvesting',u'Reading About John Maynard Keynes as a value investor'," u'https://old.reddit.com/r/ValueInvesting/comments/b3sw40/reading_about_john_maynard_keynes_as_a_value/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 7, 944482) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/ValueInvesting/comments/b3p2qb/using_per_vs_evebitda/'," datetime.datetime(2019, 4, 4, 10, 35, 7, 948150)",'2019-04-04',None,u'2','ValueInvesting',u'Using PER vs. EV/EBITDA'," u'https://old.reddit.com/r/ValueInvesting/comments/b3p2qb/using_per_vs_evebitda/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 7, 948150) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/ValueInvesting/comments/b31gv6/tips_on_focusing/'," datetime.datetime(2019, 4, 4, 10, 35, 7, 953943)",'2019-04-04',None,u'3','ValueInvesting',u'Tips on focusing'," u'https://old.reddit.com/r/ValueInvesting/comments/b31gv6/tips_on_focusing/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 7, 953943) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/ValueInvesting/comments/b2dzqg/interpretation_of_financial_statements/'," datetime.datetime(2019, 4, 4, 10, 35, 7, 957173)",'2019-04-04',None,u'6','ValueInvesting',u'Interpretation of Financial Statements'," u'https://old.reddit.com/r/ValueInvesting/comments/b2dzqg/interpretation_of_financial_statements/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 7, 957173) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/ValueInvesting/comments/b2cbjv/your_opinion_on_phil_town_rule1_investing/'," datetime.datetime(2019, 4, 4, 10, 35, 7, 965476)",'2019-04-04',None,u'10','ValueInvesting',u'Your opinion on Phil Town rule1 investing.'," u'https://old.reddit.com/r/ValueInvesting/comments/b2cbjv/your_opinion_on_phil_town_rule1_investing/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 7, 965476) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/ValueInvesting/comments/b27gkq/researching_stocks/'," datetime.datetime(2019, 4, 4, 10, 35, 7, 971693)",'2019-04-04',None,u'0','ValueInvesting',u'Researching Stocks'," u'https://www.reddit.com/r/StockMarket/comments/b27byu/researching_stocks/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 7, 971693) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/ValueInvesting/comments/b1tn2i/how_ge_built_up_and_wrote_down_22_billion_in/'," datetime.datetime(2019, 4, 4, 10, 35, 7, 974481)",'2019-04-04',None,u'2','ValueInvesting',u'How GE built up and wrote down 22 billion in assets ?'," u'https://old.reddit.com/r/ValueInvesting/comments/b1tn2i/how_ge_built_up_and_wrote_down_22_billion_in/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 7, 974481) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/ValueInvesting/comments/b1sqzo/the_most_important_quality_for_an_investment/'," datetime.datetime(2019, 4, 4, 10, 35, 7, 978689)",'2019-04-04',u'buffett',u'24','ValueInvesting',u'The Most Important Quality for an Investment Manager by Warren Buffet'," u'https://youtu.be/hr16XZ-g9SU'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 7, 978689) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/ValueInvesting/comments/b1oys0/looking_for_my_charlie_munger/'," datetime.datetime(2019, 4, 4, 10, 35, 7, 982593)",'2019-04-04',None,u'4','ValueInvesting',u'Looking for my Charlie Munger'," u'https://old.reddit.com/r/ValueInvesting/comments/b1oys0/looking_for_my_charlie_munger/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 7, 982593) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/ValueInvesting/comments/b1kg52/case_for_investing_in_novation_companies_inc/'," datetime.datetime(2019, 4, 4, 10, 35, 7, 988491)",'2019-04-04',None,u'0','ValueInvesting',u'Case for investing in Novation Companies Inc. OTCBB:NOVC'," u'https://old.reddit.com/r/ValueInvesting/comments/b1kg52/case_for_investing_in_novation_companies_inc/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 7, 988491) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/ValueInvesting/comments/b0qgay/investor_howard_marks_discusses_the_market_cycle/'," datetime.datetime(2019, 4, 4, 10, 35, 7, 997643)",'2019-04-04',None,u'9','ValueInvesting',u'Investor Howard Marks discusses the market cycle and how to master it'," u'https://old.reddit.com/r/ValueInvesting/comments/b0qgay/investor_howard_marks_discusses_the_market_cycle/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 7, 997643) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/ValueInvesting/comments/b0osae/brookfield_ceo_bruce_flatt_on_value_investing/'," datetime.datetime(2019, 4, 4, 10, 35, 8, 3603)",'2019-04-04',None,u'7','ValueInvesting',"u'Brookfield CEO Bruce Flatt on Value Investing: ""Durable Principles for Real Asset Investing"" (video 53min)'"," u'https://www.youtube.com/watch?v=vmt1Li1Rnes'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 8, 3603) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/ValueInvesting/comments/azemm2/long_term_investment_ideas/'," datetime.datetime(2019, 4, 4, 10, 35, 8, 7033)",'2019-04-04',None,u'15','ValueInvesting',u'Long term investment ideas'," u'https://old.reddit.com/r/ValueInvesting/comments/azemm2/long_term_investment_ideas/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 8, 7033) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/ValueInvesting/comments/az8lkg/roic_and_stock_prices/'," datetime.datetime(2019, 4, 4, 10, 35, 8, 10775)",'2019-04-04',None,u'2','ValueInvesting',u'ROIC and stock prices'," u'https://old.reddit.com/r/ValueInvesting/comments/az8lkg/roic_and_stock_prices/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 8, 10775) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/ValueInvesting/comments/az6bcr/value_stock_for_the_ages_novation_companies/'," datetime.datetime(2019, 4, 4, 10, 35, 8, 14192)",'2019-04-04',None,u'0','ValueInvesting',u'Value Stock for the ages Novation Companies OTCC;NOVC 4 cts backed by Fortress & Mass Mutual Barings'," u'https://old.reddit.com/r/ValueInvesting/comments/az6bcr/value_stock_for_the_ages_novation_companies/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 8, 14192) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/ValueInvesting/comments/ay55pe/choosing_stocks_within_a_market_cap_range/'," datetime.datetime(2019, 4, 4, 10, 35, 8, 19313)",'2019-04-04',None,u'5','ValueInvesting',u'Choosing stocks within a market cap range'," u'https://old.reddit.com/r/ValueInvesting/comments/ay55pe/choosing_stocks_within_a_market_cap_range/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 8, 19313) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/thewallstreet/new/> (referer: None)
2019-04-04 10:35:08 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/thewallstreet/new/> (referer: None)
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/Trading/new/> (referer: None)
2019-04-04 10:35:08 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/Trading/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/thewallstreet/comments/b9bjvh/daily_discussion_april_04/'," datetime.datetime(2019, 4, 4, 10, 35, 8, 283429)",'2019-04-04',u'Daily',u'5','thewallstreet',u'Daily Discussion - (April 04)'," u'https://old.reddit.com/r/thewallstreet/comments/b9bjvh/daily_discussion_april_04/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 8, 283429) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/thewallstreet/comments/b98jfu/daily_spx_tpos_04032019/'," datetime.datetime(2019, 4, 4, 10, 35, 8, 306177)",'2019-04-04',None,u'12','thewallstreet',"u""Daily SPX TPO's 04-03-2019"""," u'https://i.imgur.com/d2ts1xU.png'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 8, 306177) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/thewallstreet/comments/b97x98/issues_with_rtdtosrtd_in_excel/'," datetime.datetime(2019, 4, 4, 10, 35, 8, 315796)",'2019-04-04',u'Question',u'4','thewallstreet',"u'Issues with =RTD(""tos.rtd"".....) in excel'"," u'https://old.reddit.com/r/thewallstreet/comments/b97x98/issues_with_rtdtosrtd_in_excel/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 8, 315796) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/thewallstreet/comments/b97gw7/deviations_poc_and_value_area_for_thursday_april/'," datetime.datetime(2019, 4, 4, 10, 35, 8, 320240)",'2019-04-04',None,u'12','thewallstreet',"u'Deviations, POC, and Value Area for Thursday, April 4, 2019'"," u'https://i.imgur.com/dUcJdOc.png'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 8, 320240) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/thewallstreet/comments/b94a5q/nightly_trading_discussion_april_0304/'," datetime.datetime(2019, 4, 4, 10, 35, 8, 331908)",'2019-04-04',u'Daily',u'13','thewallstreet',u'Nightly Trading Discussion - (April 03/04)'," u'https://old.reddit.com/r/thewallstreet/comments/b94a5q/nightly_trading_discussion_april_0304/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 8, 331908) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/thewallstreet/comments/b92wwl/post_market_discussion_april_03/'," datetime.datetime(2019, 4, 4, 10, 35, 8, 335867)",'2019-04-04',u'Daily',u'7','thewallstreet',u'Post Market Discussion - (April 03)'," u'https://old.reddit.com/r/thewallstreet/comments/b92wwl/post_market_discussion_april_03/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 8, 335867) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/thewallstreet/comments/b8wspp/daily_discussion_april_03/'," datetime.datetime(2019, 4, 4, 10, 35, 8, 339503)",'2019-04-04',u'Daily',u'12','thewallstreet',u'Daily Discussion - (April 03)'," u'https://old.reddit.com/r/thewallstreet/comments/b8wspp/daily_discussion_april_03/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 8, 339503) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/thewallstreet/comments/b8tspr/daily_spx_tpos_04022019/'," datetime.datetime(2019, 4, 4, 10, 35, 8, 350024)",'2019-04-04',None,u'7','thewallstreet',"u""Daily SPX TPO's 04-02-2019"""," u'https://i.imgur.com/xJg31AU.png'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 8, 350024) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/thewallstreet/comments/b8t3ee/deviations_poc_and_value_area_for_wednesday_april/'," datetime.datetime(2019, 4, 4, 10, 35, 8, 355910)",'2019-04-04',None,u'13','thewallstreet',"u'Deviations, POC, and Value Area for Wednesday, April 3, 2019'"," u'https://i.imgur.com/PJmVe7E.png'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 8, 355910) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/thewallstreet/comments/b8pitc/nightly_trading_discussion_april_0203/'," datetime.datetime(2019, 4, 4, 10, 35, 8, 371246)",'2019-04-04',u'Daily',u'13','thewallstreet',u'Nightly Trading Discussion - (April 02/03)'," u'https://old.reddit.com/r/thewallstreet/comments/b8pitc/nightly_trading_discussion_april_0203/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 8, 371246) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Trading/comments/b9aanp/fractals/'," datetime.datetime(2019, 4, 4, 10, 35, 8, 426784)",'2019-04-04',None,u'0','Trading',u'Fractals'," u'https://old.reddit.com/r/Trading/comments/b9aanp/fractals/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 8, 426784) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/thewallstreet/comments/b8o4tx/post_market_discussion_april_02/'," datetime.datetime(2019, 4, 4, 10, 35, 8, 504627)",'2019-04-04',u'Daily',u'16','thewallstreet',u'Post Market Discussion - (April 02)'," u'https://old.reddit.com/r/thewallstreet/comments/b8o4tx/post_market_discussion_april_02/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 8, 504627) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Trading/comments/b8om36/highly_customizable_lightweight_trading_software/'," datetime.datetime(2019, 4, 4, 10, 35, 8, 522727)",'2019-04-04',None,u'2','Trading',u'highly customizable lightweight trading software'," u'https://old.reddit.com/r/Trading/comments/b8om36/highly_customizable_lightweight_trading_software/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 8, 522727) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/thewallstreet/comments/b8i1cj/daily_discussion_april_02/'," datetime.datetime(2019, 4, 4, 10, 35, 8, 584398)",'2019-04-04',u'Daily',u'12','thewallstreet',u'Daily Discussion - (April 02)'," u'https://old.reddit.com/r/thewallstreet/comments/b8i1cj/daily_discussion_april_02/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 8, 584398) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Trading/comments/b8jcmf/lyft_shares_sink_as_the_stock_market_is_just_a/'," datetime.datetime(2019, 4, 4, 10, 35, 8, 660931)",'2019-04-04',None,u'1','Trading',u'Lyft shares sink as the stock market is just a few percent from all time...'," u'https://old.reddit.com/r/Trading/comments/b8jcmf/lyft_shares_sink_as_the_stock_market_is_just_a/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 8, 660931) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Trading/comments/b8bj1v/swing_trading_challenges/'," datetime.datetime(2019, 4, 4, 10, 35, 8, 667892)",'2019-04-04',None,u'1','Trading',u'Swing Trading Challenges'," u'https://old.reddit.com/r/Trading/comments/b8bj1v/swing_trading_challenges/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 8, 667892) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/thewallstreet/comments/b8eqmg/daily_spx_tpos_04012019/'," datetime.datetime(2019, 4, 4, 10, 35, 8, 723464)",'2019-04-04',None,u'7','thewallstreet',"u""Daily SPX TPO's 04-01-2019"""," u'https://i.imgur.com/YGX8V1f.png'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 8, 723464) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Trading/comments/b8bbub/bps_vs_pips/'," datetime.datetime(2019, 4, 4, 10, 35, 8, 735534)",'2019-04-04',None,u'2','Trading',u'Bps vs Pips'," u'https://old.reddit.com/r/Trading/comments/b8bbub/bps_vs_pips/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 8, 735534) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/thewallstreet/comments/b8e3c6/deviations_poc_and_value_area_for_tuesday_april_2/'," datetime.datetime(2019, 4, 4, 10, 35, 8, 786615)",'2019-04-04',None,u'25','thewallstreet',"u'Deviations, POC, and Value Area for Tuesday, April 2, 2019'"," u'https://i.imgur.com/2PFImZZ.png'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 8, 786615) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/thewallstreet/comments/b8avz8/technical_analysis_of_stocks_and_commodities/'," datetime.datetime(2019, 4, 4, 10, 35, 8, 792017)",'2019-04-04',u'Resources',u'9','thewallstreet',u'Technical Analysis of Stocks and Commodities - April Added'," u'https://drive.google.com/open?id=1MXblFcwseuPVy_64R5sEhgD44BamoeiM'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 8, 792017) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Trading/comments/b7sf82/looking_for_a_papertrading_platform/'," datetime.datetime(2019, 4, 4, 10, 35, 8, 802006)",'2019-04-04',None,u'2','Trading',u'Looking for a paper-trading platform'," u'https://old.reddit.com/r/Trading/comments/b7sf82/looking_for_a_papertrading_platform/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 8, 802006) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/Stock_Picks/new/> (referer: None)
2019-04-04 10:35:08 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/Stock_Picks/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/thewallstreet/comments/b89sp9/nightly_trading_discussion_april_0102/'," datetime.datetime(2019, 4, 4, 10, 35, 8, 826303)",'2019-04-04',u'Daily',u'14','thewallstreet',u'Nightly Trading Discussion - (April 01/02)'," u'https://old.reddit.com/r/thewallstreet/comments/b89sp9/nightly_trading_discussion_april_0102/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 8, 826303) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/thewallstreet/comments/b87z6b/post_market_discussion_april_01/'," datetime.datetime(2019, 4, 4, 10, 35, 8, 833144)",'2019-04-04',u'Daily',u'10','thewallstreet',u'Post Market Discussion - (April 01)'," u'https://old.reddit.com/r/thewallstreet/comments/b87z6b/post_market_discussion_april_01/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 8, 833144) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Trading/comments/b7d084/identify_the_subtle_direction_of_compression/'," datetime.datetime(2019, 4, 4, 10, 35, 8, 837585)",'2019-04-04',None,u'1','Trading',u'Identify the subtle direction of compression'," u'https://old.reddit.com/r/Trading/comments/b7d084/identify_the_subtle_direction_of_compression/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 8, 837585) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Trading/comments/b7c0qi/can_also_the_mm_market_makers_lose/'," datetime.datetime(2019, 4, 4, 10, 35, 8, 847624)",'2019-04-04',None,u'0','Trading',u'Can also the MM (market makers) lose?'," u'https://old.reddit.com/r/Trading/comments/b7c0qi/can_also_the_mm_market_makers_lose/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 8, 847624) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/thewallstreet/comments/b81491/daily_discussion_april_01/'," datetime.datetime(2019, 4, 4, 10, 35, 8, 851639)",'2019-04-04',u'Daily',u'13','thewallstreet',u'Daily Discussion - (April 01)'," u'https://old.reddit.com/r/thewallstreet/comments/b81491/daily_discussion_april_01/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 8, 851639) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/thewallstreet/comments/b7ywi3/weekly_question_thread_week_13_2019/'," datetime.datetime(2019, 4, 4, 10, 35, 8, 855219)",'2019-04-04',None,u'6','thewallstreet',"u'Weekly Question Thread - Week 13, 2019'"," u'https://old.reddit.com/r/thewallstreet/comments/b7ywi3/weekly_question_thread_week_13_2019/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 8, 855219) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Trading/comments/b732ul/new_fx_firm_tiomarkets_is_opening_its_doors_for/'," datetime.datetime(2019, 4, 4, 10, 35, 8, 885458)",'2019-04-04',None,u'0','Trading',u'New FX firm TIOmarkets is opening its doors for pre-registration on April 1st.'," u'https://old.reddit.com/r/Trading/comments/b732ul/new_fx_firm_tiomarkets_is_opening_its_doors_for/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 8, 885458) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Trading/comments/b6cmf0/how_to_trade_on_szse/'," datetime.datetime(2019, 4, 4, 10, 35, 8, 889215)",'2019-04-04',None,u'3','Trading',u'How to trade on SZSE'," u'https://old.reddit.com/r/Trading/comments/b6cmf0/how_to_trade_on_szse/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 8, 889215) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Trading/comments/b69xb2/connecting_with_traders_whats_your_endgame/'," datetime.datetime(2019, 4, 4, 10, 35, 8, 895142)",'2019-04-04',None,u'2','Trading',u'Connecting with traders: What\u2019s your endgame?'," u'https://old.reddit.com/r/Trading/comments/b69xb2/connecting_with_traders_whats_your_endgame/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 8, 895142) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/thewallstreet/comments/b7wazd/deviations_poc_and_value_area_for_monday_march_32/'," datetime.datetime(2019, 4, 4, 10, 35, 8, 900845)",'2019-04-04',None,u'12','thewallstreet',"u'Deviations, POC, and Value Area for Monday, March 32, 2019'"," u'https://i.imgur.com/hhBGPGq.png'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 8, 900845) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/thewallstreet/comments/b7tbdx/nightly_trading_discussion_march_3101/'," datetime.datetime(2019, 4, 4, 10, 35, 8, 904530)",'2019-04-04',u'Daily',u'10','thewallstreet',u'Nightly Trading Discussion - (March 31/01)'," u'https://old.reddit.com/r/thewallstreet/comments/b7tbdx/nightly_trading_discussion_march_3101/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 8, 904530) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/thewallstreet/comments/b7bzua/daily_spx_tpos_03302019/'," datetime.datetime(2019, 4, 4, 10, 35, 8, 907839)",'2019-04-04',None,u'20','thewallstreet',"u""Daily SPX TPO's 03-30-2019"""," u'https://i.imgur.com/9zeQwth.png'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 8, 907839) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Trading/comments/b69mf4/forex_trading_hours/'," datetime.datetime(2019, 4, 4, 10, 35, 8, 911232)",'2019-04-04',None,u'1','Trading',u'Forex Trading Hours'," u'https://old.reddit.com/r/Trading/comments/b69mf4/forex_trading_hours/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 8, 911232) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Trading/comments/b64ldx/good_free_charting_software/'," datetime.datetime(2019, 4, 4, 10, 35, 8, 914636)",'2019-04-04',None,u'1','Trading',u'Good free charting software'," u'https://old.reddit.com/r/Trading/comments/b64ldx/good_free_charting_software/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 8, 914636) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Trading/comments/b648xa/comprehensive_information_on_all_technical/'," datetime.datetime(2019, 4, 4, 10, 35, 8, 917836)",'2019-04-04',None,u'4','Trading',u'Comprehensive information on all technical indicators'," u'https://old.reddit.com/r/Trading/comments/b648xa/comprehensive_information_on_all_technical/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 8, 917836) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Trading/comments/b5tkr7/brokers_for_mt4mt5_platform/'," datetime.datetime(2019, 4, 4, 10, 35, 8, 920751)",'2019-04-04',None,u'1','Trading',u'Brokers for MT4/MT5 platform'," u'https://old.reddit.com/r/Trading/comments/b5tkr7/brokers_for_mt4mt5_platform/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 8, 920751) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/thewallstreet/comments/b73fth/random_discussion_thread_anything_goes/'," datetime.datetime(2019, 4, 4, 10, 35, 8, 924991)",'2019-04-04',None,u'15','thewallstreet',u'Random discussion thread. Anything goes.'," u'https://old.reddit.com/r/thewallstreet/comments/b73fth/random_discussion_thread_anything_goes/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 8, 924991) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Trading/comments/b5q0hd/noob_question/'," datetime.datetime(2019, 4, 4, 10, 35, 8, 942314)",'2019-04-04',None,u'1','Trading',u'Noob question'," u'https://old.reddit.com/r/Trading/comments/b5q0hd/noob_question/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 8, 942314) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Trading/comments/b5mtxj/i_need_psychological_help_about_trading/'," datetime.datetime(2019, 4, 4, 10, 35, 8, 945820)",'2019-04-04',None,u'11','Trading',u'I need psychological help about trading'," u'https://old.reddit.com/r/Trading/comments/b5mtxj/i_need_psychological_help_about_trading/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 8, 945820) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Trading/comments/b4lexk/etf/'," datetime.datetime(2019, 4, 4, 10, 35, 8, 949872)",'2019-04-04',None,u'2','Trading',u'ETF'," u'https://old.reddit.com/r/Trading/comments/b4lexk/etf/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 8, 949872) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/thewallstreet/comments/b71yl0/post_market_discussion_march_29/'," datetime.datetime(2019, 4, 4, 10, 35, 8, 954522)",'2019-04-04',u'Daily',u'9','thewallstreet',u'Post Market Discussion - (March 29)'," u'https://old.reddit.com/r/thewallstreet/comments/b71yl0/post_market_discussion_march_29/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 8, 954522) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Trading/comments/b497dy/for_all_the_swing_traders_out_there/'," datetime.datetime(2019, 4, 4, 10, 35, 8, 992128)",'2019-04-04',None,u'3','Trading',u'For all the swing traders out there!'," u'https://old.reddit.com/r/Trading/comments/b497dy/for_all_the_swing_traders_out_there/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 8, 992128) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/StockMarket/new/> (referer: None)
2019-04-04 10:35:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/StockMarket/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Stock_Picks/comments/b954ji/caterpillar_shares_fall_after_deutsche_bank/'," datetime.datetime(2019, 4, 4, 10, 35, 9, 260089)",'2019-04-04',u'News',u'19','Stock_Picks',"u""Caterpillar shares fall after Deutsche Bank downgrade: 'Synchronized global growth has collapsed'"""," u'https://www.cnbc.com/2019/04/03/caterpillar-shares-fall-after-deutsche-bank-downgrade.html'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 9, 260089) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/thewallstreet/comments/b6w5ra/daily_discussion_march_29/'," datetime.datetime(2019, 4, 4, 10, 35, 9, 265634)",'2019-04-04',u'Daily',u'11','thewallstreet',u'Daily Discussion - (March 29)'," u'https://old.reddit.com/r/thewallstreet/comments/b6w5ra/daily_discussion_march_29/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 9, 265634) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Trading/comments/b3zdh3/looking_for_trading_accountability_partners/'," datetime.datetime(2019, 4, 4, 10, 35, 9, 275676)",'2019-04-04',None,u'2','Trading',u'Looking for Trading Accountability Partners'," u'https://old.reddit.com/r/Trading/comments/b3zdh3/looking_for_trading_accountability_partners/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 9, 275676) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Trading/comments/b3o4hx/whats_the_catch_with_a_basic_crossing_ma_trading/'," datetime.datetime(2019, 4, 4, 10, 35, 9, 279228)",'2019-04-04',None,u'1','Trading',"u""What's the catch with a basic crossing MA trading strategy?"""," u'https://old.reddit.com/r/Trading/comments/b3o4hx/whats_the_catch_with_a_basic_crossing_ma_trading/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 9, 279228) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Stock_Picks/comments/b8pasy/sega_sgamy_stock_potential_after_retro_mini/'," datetime.datetime(2019, 4, 4, 10, 35, 9, 415768)",'2019-04-04',None,u'18','Stock_Picks',u'Sega: SGAMY stock potential after retro mini announcement set for September 19th 2019 + new Nintendo game with Mario.'," u'https://www.google.com/amp/s/www.gamespot.com/amp-articles/sega-reveals-mario-and-sonic-at-the-olympic-games-/1100-6465926/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 9, 415768) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Stock_Picks/comments/b8a2tk/lyft_drops_below_ipo_price_in_second_day_of/'," datetime.datetime(2019, 4, 4, 10, 35, 9, 425244)",'2019-04-04',u'News',u'34','Stock_Picks',u'Lyft Drops Below I.P.O. Price in Second Day of Trading'," u'https://www.nytimes.com/2019/04/01/technology/lyft-stock-ipo.html?partner=IFTTT'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 9, 425244) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Trading/comments/b3l17y/overall_market_trend_and_trading/'," datetime.datetime(2019, 4, 4, 10, 35, 9, 437934)",'2019-04-04',None,u'3','Trading',u'Overall market trend and trading'," u'https://old.reddit.com/r/Trading/comments/b3l17y/overall_market_trend_and_trading/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 9, 437934) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b96vgp/chaparral_energy_inc_chap_market_price_absurdly/'," datetime.datetime(2019, 4, 4, 10, 35, 9, 538942)",'2019-04-04',None,u'2','StockMarket',u'Chaparral Energy Inc. (CHAP) market price absurdly low?'," u'https://old.reddit.com/r/StockMarket/comments/b96vgp/chaparral_energy_inc_chap_market_price_absurdly/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 9, 538942) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Stock_Picks/comments/b89kbf/march_stock_pick_contest_over_april_stock_pick/'," datetime.datetime(2019, 4, 4, 10, 35, 9, 542866)",'2019-04-04',None,u'10','Stock_Picks',"u'March Stock Pick Contest over, April Stock Pick Contest now open!'"," u'https://old.reddit.com/r/Stock_Picks/comments/b89kbf/march_stock_pick_contest_over_april_stock_pick/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 9, 542866) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Stock_Picks/comments/b7vkiz/these_silicon_valley_investors_bets_may_pay_off/'," datetime.datetime(2019, 4, 4, 10, 35, 9, 548888)",'2019-04-04',u'Analysis/Opinion',u'12','Stock_Picks',u'These Silicon Valley Investors\u2019 Bets May Pay Off'," u'https://www.nytimes.com/2019/03/31/technology/silicon-valley-venture-capitalists.html?partner=IFTTT'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 9, 548888) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Trading/comments/b2n3ry/where_do_you_get_info/'," datetime.datetime(2019, 4, 4, 10, 35, 9, 568426)",'2019-04-04',None,u'8','Trading',u'Where do you get info?'," u'https://old.reddit.com/r/Trading/comments/b2n3ry/where_do_you_get_info/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 9, 568426) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Trading/comments/b1jxym/why_and_how_can_a_stock_jump_and_drop_in_price_so/'," datetime.datetime(2019, 4, 4, 10, 35, 9, 571605)",'2019-04-04',None,u'6','Trading',u'Why and how can a stock jump and drop in price so drastically during after hours?'," u'https://old.reddit.com/r/Trading/comments/b1jxym/why_and_how_can_a_stock_jump_and_drop_in_price_so/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 9, 571605) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Trading/comments/b0ygli/questions_on_interactive_brokers_api/'," datetime.datetime(2019, 4, 4, 10, 35, 9, 574991)",'2019-04-04',None,u'5','Trading',u'Questions on Interactive Brokers API'," u'https://old.reddit.com/r/Trading/comments/b0ygli/questions_on_interactive_brokers_api/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 9, 574991) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b96q4w/httpswwwcnbccom20190404trumpplanstomeetwithchinese/'," datetime.datetime(2019, 4, 4, 10, 35, 9, 585576)",'2019-04-04',None,u'0','StockMarket',u'https://www.cnbc.com/2019/04/04/trump-plans-to-meet-with-chinese-vice-premier-liu-he-on-thursday.html'," u'https://old.reddit.com/r/StockMarket/comments/b96q4w/httpswwwcnbccom20190404trumpplanstomeetwithchinese/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 9, 585576) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b96hgv/every_single_fed_tightening_cycle_has_ended_with/'," datetime.datetime(2019, 4, 4, 10, 35, 9, 588808)",'2019-04-04',None,u'9','StockMarket',u'Every single Fed tightening cycle has ended with an earnings recession except in 1994'," u'https://old.reddit.com/r/StockMarket/comments/b96hgv/every_single_fed_tightening_cycle_has_ended_with/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 9, 588808) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b95y6p/tesla_ships_63000_vehicles_in_q1_produced_77100/'," datetime.datetime(2019, 4, 4, 10, 35, 9, 592181)",'2019-04-04',None,u'140','StockMarket',"u'Tesla ships 63,000 vehicles in Q1, produced 77,100'"," u'https://old.reddit.com/r/StockMarket/comments/b95y6p/tesla_ships_63000_vehicles_in_q1_produced_77100/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 9, 592181) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Stock_Picks/comments/b6z44f/lyft_pops_20_in_trading_debut_after_opening_at/'," datetime.datetime(2019, 4, 4, 10, 35, 9, 598753)",'2019-04-04',u'News',u'29','Stock_Picks',u'Lyft pops 20% in trading debut after opening at $87.24'," u'https://www.cnbc.com/2019/03/29/lyft-ipo-stock-starts-trading-on-public-market.html'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 9, 598753) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b94udi/stock_market_finds_thirteen_supports/'," datetime.datetime(2019, 4, 4, 10, 35, 9, 625477)",'2019-04-04',None,u'1','StockMarket',u'Stock Market Finds Thirteen Supports'," u'https://old.reddit.com/r/StockMarket/comments/b94udi/stock_market_finds_thirteen_supports/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 9, 625477) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Stock_Picks/comments/b6om78/lyft_prices_ipo_at_72_a_share_marking_arrival_of/'," datetime.datetime(2019, 4, 4, 10, 35, 9, 637078)",'2019-04-04',u'News',u'25','Stock_Picks',"u'Lyft Prices I.P.O. at $72 a Share, Marking Arrival of Gig Economy to Wall Street'"," u'https://www.nytimes.com/2019/03/28/technology/lyft-ipo.html?partner=IFTTT'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 9, 637078) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Stock_Picks/comments/b64xcp/trading/'," datetime.datetime(2019, 4, 4, 10, 35, 9, 640616)",'2019-04-04',u'Discussion',u'5','Stock_Picks',u'Trading'," u'https://old.reddit.com/r/Stock_Picks/comments/b64xcp/trading/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 9, 640616) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b93zzy/who_is_the_comparable_of_levis_to_the_stock_market/'," datetime.datetime(2019, 4, 4, 10, 35, 9, 661501)",'2019-04-04',None,u'3','StockMarket',u'Who is the comparable of Levi\u2019s to the stock market'," u'https://old.reddit.com/r/StockMarket/comments/b93zzy/who_is_the_comparable_of_levis_to_the_stock_market/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 9, 661501) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b9388j/so_if_i_lend_shares_to_a_shorting_position_i_am/'," datetime.datetime(2019, 4, 4, 10, 35, 9, 666552)",'2019-04-04',None,u'4','StockMarket',"u'So, if I lend shares to a shorting position, I am paid out by my broker of just interest? Or am I paid for the shares in total at current price, and still get them back once the position is closed? Is there anyway I can live just off lending my shares.'"," u'https://old.reddit.com/r/StockMarket/comments/b9388j/so_if_i_lend_shares_to_a_shorting_position_i_am/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 9, 666552) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Stock_Picks/comments/b5snzy/china_plans_record_us_pork_imports_to_resolve/'," datetime.datetime(2019, 4, 4, 10, 35, 9, 674914)",'2019-04-04',u'News',u'34','Stock_Picks',u'China Plans Record U.S. Pork Imports to Resolve Trade War'," u'https://www.bloomberg.com/news/articles/2019-03-26/china-is-said-to-be-planning-record-u-s-pork-imports-this-year'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 9, 674914) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Stock_Picks/comments/b4aly8/pinterest_ahead_of_ipo_reveals_fast_growth_and/'," datetime.datetime(2019, 4, 4, 10, 35, 9, 680303)",'2019-04-04',u'News',u'4','Stock_Picks',"u'Pinterest, Ahead of I.P.O., Reveals Fast Growth and Losses'"," u'https://www.nytimes.com/2019/03/22/technology/pinterest-ipo.html?partner=IFTTT'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 9, 680303) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Stock_Picks/comments/b48gi8/dow_drops_more_than_350_points_sp_500_heads_for/'," datetime.datetime(2019, 4, 4, 10, 35, 9, 683573)",'2019-04-04',u'Analysis/Opinion',u'47','Stock_Picks',"u'Dow drops more than 350 points, S&P 500 heads for worst day since January'"," u'https://www.cnbc.com/2019/03/22/stock-market-wall-street-in-focus-as-growth-concerns-persist.html'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 9, 683573) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b91zjs/cvs_stock/'," datetime.datetime(2019, 4, 4, 10, 35, 9, 694785)",'2019-04-04',None,u'1','StockMarket',u'Cvs stock'," u'https://old.reddit.com/r/StockMarket/comments/b91zjs/cvs_stock/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 9, 694785) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Stock_Picks/comments/b3lyqp/prediction_nasdaqphii/'," datetime.datetime(2019, 4, 4, 10, 35, 9, 709373)",'2019-04-04',u'Analysis/Opinion',u'5','Stock_Picks',u'Prediction NASDAQ:PHII'," u'https://old.reddit.com/r/Stock_Picks/comments/b3lyqp/prediction_nasdaqphii/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 9, 709373) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Stock_Picks/comments/b2kei1/lyft_sets_23_billion_value_as_highend_goal_for_ipo/'," datetime.datetime(2019, 4, 4, 10, 35, 9, 715420)",'2019-04-04',u'News',u'22','Stock_Picks',u'Lyft Sets $23 Billion Value as High-End Goal for I.P.O.'," u'https://www.nytimes.com/2019/03/18/technology/lyft-ipo.html?partner=IFTTT'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 9, 715420) is not JSON serializable
DEBUG:scrapy.core.engine:Crawled (200) <GET https://old.reddit.com/r/Daytrading/new/> (referer: None)
2019-04-04 10:35:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://old.reddit.com/r/Daytrading/new/> (referer: None)
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b91kn6/pump_and_dump/'," datetime.datetime(2019, 4, 4, 10, 35, 9, 733903)",'2019-04-04',None,u'0','StockMarket',u'Pump and Dump'," u'https://old.reddit.com/r/StockMarket/comments/b91kn6/pump_and_dump/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 9, 733903) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Stock_Picks/comments/b18mxn/exclusive_uber_plans_to_kick_off_ipo_in_april/'," datetime.datetime(2019, 4, 4, 10, 35, 9, 744068)",'2019-04-04',u'News',u'10','Stock_Picks',u'Exclusive: Uber plans to kick off IPO in April - sources'," u'https://www.reuters.com/article/us-uber-ipo-exclusive/exclusive-uber-plans-to-kick-off-ipo-in-april-sources-idUSKCN1QV2QU'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 9, 744068) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Stock_Picks/comments/b14upm/boeings_value_has_sunk_27_billion_since_ethiopian/'," datetime.datetime(2019, 4, 4, 10, 35, 9, 753993)",'2019-04-04',None,u'60','Stock_Picks',"u""Boeing's value has sunk $27 billion since Ethiopian Air crash"""," u'https://www.cbsnews.com/news/boeing-stock-price-value-has-sunk-28-billion-since-ethiopian-air-crash/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 9, 753993) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b9173b/what_stocks_would_you_prefer_to_buy_for_one_year/'," datetime.datetime(2019, 4, 4, 10, 35, 9, 760386)",'2019-04-04',None,u'0','StockMarket',u'What Stocks would you prefer to buy for one year time from now?'," u'https://old.reddit.com/r/StockMarket/comments/b9173b/what_stocks_would_you_prefer_to_buy_for_one_year/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 9, 760386) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b90fxk/verizon_begins_rolling_out_its_5g_wireless/'," datetime.datetime(2019, 4, 4, 10, 35, 9, 765610)",'2019-04-04',None,u'184','StockMarket',u'Verizon begins rolling out its 5G wireless network for smartphones'," u'https://old.reddit.com/r/StockMarket/comments/b90fxk/verizon_begins_rolling_out_its_5g_wireless/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 9, 765610) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Stock_Picks/comments/b0osx0/elon_musk_set_to_unveil_a_new_tesla_for_the/'," datetime.datetime(2019, 4, 4, 10, 35, 9, 776051)",'2019-04-04',u'News',u'2','Stock_Picks',u'Elon Musk Set to Unveil a New Tesla for the Masses\u2014the Model Y SUV'," u'https://outline.com/xdyXDF'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 9, 776051) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Stock_Picks/comments/b0n275/africas_first_tech_unicorn_jumia_files_for_ipo/'," datetime.datetime(2019, 4, 4, 10, 35, 9, 780018)",'2019-04-04',u'News',u'41','Stock_Picks',"u""Africa's first tech unicorn Jumia files for IPO"""," u'https://www.axios.com/jumia-africas-tech-unicorn-ipo-ecommerce-613c6352-1171-4dd9-ae4f-6f9ae114ebbb.html'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 9, 780018) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Stock_Picks/comments/b0kzw6/closing_bell_sensex_ends_216_pts_higher_nifty_at/'," datetime.datetime(2019, 4, 4, 10, 35, 9, 783908)",'2019-04-04',None,u'9','Stock_Picks',"u'Closing Bell: Sensex ends 216 pts higher, Nifty at 11,340; Bharti Airtel slips 4%'"," u'https://www.moneycontrol.com/news/business/markets/closing-bell-sensex-ends-216-pts-higher-nifty-at-11340-bharti-airtel-slips-4-3639631.html'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 9, 783908) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b907fk/school_finance_project/'," datetime.datetime(2019, 4, 4, 10, 35, 9, 791862)",'2019-04-04',None,u'0','StockMarket',u'School finance project'," u'https://old.reddit.com/r/StockMarket/comments/b907fk/school_finance_project/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 9, 791862) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Stock_Picks/comments/azx2bl/tesla_secures_chinese_funding_for_gigafactory_in/'," datetime.datetime(2019, 4, 4, 10, 35, 10, 47970)",'2019-04-04',u'News',u'43','Stock_Picks',"u""Tesla secures Chinese funding for 'gigafactory' in Shanghai"""," u'https://www.driven.co.nz/news/news/tesla-secures-chinese-funding-for-gigafactory-in-shanghai/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 10, 47970) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Stock_Picks/comments/azwmqq/us_stocks_move_broadly_higher_on_tech_strength/'," datetime.datetime(2019, 4, 4, 10, 35, 10, 55291)",'2019-04-04',u'News',u'6','Stock_Picks',u'US stocks move broadly higher on tech strength; Boeing drops'," u'https://www.apnews.com/6cb89f45b44a46348ca4d4a52280604b'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 10, 55291) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b8zqbc/nvidia/'," datetime.datetime(2019, 4, 4, 10, 35, 10, 79436)",'2019-04-04',None,u'5','StockMarket',u'Nvidia!!!'," u'https://old.reddit.com/r/StockMarket/comments/b8zqbc/nvidia/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 10, 79436) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Stock_Picks/comments/azfv8p/manage_2000000_competitive_stock_market_game/'," datetime.datetime(2019, 4, 4, 10, 35, 10, 90630)",'2019-04-04',u'Contest',u'16','Stock_Picks',"u""Manage $2,000,000 Competitive Stock Market game - 'Overnight Billionaire 2019\u2019"""," u'https://www.marketwatch.com/game/overnightbillionaire2019'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 10, 90630) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Daytrading/comments/b99bda/two_vwap_rejection_trades_and_how_i_approach_them/'," datetime.datetime(2019, 4, 4, 10, 35, 10, 268030)",'2019-04-04',None,u'6','Daytrading',u'Two VWAP rejection trades and how I approach them. 2nd trade was a textbook example.'," u'https://youtu.be/r1esd7O3LBc'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 10, 268030) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b8ye3b/morning_market_news_20190403/'," datetime.datetime(2019, 4, 4, 10, 35, 10, 271862)",'2019-04-04',None,u'32','StockMarket',u'Morning Market News (2019-04-03)'," u'https://old.reddit.com/r/StockMarket/comments/b8ye3b/morning_market_news_20190403/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 10, 271862) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b8swlx/how_does_shorting_exactly_work/'," datetime.datetime(2019, 4, 4, 10, 35, 10, 279519)",'2019-04-04',None,u'23','StockMarket',u'How does shorting exactly work?'," u'https://old.reddit.com/r/StockMarket/comments/b8swlx/how_does_shorting_exactly_work/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 10, 279519) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Stock_Picks/comments/ayvgd3/opinions_sangoma_technologies_corporation_stcv/'," datetime.datetime(2019, 4, 4, 10, 35, 10, 289090)",'2019-04-04',u'Analysis/Opinion',u'7','Stock_Picks',u'Opinions? Sangoma Technologies Corporation (STC.V)'," u'https://old.reddit.com/r/Stock_Picks/comments/ayvgd3/opinions_sangoma_technologies_corporation_stcv/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 10, 289090) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Stock_Picks/comments/aynlen/im_a_beginner_to_the_stock_market_and_i_need_some/'," datetime.datetime(2019, 4, 4, 10, 35, 10, 293178)",'2019-04-04',u'Discussion',u'4','Stock_Picks',"u""I'm a beginner to the stock market and I need some help."""," u'https://old.reddit.com/r/Stock_Picks/comments/aynlen/im_a_beginner_to_the_stock_market_and_i_need_some/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 10, 293178) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Daytrading/comments/b9787p/im_thinking_of_posting_my_papertrades_in_real/'," datetime.datetime(2019, 4, 4, 10, 35, 10, 299311)",'2019-04-04',None,u'5','Daytrading',u'Im thinking of posting my papertrades in real time tomorrow. Would anyone be interested to follow along?'," u'https://old.reddit.com/r/Daytrading/comments/b9787p/im_thinking_of_posting_my_papertrades_in_real/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 10, 299311) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Daytrading/comments/b96o44/is_the_steven_dux_dvd_worth_it/'," datetime.datetime(2019, 4, 4, 10, 35, 10, 303413)",'2019-04-04',None,u'9','Daytrading',u'Is the Steven Dux DVD worth it?'," u'https://old.reddit.com/r/Daytrading/comments/b96o44/is_the_steven_dux_dvd_worth_it/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 10, 303413) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Daytrading/comments/b8zwtb/is_it_possible_to_make_a_living_in_a_year_just/'," datetime.datetime(2019, 4, 4, 10, 35, 10, 308538)",'2019-04-04',None,u'12','Daytrading',u'Is it possible to make a living (in a year) just trading 2 stocks over and over?'," u'https://old.reddit.com/r/Daytrading/comments/b8zwtb/is_it_possible_to_make_a_living_in_a_year_just/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 10, 308538) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b8qn0p/meal_kit_delivery_blueapron_goodfood_etc/'," datetime.datetime(2019, 4, 4, 10, 35, 10, 314446)",'2019-04-04',None,u'2','StockMarket',"u'Meal kit delivery (BlueApron, Goodfood etc)'"," u'https://old.reddit.com/r/StockMarket/comments/b8qn0p/meal_kit_delivery_blueapron_goodfood_etc/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 10, 314446) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b8pzjm/meso_on_a_tear/'," datetime.datetime(2019, 4, 4, 10, 35, 10, 321816)",'2019-04-04',None,u'2','StockMarket',u'MESO on a tear?'," u'https://old.reddit.com/r/StockMarket/comments/b8pzjm/meso_on_a_tear/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 10, 321816) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Stock_Picks/comments/ayhhaw/grab_a_southeast_asian_ridehailing_giant_raises/'," datetime.datetime(2019, 4, 4, 10, 35, 10, 342668)",'2019-04-04',u'News',u'27','Stock_Picks',"u'Grab, a Southeast Asian Ride-Hailing Giant, Raises $1.5 Billion'"," u'https://www.nytimes.com/2019/03/06/technology/grab-funding-softbank.html?partner=IFTTT'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 10, 342668) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Stock_Picks/comments/awym57/as_uber_prepares_to_go_public_its_lead_lawyer/'," datetime.datetime(2019, 4, 4, 10, 35, 10, 345954)",'2019-04-04',u'Analysis/Opinion',u'31','Stock_Picks',"u'As Uber Prepares to Go Public, Its Lead Lawyer Races to Clean It Up'"," u'https://www.nytimes.com/2019/03/03/technology/uber-ipo-legal-issues.html?partner=IFTTT'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 10, 345954) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Daytrading/comments/b8z7mt/i_alerted_optt_3_weeks_ago_on_my_youtube_channel/'," datetime.datetime(2019, 4, 4, 10, 35, 10, 359686)",'2019-04-04',None,u'0','Daytrading',u'I alerted $OPTT 3 weeks ago on my youtube channel.'," u'https://old.reddit.com/r/Daytrading/comments/b8z7mt/i_alerted_optt_3_weeks_ago_on_my_youtube_channel/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 10, 359686) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Daytrading/comments/b8s881/question_about_level_2/'," datetime.datetime(2019, 4, 4, 10, 35, 10, 364112)",'2019-04-04',None,u'9','Daytrading',u'Question about Level 2'," u'https://old.reddit.com/r/Daytrading/comments/b8s881/question_about_level_2/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 10, 364112) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b8pu6l/sonos_stock_sonos_initiated_at_buy_by_da_davidson/'," datetime.datetime(2019, 4, 4, 10, 35, 10, 367150)",'2019-04-04',None,u'2','StockMarket',u'#sonos stock (Sonos initiated at buy by DA Davidson ) at $20 ?'," u'https://old.reddit.com/r/StockMarket/comments/b8pu6l/sonos_stock_sonos_initiated_at_buy_by_da_davidson/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 10, 367150) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b8psvu/are_wallet_investor_tipranks_good_websites_to/'," datetime.datetime(2019, 4, 4, 10, 35, 10, 370099)",'2019-04-04',None,u'2','StockMarket',"u'Are ""Wallet investor"" & ""Tipranks"" good websites to explore the stock market?'"," u'https://old.reddit.com/r/StockMarket/comments/b8psvu/are_wallet_investor_tipranks_good_websites_to/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 10, 370099) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b8po9e/thoughts_on_my_portfolio/'," datetime.datetime(2019, 4, 4, 10, 35, 10, 375900)",'2019-04-04',None,u'1','StockMarket',u'Thoughts on my portfolio?'," u'https://old.reddit.com/r/StockMarket/comments/b8po9e/thoughts_on_my_portfolio/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 10, 375900) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Daytrading/comments/b8qh9u/candlestick_trading_where_next/'," datetime.datetime(2019, 4, 4, 10, 35, 10, 382207)",'2019-04-04',None,u'3','Daytrading',u'Candlestick trading where next?'," u'https://old.reddit.com/r/Daytrading/comments/b8qh9u/candlestick_trading_where_next/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 10, 382207) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Daytrading/comments/b8l884/i_want_to_start_with_1000_stocks_or_forex/'," datetime.datetime(2019, 4, 4, 10, 35, 10, 385592)",'2019-04-04',None,u'12','Daytrading',"u'I want to start with $1,000 - Stocks or FOREX?'"," u'https://old.reddit.com/r/Daytrading/comments/b8l884/i_want_to_start_with_1000_stocks_or_forex/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 10, 385592) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Daytrading/comments/b8l5c9/can_you_suggest_a_profitable_day_trading_alert/'," datetime.datetime(2019, 4, 4, 10, 35, 10, 389080)",'2019-04-04',None,u'1','Daytrading',u'Can you suggest a profitable day trading alert service?'," u'https://old.reddit.com/r/Daytrading/comments/b8l5c9/can_you_suggest_a_profitable_day_trading_alert/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 10, 389080) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b8osyg/jp_morgan_chase_jpm_technical_analysis_stock/'," datetime.datetime(2019, 4, 4, 10, 35, 10, 392701)",'2019-04-04',None,u'5','StockMarket',u'J.P. Morgan Chase (JPM) Technical Analysis Stock Report'," u'https://old.reddit.com/r/StockMarket/comments/b8osyg/jp_morgan_chase_jpm_technical_analysis_stock/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 10, 392701) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b8m99v/im_looking_for_advice/'," datetime.datetime(2019, 4, 4, 10, 35, 10, 397627)",'2019-04-04',None,u'0','StockMarket',u'i\u2019m looking for advice'," u'https://old.reddit.com/r/StockMarket/comments/b8m99v/im_looking_for_advice/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 10, 397627) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Daytrading/comments/b8kzh1/can_this_be_a_good_trading_strategy/'," datetime.datetime(2019, 4, 4, 10, 35, 10, 404029)",'2019-04-04',None,u'10','Daytrading',u'Can this be a good trading strategy?'," u'https://old.reddit.com/r/Daytrading/comments/b8kzh1/can_this_be_a_good_trading_strategy/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 10, 404029) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/StockMarket/comments/b8kp5q/ditch_or_hold_vktx/'," datetime.datetime(2019, 4, 4, 10, 35, 10, 418385)",'2019-04-04',None,u'2','StockMarket',u'Ditch or hold VKTX?'," u'https://old.reddit.com/r/StockMarket/comments/b8kp5q/ditch_or_hold_vktx/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 10, 418385) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Daytrading/comments/b8j9e4/tos_paper_trading_account_will_not_filter_ah/'," datetime.datetime(2019, 4, 4, 10, 35, 10, 428204)",'2019-04-04',None,u'2','Daytrading',u'ToS paper trading account will not filter AH activity for my Gap scan. Is this normal?'," u'https://old.reddit.com/r/Daytrading/comments/b8j9e4/tos_paper_trading_account_will_not_filter_ah/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 10, 428204) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Daytrading/comments/b8hktx/day_trading_rty/'," datetime.datetime(2019, 4, 4, 10, 35, 10, 439077)",'2019-04-04',None,u'4','Daytrading',u'Day Trading RTY'," u'https://old.reddit.com/r/Daytrading/comments/b8hktx/day_trading_rty/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 10, 439077) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Daytrading/comments/b89m27/psychology_of_vwap/'," datetime.datetime(2019, 4, 4, 10, 35, 10, 453055)",'2019-04-04',None,u'2','Daytrading',u'Psychology of VWAP?'," u'https://old.reddit.com/r/Daytrading/comments/b89m27/psychology_of_vwap/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 10, 453055) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Daytrading/comments/b89iv0/overnight_small_cap_penny_stocks_set_up_made_this/'," datetime.datetime(2019, 4, 4, 10, 35, 10, 456659)",'2019-04-04',None,u'26','Daytrading',"u""Overnight small cap penny stocks set up. Made this video after getting a lot of DM's here"""," u'https://youtu.be/7e0AgpjYwjg'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 10, 456659) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Daytrading/comments/b875oz/top_5_growth_stocks_for_april_robinhood_challenge/'," datetime.datetime(2019, 4, 4, 10, 35, 10, 461881)",'2019-04-04',None,u'0','Daytrading',u'Top 5 Growth Stocks for April - Robinhood Challenge #2'," u'https://youtu.be/Z1kQOcmXJZQ'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 10, 461881) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Daytrading/comments/b85mnb/best_charting_platforms_tradingview/'," datetime.datetime(2019, 4, 4, 10, 35, 10, 466330)",'2019-04-04',None,u'1','Daytrading',u'Best charting platforms? (TradingView?)'," u'https://old.reddit.com/r/Daytrading/comments/b85mnb/best_charting_platforms_tradingview/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 10, 466330) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Daytrading/comments/b7ys3j/bitcoin_predicts_moves_cannabis_stock_30_days_in/'," datetime.datetime(2019, 4, 4, 10, 35, 10, 469330)",'2019-04-04',None,u'0','Daytrading',"u'Bitcoin predicts moves Cannabis Stock, 30 DAYS in ADVANCE.'"," u'https://www.reddit.com/user/maverick91100/comments/b7yrv6/bitcoin_predicts_moves_cannabis_stock_30_days_in/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 10, 469330) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Daytrading/comments/b7n3g3/help/'," datetime.datetime(2019, 4, 4, 10, 35, 10, 472619)",'2019-04-04',None,u'0','Daytrading',u'HELP'," u'https://old.reddit.com/r/Daytrading/comments/b7n3g3/help/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 10, 472619) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Daytrading/comments/b7gq09/overnight_buy_on_small_cap_penny_stocks_was/'," datetime.datetime(2019, 4, 4, 10, 35, 10, 475548)",'2019-04-04',None,u'34','Daytrading',u'Overnight buy on small cap penny stocks. Was delighted by my own stats'," u'https://old.reddit.com/r/Daytrading/comments/b7gq09/overnight_buy_on_small_cap_penny_stocks_was/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 10, 475548) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Daytrading/comments/b7ftes/suretrader_and_tradezero_which_one/'," datetime.datetime(2019, 4, 4, 10, 35, 10, 478985)",'2019-04-04',None,u'1','Daytrading',u'Suretrader and Tradezero. Which one?'," u'https://old.reddit.com/r/Daytrading/comments/b7ftes/suretrader_and_tradezero_which_one/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 10, 478985) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Daytrading/comments/b7f94g/what_is_the_most_important_thing_to_become/'," datetime.datetime(2019, 4, 4, 10, 35, 10, 482695)",'2019-04-04',None,u'12','Daytrading',u'What is the most important thing to become sucsessful?'," u'https://old.reddit.com/r/Daytrading/comments/b7f94g/what_is_the_most_important_thing_to_become/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 10, 482695) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Daytrading/comments/b7eq5x/a_good_read_for_everybody_here_to_help_with_the/'," datetime.datetime(2019, 4, 4, 10, 35, 10, 486167)",'2019-04-04',None,u'5','Daytrading',u'A good read for everybody here to help with the psychology of trading'," u'https://youarenotsosmart.com/2011/03/25/the-sunk-cost-fallacy/?utm_source=share&utm_medium=ios_app'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 10, 486167) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Daytrading/comments/b7dj02/journaling_software/'," datetime.datetime(2019, 4, 4, 10, 35, 10, 489671)",'2019-04-04',None,u'4','Daytrading',u'Journaling software?'," u'https://old.reddit.com/r/Daytrading/comments/b7dj02/journaling_software/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 10, 489671) is not JSON serializable
ERROR:scrapy.core.scraper:Error processing "
https://old.reddit.com/r/Daytrading/comments/b7czyx/advice_tell_me_why_my_idea_wont_work/'," datetime.datetime(2019, 4, 4, 10, 35, 10, 498611)",'2019-04-04',None,u'10','Daytrading',u'Advice: Tell Me Why My Idea Won\u2019t Work?'," u'https://old.reddit.com/r/Daytrading/comments/b7czyx/advice_tell_me_why_my_idea_wont_work/'}
Traceback (most recent call last):
  File ""/Users/selenacordona/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py"", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File ""/Users/selenacordona/redditcrawler/redditcrawler/pipelines.py"", line 56, in process_item
    line = json.dumps(dict(item)) + ""\n""
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/__init__.py"", line 244, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/selenacordona/anaconda2/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: datetime.datetime(2019, 4, 4, 10, 35, 10, 498611) is not JSON serializable
INFO:scrapy.core.engine:Closing spider (finished)
2019-04-04 10:35:10 [scrapy.core.engine] INFO: Closing spider (finished)
INFO:scrapy.statscollectors:Dumping Scrapy stats:
"
